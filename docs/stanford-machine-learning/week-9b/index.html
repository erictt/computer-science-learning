<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">

    
      <link rel="icon" href="/favicon.png" />
    

    <title>
        
          stanford-machine-learning/week-9b - Eric&#39;s CS Notes
        
    </title>

    <!-- Spectre.css framework -->
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-exp.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-icons.min.css">

    <!-- theme css & js -->
    
<link rel="stylesheet" href="/css/book.css">

    
<script src="/js/book.js"></script>


    <!-- tocbot -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css">
    
    <!-- katex -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">

    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/zooming/2.1.1/zooming.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    const zooming = new Zooming()
    zooming.listen('.book-content img')
})
</script>

<meta name="generator" content="Hexo 4.2.0"></head>

<body>

<div class="book-container">
  <div class="book-sidebar">
    <div class="book-brand">
  <a href="/">
    <img src="/favicon.png">
    <span>ERIC&#39;S CS NOTES</span>
  </a>
</div>
    <div class="book-menu">
  <h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2>
<ul>
<li><a href="/README/index.html">Introduction</a></li>
<li><a href="/SYMBOLS/index.html">Symbols of Mathematics</a></li>
</ul>
<!--
## Stanford Statistical Learning

* [Chapter 2 - Overview of Statistical Learning](/statistical-learning/chapter-2/index.html)
* Chapter 3 - Linear Regression
* Chapter 4 - Classification
* Chapter 5 - Resampling Methods
* Chapter 6 - Linear Model Selection and Regularization
* Chapter 7 - Moving Beyond Linearity
* Chapter 8 - Tree-Based Methods
* Chapter 9 - Support Vector Machines
* Chapter 10 - Unsupervised Learning
-->
<h2 id="introduction-to-probability"><a class="markdownIt-Anchor" href="#introduction-to-probability"></a> Introduction to Probability</h2>
<ul>
<li><a href="/introduction-to-probability/unit-1/index.html">Unit 1: Probability models and axioms</a></li>
<li><a href="/introduction-to-probability/unit-2/index.html">Unit 2: Conditioning and independence</a></li>
<li><a href="/introduction-to-probability/unit-3/index.html">Unit 3: Counting</a></li>
</ul>
<!--
* [Unit 4: Discrete random variables](/introduction-to-probability/unit-4/index.html)
* [Unit 5: Continuous random variables](/introduction-to-probability/unit-5/index.html)
* [Unit 6: Further topics on random variables](/introduction-to-probability/unit-6/index.html)
* [Unit 7: Bayesian inference](/introduction-to-probability/unit-7/index.html)
* [Unit 8: Limit theorems and classical statistics](/introduction-to-probability/unit-8/index.html)
* [Unit 9: Bernoulli and Poisson processes](/introduction-to-probability/unit-9/index.html)
* [Unit 10: Markov chains](/introduction-to-probability/unit-10/index.html)
-->
<h2 id="multivariable-calculus"><a class="markdownIt-Anchor" href="#multivariable-calculus"></a> Multivariable Calculus</h2>
<ul>
<li><a href="/multivariable-calculus/unit-1/index.html">Unit 1: Thinking about multivariable functions</a></li>
<li><a href="/multivariable-calculus/unit-2/index.html">Unit 2: Derivatives of multivariable functions</a></li>
</ul>
<!--
* [Unit 3: Applications of multivariable derivatives](/multivariable-calculus/unit-3/index.html)
* [Unit 4: Integrating multivariable functions](/multivariable-calculus/unit-4/index.html)
* [Unit 5: Green's, Stokes', and the divergence theorems](/multivariable-calculus/unit-5/index.html)
-->
<h2 id="algorithms-part-ii"><a class="markdownIt-Anchor" href="#algorithms-part-ii"></a> Algorithms: Part II</h2>
<ul>
<li><a href="/algorithms-2/week-1/index.html">Week 1 - Undirected Graph &amp; Directed Graph</a></li>
<li><a href="/algorithms-2/week-3/index.html">Week 3 - Maximum Flow and Minimum Cut &amp; Radix Sort</a></li>
<li><a href="/algorithms-2/week-4/index.html">Week 4 - Tries &amp; Substring Search</a></li>
</ul>
<h2 id="algorithms-part-i"><a class="markdownIt-Anchor" href="#algorithms-part-i"></a> Algorithms: Part I</h2>
<ul>
<li><a href="/algorithms-1/week-1/index.html">Week 1 - Union-Find &amp; Analysis of Algorithms</a></li>
<li><a href="/algorithms-1/week-2/index.html">Week 2 - Stacks and Queues &amp; Elementary Sorts</a></li>
<li><a href="/algorithms-1/week-3/index.html">Week 3 - Mergesort &amp; Quicksort</a></li>
<li><a href="/algorithms-1/week-4/index.html">Week 4 - Priority Queues &amp; Elementary Symbols</a></li>
<li><a href="/algorithms-1/week-5/index.html">Week 5 - Balanced Search Trees</a></li>
<li><a href="/algorithms-1/week-6/index.html">Week 6 - Hash Tables</a></li>
</ul>
<h2 id="introduction-to-software-design-and-architecture"><a class="markdownIt-Anchor" href="#introduction-to-software-design-and-architecture"></a> Introduction to Software Design and Architecture</h2>
<ul>
<li><a href="/introduction-to-software-design-and-architecture/design-pattern/index.html">Design Pattern</a></li>
</ul>
<h2 id="calculus-two-sequences-and-series"><a class="markdownIt-Anchor" href="#calculus-two-sequences-and-series"></a> Calculus Two: Sequences and Series</h2>
<ul>
<li><a href="/calculus-two/week-1/index.html">Week 1 - Sequences</a></li>
<li><a href="/calculus-two/week-2/index.html">Week 2 - Series</a></li>
<li><a href="/calculus-two/week-3/index.html">Week 3 - Convergence Tests</a></li>
<li><a href="/calculus-two/week-4/index.html">Week 4 - Alternating Series</a></li>
<li><a href="/calculus-two/week-5/index.html">Week 5 - Power Series</a></li>
<li><a href="/calculus-two/week-6/index.html">Week 6 - Taylor Series</a></li>
</ul>
<h2 id="laff-linear-algebra"><a class="markdownIt-Anchor" href="#laff-linear-algebra"></a> LAFF Linear Algebra</h2>
<ul>
<li><a href="/laff-linear-algebra/week-1/index.html">Week 1 - Vectors in Linear Algebra</a></li>
<li><a href="/laff-linear-algebra/week-2/index.html">Week 2 - Linear Transformations and Matrices</a></li>
<li><a href="/laff-linear-algebra/week-3/index.html">Week 3 - Matrix-Vector Operations</a></li>
<li><a href="/laff-linear-algebra/week-4/index.html">Week 4 - Matrix-Vector to Matrix-Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-5/index.html">Week 5 - Matrix- Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-6/index.html">Week 6 - Gaussian Elimination</a></li>
<li><a href="/laff-linear-algebra/week-7/index.html">Week 7 - More Gaussian Elimination and Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-8/index.html">Week 8 - More on Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-9/index.html">Week 9 - Vector Spaces</a></li>
<li><a href="/laff-linear-algebra/week-10/index.html">Week 10 - Vector Spaces, Orthogonality, and Linear Least-Squares</a></li>
<li><a href="/laff-linear-algebra/week-11/index.html">Week 11 - Orthogonal Projection, Low Rank Approximation, and Orthogonal Bases</a></li>
<li><a href="/laff-linear-algebra/week-12/index.html">Week 12 - Eigenvalues and Eigenvectors</a></li>
</ul>
<h2 id="stanford-machine-learning"><a class="markdownIt-Anchor" href="#stanford-machine-learning"></a> Stanford Machine Learning</h2>
<ul>
<li><a href="/stanford-machine-learning/week-1/index.html">Week 1 - Introduction</a></li>
<li><a href="/stanford-machine-learning/week-2/index.html">Week 2 - Linear Regression with Multiple Variables</a></li>
<li><a href="/stanford-machine-learning/week-3/index.html">Week 3 - Logistic Regression &amp; Regularization</a></li>
<li><a href="/stanford-machine-learning/week-4/index.html">Week 4 - Neural Networks: Representation</a></li>
<li><a href="/stanford-machine-learning/week-5/index.html">Week 5 - Neural Networks: Learning</a></li>
<li><a href="/stanford-machine-learning/week-6a/index.html">Week 6a - Advice for Applying Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-6b/index.html">Week 6b - Machine Learning System Design</a></li>
<li><a href="/stanford-machine-learning/week-7/index.html">Week 7 - Support Vector Machines</a></li>
<li><a href="/stanford-machine-learning/week-8/index.html">Week 8 - Unsupervised Learning &amp; Dimensionality Reduction</a></li>
<li><a href="/stanford-machine-learning/week-9a/index.html">Week 9a - Anomaly Detection</a></li>
<li><a href="/stanford-machine-learning/week-9b/index.html">Week 9b - Recommender Systems</a></li>
<li><a href="/stanford-machine-learning/week-10/index.html">Week 10 - Large Scale Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-11/index.html">Week 11 - Application Example: Photo OCR</a></li>
</ul>
<h2 id="calculus-one"><a class="markdownIt-Anchor" href="#calculus-one"></a> Calculus One</h2>
<ul>
<li><a href="/calculus-one/week-2-3/index.html">Week 2-3 - Functions &amp; Limits</a></li>
<li><a href="/calculus-one/week-4/index.html">Week 4 - The Beginning of Derivatives</a></li>
<li><a href="/calculus-one/week-5/index.html">Week 5 - Techniques of Differentiation</a></li>
<li><a href="/calculus-one/week-6/index.html">Week 6 - Chain Rule</a></li>
<li><a href="/calculus-one/week-7/index.html">Week 7 - Derivatives of Trigonometric Functions</a></li>
<li><a href="/calculus-one/week-8/index.html">Week 8 - Derivatives in the Real World</a></li>
<li><a href="/calculus-one/week-9/index.html">Week 9 - Optimization</a></li>
<li><a href="/calculus-one/week-10/index.html">Week 10 - Linear Approximation</a></li>
<li><a href="/calculus-one/week-11-12/index.html">Week 11-12 - Antidifferentiation &amp; Integration</a></li>
<li><a href="/calculus-one/week-13/index.html">Week 13 - Fundamental Theorem of Calculus</a></li>
<li><a href="/calculus-one/week-14/index.html">Week 14 - Substitution Rule</a></li>
<li><a href="/calculus-one/week-15/index.html">Week 15 - Techniques of Integration</a></li>
<li><a href="/calculus-one/week-16/index.html">Week 16 - Applications of Integration</a></li>
</ul>
<h2 id="computational-thinking"><a class="markdownIt-Anchor" href="#computational-thinking"></a> Computational Thinking</h2>
<ul>
<li><a href="/computational-thinking/lecture-1/index.html">Lecture 1 - Optimization and Knapsack Problem</a></li>
<li><a href="/computational-thinking/lecture-2/index.html">Lecture 2 - Decision Trees and Dynamic Programming</a>
<ul>
<li><a href="/computational-thinking/lecture-2-powerset/index.html">Exercise: Power Set Function</a></li>
</ul>
</li>
<li><a href="/computational-thinking/lecture-3/index.html">Lecture 3 - Graphs</a></li>
<li><a href="/computational-thinking/lecture-4-5/index.html">Lecture 4-5 - Plotting</a></li>
<li><a href="/computational-thinking/lecture-6-7/index.html">Lecture 6-7 - Stochastic Programs &amp; Inferential Statistics</a></li>
<li><a href="/computational-thinking/lecture-8/index.html">Lecture 8 - Monte Carlo Simulation</a></li>
<li><a href="/computational-thinking/lecture-9/index.html">Lecture 9 - Sampling and Standard Error</a></li>
<li><a href="/computational-thinking/lecture-10-11/index.html">Lecture 10-11 - Experimental Data</a></li>
<li><a href="/computational-thinking/lecture-12/index.html">Lecture 12 - Machine Learning</a></li>
<li><a href="/computational-thinking/lecture-13/index.html">Lecture 13 - Statistical Abuses</a></li>
</ul>
<h2 id="effective-thinking-through-mathematics"><a class="markdownIt-Anchor" href="#effective-thinking-through-mathematics"></a> Effective Thinking Through Mathematics</h2>
<ul>
<li><a href="/effective-thinking-through-mathematics/note/index.html">Note</a></li>
<li><a href="/effective-thinking-through-mathematics/week-4-telling-the-story-of-infinity/index.html">Week 4 (/Telling the Story of Infinity)</a></li>
<li><a href="/effective-thinking-through-mathematics/week-5-telling-the-story-of-the-euler-circuit-theorem/index.html">Week 5 (/Telling the Story of Euler Circuit Theorem)</a></li>
</ul>
<h2 id="cs50-introduction-to-computer-science"><a class="markdownIt-Anchor" href="#cs50-introduction-to-computer-science"></a> CS50 Introduction to Computer Science</h2>
<ul>
<li><a href="/cs50/week-1/index.html">Week 1 - C</a></li>
<li><a href="/cs50/week-2/index.html">Week 2 - Arrays</a></li>
<li><a href="/cs50/week-3/index.html">Week 3 - Algorithms</a></li>
<li><a href="/cs50/week-4/index.html">Week 4 - Memory</a></li>
<li><a href="/cs50/week-5/index.html">Week 5 - Data Structures</a></li>
<li><a href="/cs50/week-6/index.html">Week 6 - HTTP</a></li>
<li><a href="/cs50/week-7-10/index.html">Week 7-10 - Machine Learning/Python/SQL/Javascript</a></li>
</ul>

</div>


<script src="/js/book-menu.js"></script>

  </div>

  <div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseover="add_inner()" onmouseleave="remove_inner()">
  <div class="sidebar-toggle-inner"></div>
</div>

<script>
function add_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.add('show')  
}

function remove_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.remove('show')
}

function sidebar_toggle() {
    let sidebar_toggle = document.querySelector('.sidebar-toggle')
    let sidebar = document.querySelector('.book-sidebar')
    let content = document.querySelector('.off-canvas-content')
    if (sidebar_toggle.classList.contains('extend')) { // show
        sidebar_toggle.classList.remove('extend')
        sidebar.classList.remove('hide')
        content.classList.remove('extend')
    }
    else { // hide
        sidebar_toggle.classList.add('extend')
        sidebar.classList.add('hide')
        content.classList.add('extend')
    }
}
</script>

  <div class="off-canvas-content">
    <div class="columns">
      <div class="column col-10 col-lg-12">
        <div class="book-navbar">
          <!-- For Responsive Layout -->

<header class="navbar">
  <section class="navbar-section">
    <a onclick="open_sidebar()">
      <i class="icon icon-menu"></i>
    </a>
  </section>
</header>

        </div>
        <div class="book-content">
          <div class="book-post">
  <h1 id="week-9b-recommender-systems"><a class="markdownIt-Anchor" href="#week-9b-recommender-systems"></a> Week 9b - Recommender Systems</h1>
<h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2>
<ul>
<li>Two motivations for talking about recommender systems
<ul>
<li>Important application of ML systems</li>
<li>Share some big ideas in machine learning
<ul>
<li>like, learn what features to use.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="problem-formulation"><a class="markdownIt-Anchor" href="#problem-formulation"></a> Problem Formulation</h2>
<ul>
<li>Example: Predicting movie ratings
<ul>
<li>
<img src="https://i.imgur.com/YF4H0KX.jpg" style="width:500px" />
</li>
<li>The users have already rated some of movies. And we want to know that if they like the movies they haven’t rated.</li>
<li>To simplify this example, we set that, users rates movies using from zero to five stars.</li>
<li>Notations:
<ul>
<li>\(n_u\) = no. users</li>
<li>\(n_m\) = no. movies
<ul>
<li>In this case, \(n_u = 4, n_m = 5\)</li>
</ul>
</li>
<li>\(r(i,j)\) = 1 if user <strong>j</strong> has rated movie <strong>i</strong> (o otherwise).</li>
<li>\(y^{ (i,j) }\) = rating given by user <strong>j</strong> to movie <strong>i</strong> (defined only if \(r(i,j)=1\).</li>
</ul>
</li>
<li>For each user \(j\), learn a parameter \(\theta^{ (j) } \in \mathbb{ R }^3\). Predict user \(j\) as rating movie \(i\) with \((\theta^{ (j) })<sup>T(x</sup>{ (i) })\) stars.
<ul>
<li>More generally, \(\theta^{ (j) } \in \mathbb{ R }^{ n+1 }\), included the bias term \(\theta_0^{ (j) }\). <strong>n</strong> is the number of features.</li>
<li>In this example, we got <strong>romance</strong> and <strong>action</strong> two features for every movie.</li>
</ul>
</li>
<li>Extra notaions:
<ul>
<li>\(\theta^{ (j) }\) = parameter vector for user \(j\)
<ul>
<li>\(\theta^{ (1) }\) is for Alice</li>
</ul>
</li>
<li>\(x^{ (i) }\) = feature vector for movie \(i\)
<ul>
<li>For movie “Love at Last”: \(x^{ (1) } = \begin {bmatrix}1 \ 0.9 \ 0\end {bmatrix}\)</li>
</ul>
</li>
<li>\(m^{ (j) }\) = no. of movies rated by user \(j\).</li>
</ul>
</li>
<li>For user \(j\), movie \(i\), predicted rating: \((\theta^{ (j) })<sup>T(x</sup>{ (i) })\)</li>
<li>So if  \(\theta^{ (1) } = \begin {bmatrix}0 \ 5 \ 0\end {bmatrix}\). Check the movie “Cute puppies of love”:  \(x^{ (3) } = \begin {bmatrix}1 \ 0.99 \ 0\end {bmatrix}\), then \((\theta^{ (1) })<sup>T(x</sup>{ (3) }) = 5 \times 0.99 = 4.95\), which is pretty good estimate for user Alice.</li>
</ul>
</li>
</ul>
<h3 id="optimization-objective"><a class="markdownIt-Anchor" href="#optimization-objective"></a> Optimization objective</h3>
<ul>
<li>To learn \(\theta^{ (j) }\) ( parameter for user \(j\) ): \[\underset{ \theta^{ (j) } }{ \text{ min } } \frac{ 1 }{ 2 } \sum_{ i: r(i,j) = 1 }((\theta^{ (j) })<sup>Tx</sup>{ (i) } - y^{ (i,j) })^2 + \frac{ \lambda }{ 2 }\sum_{ k=1 }<sup>n(\theta_k</sup>{ (j) })^2\]
<ul>
<li>Compare to the linear regression, we just get rid of the \(\frac{ 1 }{ m }\) term.</li>
</ul>
</li>
<li>To learn \(\theta^{ (1) }, \theta^{ (2) }, \ldots, \theta^{ (n_u) }\): \[\underset{ \theta^{ (1) }, \ldots, \theta^{ (n_u) } }{ \text{ min } } \frac{ 1 }{ 2 } \sum_{ j=1 }^{ n_u }\sum_{ i: r(i,j) = 1 }((\theta^{ (j) })<sup>Tx</sup>{ (i) } - y^{ (i,j) })^2 + \frac{ \lambda }{ 2 }\sum_{ j=1 }^{ n_u }\sum_{ k=1 }<sup>n(\theta_k</sup>{ (j) })^2\]</li>
<li>Gradient descent update: \[\begin {aligned}<br />
\theta_k^{ (j) } &amp;:= \theta_k^{ (j) } - \alpha \sum_{ i: r(i,j) = 1 }((\theta^{ (j) })^T x^{ (i) } - y^{ (i,j) }) x_k^{ (i) } \ (\text{ for }\ k = 0) \<br />
\theta_k^{ (j) } &amp;:= \theta_k^{ (j) } - \alpha \Big(\sum_{ i: r(i,j) = 1 }((\theta^{ (j) })^T x^{ (i) } - y^{ (i,j) }) x_k^{ (i) } + \lambda \theta_k^{ (j) }\Big) \ (\text{ for }\ k \ne 0)<br />
\end {aligned}\]</li>
</ul>
<h2 id="collaborative-filtering"><a class="markdownIt-Anchor" href="#collaborative-filtering"></a> Collaborative Filtering</h2>
<ul>
<li>
<p>One of the property of collaborative filtering is: <strong>feature learning</strong>, which is an algorithm that can learn for itself what features to use.</p>
</li>
<li>
<p>Let’s make a different assumption:</p>
<ul>
<li>
<img src="https://i.imgur.com/wcqYMwY.jpg" style="width:500px" />
</li>
<li>We don’t know the movies’ categories, romance or action.</li>
<li>But we know our users’ hobbits. Alice and Bob like romance movie, and Carol and Dave like action movie. So we generated the \(\theta{ (j) }\) vectors above.</li>
<li>So, to movie “Love at last” (\(x^{ (1) }\)) and User “Alice” (\(\theta^{ (1) }\)), we should get \((\theta^{ (1) })<sup>Tx</sup>{ (1) } = 5\), and to the rest users: \((\theta^{ (2) })<sup>Tx</sup>{ (1) } = 5, (\theta^{ (3) })<sup>Tx</sup>{ (1) } = 0, (\theta^{ (4) })<sup>Tx</sup>{ (1) } = 0\), then we can guess: \[x^{ (1) } = \begin {bmatrix}1 \ 1.0 \ 0.0 \end {bmatrix}\]</li>
</ul>
</li>
</ul>
<h3 id="optimization-algorithm"><a class="markdownIt-Anchor" href="#optimization-algorithm"></a> Optimization Algorithm</h3>
<ul>
<li>Given \(\theta^{ (1) }, \ldots, \theta^{ (n_u) }\), to learn \(x^{ (i) }\) : \[\underset{ x^{ (i) } }{ \text{ min } } \frac{ 1 }{ 2 } \sum_{ i: r(i,j) = 1 }((\theta^{ (j) })<sup>Tx</sup>{ (i) } - y^{ (i,j) })^2 + \frac{ \lambda }{ 2 }\sum_{ k=1 }<sup>n(x_k</sup>{ (j) })^2\]</li>
<li>Given \(\theta^{ (1) }, \ldots, \theta^{ (n_u) }\), to learn \( x^{ (1) }, x^{ (2) }, \ldots, x^{ (n_m) } \): \[\underset{ x^{ (1) }, \ldots, x^{ (n_m) } }{ \text{ min } } \frac{ 1 }{ 2 } \sum_{ i=1 }^{ n_m }\sum_{ j: r(i,j) = 1 }((\theta^{ (j) })<sup>Tx</sup>{ (i) } - y^{ (i,j) })^2 + \frac{ \lambda }{ 2 }\sum_{ i=1 }^{ n_m }\sum_{ k=1 }<sup>n(x_k</sup>{ (i) })^2\]</li>
</ul>
<h3 id="combine-the-algorithms-together"><a class="markdownIt-Anchor" href="#combine-the-algorithms-together"></a> Combine the Algorithms Together</h3>
<ul>
<li>In the past two part we know:
<ul>
<li>Given \(\theta^{ (1) }, \ldots, \theta^{ (n_u) }\), can estimate \(x^{ (1) }, x^{ (2) }, \ldots, x^{ (n_m) }\),</li>
<li>Given \(x^{ (1) }, x^{ (2) }, \ldots, x^{ (n_m) }\), can estimate \(\theta^{ (1) }, \ldots, \theta^{ (n_u) }\).</li>
</ul>
</li>
<li>But which goes first?</li>
<li>In some ideas, we can random initialize \(\Theta\)s to get \(X\), then use \(X\) to get a better \(\Theta\). In this back and forth process to estimate <strong>theta</strong> and <strong>x</strong>.</li>
</ul>
<h3 id="collaborative-filtering-algorithm"><a class="markdownIt-Anchor" href="#collaborative-filtering-algorithm"></a> Collaborative Filtering Algorithm</h3>
<ul>
<li>
<p>To avoid back and forth process, we come out this new algorithm, to minimize \(x^{ (1) }, x^{ (2) }, \ldots, x^{ (n_m) }\) and \(\theta^{ (1) }, \ldots, \theta^{ (n_u) }\) simultaneously: \[ J(x^{ (1) },  \ldots, x^{ (n_m) }, \theta^{ (1) }, \ldots, \theta^{ (n_u) }) =  \frac{ 1 }{ 2 } \sum_{ (i,j): r(i,j) = 1 }((\theta^{ (j) })<sup>Tx</sup>{ (i) } - y^{ (i,j) })^2 + \frac{ \lambda }{ 2 }\sum_{ i=1 }^{ n_m }\sum_{ k=1 }<sup>n(x_k</sup>{ (i) })^2 + \frac{ \lambda }{ 2 }\sum_{ j=1 }^{ n_u }\sum_{ k=1 }<sup>n(\theta_k</sup>{ (j) })^2 \]</p>
</li>
<li>
<p>\[\underset{ x^{ (1) }, \ldots, x^{ (n_m) }, \theta^{ (1) }, \ldots, \theta^{ (n_u) } }{ \text{ min } }\ J(x^{ (1) },  \ldots, x^{ (n_m) }, \theta^{ (1) }, \ldots, \theta^{ (n_u) })\]</p>
<ul>
<li>Notice, \(\displaystyle\sum_{ (i,j): r(i,j) = 1 }((\theta^{ (j) })<sup>Tx</sup>{ (i) } - y^{ (i,j) })^2\)
<ul>
<li>= \(\displaystyle\sum_{ i=1 }^{ n_m }\sum_{ j: r(i,j) = 1 }((\theta^{ (j) })<sup>Tx</sup>{ (i) } - y^{ (i,j) })^2\)</li>
<li>= \(\displaystyle\sum_{ j=1 }^{ n_u }\sum_{ i: r(i,j) = 1 }((\theta^{ (j) })<sup>Tx</sup>{ (i) } - y^{ (i,j) })^2\)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>For convenience, we get rid of \(\theta_0^{ (j) }\) and \(x_0^{ (i) }\), so \(x^{ (i) } \in \mathbb{ R }^n\) and \(\theta^{ (i) } \in \mathbb{ R }^n\).</p>
<ul>
<li>The reason we do this, is that we are now learning all the features, so, there is no need to hard code the feature which always equals one.</li>
</ul>
</li>
</ul>
<h3 id="steps-of-collaborative-filtering-algorithm"><a class="markdownIt-Anchor" href="#steps-of-collaborative-filtering-algorithm"></a> Steps of Collaborative Filtering Algorithm</h3>
<ol>
<li>Initialize \(x^{ (1) },  \ldots, x^{ (n_m) }, \theta^{ (1) }, \ldots, \theta^{ (n_u) }\) to small random values.</li>
<li>Minimize \(J(x^{ (1) },  \ldots, x^{ (n_m) }, \theta^{ (1) }, \ldots, \theta^{ (n_u) })\) use gradient descent (or an advanced optimization algorithm). E.g. for every \(j = 1, \ldots, n_u, i= 1, \ldots, n_m\): \[\begin {aligned}<br />
x_k^{ (j) } &amp;:= x_k^{ (j) } - \alpha \Big(\sum_{ j: r(i,j) = 1 }((\theta^{ (j) })^T x^{ (i) } - y^{ (i,j) }) \theta_k^{ (j) } + \lambda x_k^{ (i) }\Big) \<br />
\theta_k^{ (j) } &amp;:= \theta_k^{ (j) } - \alpha \Big(\sum_{ i: r(i,j) = 1 }((\theta^{ (j) })^T x^{ (i) } - y^{ (i,j) }) x_k^{ (i) } + \lambda \theta_k^{ (j) }\Big)<br />
\end {aligned}\]
<ul>
<li>We already get rid of \(\theta_0^{ (j) }\) and \(x_0^{ (i) }\), so no special cases.</li>
</ul>
</li>
<li>For a user with parameters \(theta\) and a movie with (learned) features \(x\), predict a star rating of \(\theta^T x\).</li>
</ol>
<h2 id="low-rank-matrix-factorization"><a class="markdownIt-Anchor" href="#low-rank-matrix-factorization"></a> Low Rank Matrix Factorization</h2>
<h3 id="vectorization-low-rank-matrix-factorization"><a class="markdownIt-Anchor" href="#vectorization-low-rank-matrix-factorization"></a> Vectorization: Low Rank Matrix Factorization</h3>
<ul>
<li>
<img src="https://i.imgur.com/rTKruWo.jpg" style="width:400px" />
</li>
<li>Group the data with matrix, we get: \[Y = \begin {bmatrix}5 &amp; 5 &amp; 0 &amp; 0 \ 5 &amp; ? &amp; ? &amp; 0 \ ? &amp; 4 &amp; 0 &amp; ? \ 0 &amp; 0 &amp; 5 &amp; 4 \ 0 &amp; 0 &amp; 5 &amp; 0 \end {bmatrix}\]</li>
<li>And the predicted ratings algorithms can be writen as: \[\begin {bmatrix} (\theta^{ (1) })<sup>T(x</sup>{ (1) }) &amp; (\theta^{ (2) })<sup>T(x</sup>{ (1) }) &amp; \ldots &amp; (\theta^{ (n_u) })<sup>T(x</sup>{ (1) }) \ (\theta^{ (1) })<sup>T(x</sup>{ (2) }) &amp; (\theta^{ (2) })<sup>T(x</sup>{ (2) }) &amp; \ldots &amp; (\theta^{ (n_u) })<sup>T(x</sup>{ (2) }) \<br />
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \<br />
(\theta^{ (1) })<sup>T(x</sup>{ (n_m) }) &amp; (\theta^{ (2) })<sup>T(x</sup>{ (n_m) }) &amp; \ldots &amp; (\theta^{ (n_u) })<sup>T(x</sup>{ (n_m) }) \ \end {bmatrix}\]
<ul>
<li>The index \((i, j)\) corresponds to the rating that we predict user <strong>j</strong> give to movie <strong>i</strong>, which is \((\theta^{ (j) })<sup>T(x</sup>{ (i) })\)</li>
</ul>
</li>
<li>There is a vectorized way to write this:
<ul>
<li>In particular, if we define the matrix: \[X = \begin {bmatrix} - &amp; (x^{ (1) })^T &amp; - \ &amp; \vdots &amp; \ - &amp; (x^{ (n_m) } &amp; - \end {bmatrix},\ \Theta = \begin {bmatrix} - &amp; (\theta^{ (1) })^T &amp; - \ &amp; \vdots &amp; \ - &amp; (\theta^{ (n_u) } &amp; - \end {bmatrix}\]</li>
<li>Then we can write the predicted ratings as \(X\Theta^T\).</li>
<li>And this algorithm called <strong>Low Rank Matrix Factorization</strong>(矩阵分解).
<ul>
<li>将高维矩阵映射为两个低维矩阵.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="finding-related-movies"><a class="markdownIt-Anchor" href="#finding-related-movies"></a> Finding Related Movies</h3>
<ul>
<li>For each product \(i\), we learn a feature vector \(x^{ (i) } \in \mathbb{ R }^n\).
<ul>
<li>Like, x_1 = romance, x_2 = action, …</li>
</ul>
</li>
<li>How to find movies \(j\) related to movie \(i\) ?
<ul>
<li>If the value of \(\lVert x^{ (i) } - x^{ (j) } \rVert\) is small.</li>
</ul>
</li>
<li>So, if we want 5 most movies to movie \(i\):
<ul>
<li>Just find the 5 movies \(j\) with the smallest \(\lVert x^{ (i) } - x^{ (j) } \rVert\).</li>
</ul>
</li>
</ul>
<h3 id="implementational-detail-mean-normalization"><a class="markdownIt-Anchor" href="#implementational-detail-mean-normalization"></a> Implementational Detail: Mean Normalization</h3>
<ul>
<li>Say, we have a user <strong>Eve</strong> who haven’t rated any movie yet. So,
<ul>
<li>
<img src="https://i.imgur.com/JEJffVt.jpg" style="width:500px" />
</li>
</ul>
</li>
<li>We can initial all of the rating values to <strong>0</strong>. Then to minimize cost <strong>J</strong>.
<ul>
<li>Since all of the ratings are <strong>0</strong>, the other terms are irrelevant. We just need to minimize \(\frac{ \lambda }{ 2 }\sum_{ j=1 }^{ n_u }\sum_{ k=1 }<sup>n(\theta_k</sup>{ (j) })^2\), which equals \(\frac{ \lambda }{ 2 }\Big[(\theta^{ (5) }_1)^2 + (\theta^{ (5) }_2)^2\Big]\). We can easily get \(\theta^{ (5) } = \begin {bmatrix}0 \ 0\end {bmatrix}\). But this conclusion doesn’t help, we need a better way to do this.</li>
</ul>
</li>
</ul>
<h4 id="mean-normalization"><a class="markdownIt-Anchor" href="#mean-normalization"></a> Mean Normalization</h4>
<ul>
<li>Calculate all of the movies’ average rating, then set their means to zero.</li>
<li>We know that \[Y = \begin {bmatrix}5 &amp; 5 &amp; 0 &amp; 0 &amp; ? \ 5 &amp; ? &amp; ? &amp; 0 &amp; ? \ ? &amp; 4 &amp; 0 &amp; ? &amp; ? \ 0 &amp; 0 &amp; 5 &amp; 4 &amp; ? \ 0 &amp; 0 &amp; 5 &amp; 0 &amp; ? \end {bmatrix}\]</li>
<li>Then \[\mu = \begin {bmatrix} 2.5 \ 2.5 \ 2 \ 2.25 \ 1.25 \end {bmatrix} \to Y = \begin {bmatrix}2.5 &amp; 2.5 &amp; -2.5 &amp; -2.5 &amp; ? \ 2.5 &amp; ? &amp; ? &amp; -2.5 &amp; ? \ ? &amp; 2 &amp; -2 &amp; ? &amp; ? \ -2.25 &amp; -2.25 &amp; 2.75 &amp; 1.75 &amp; ? \ -1.25 &amp; -1.25 &amp; 3.75 &amp; -1.25 &amp; ? \end {bmatrix} \]</li>
<li>And we can initialize \[\theta^{ (5) } = \begin {bmatrix} 0 \ 0 \ 0 \ 0 \ 0 \end {bmatrix}\]</li>
<li>This makes sense. Because, if Eve hasn’t rated any movies, predict the average rating should be the best.</li>
<li>Don’t forgot that, for predicting user \(j\) on movie \(i\), we should plus the mean: \[(\theta^{ (j) })(x^{ (i) })+\mu_i\]</li>
</ul>

</div>


  <div class="book-comments">
    




  </div>



<script src="/js/book-post.js"></script>

        </div>
      </div>
      <div class="column col-2 hide-lg">
        <div class="book-post-info">
  
    <div class="book-post-meta">

  <div class="author">

    <!-- Author image -->
    <div class="author-img">
      
        <figure
          class="avatar avatar-lg"
          data-initial="E"
          style="background-color: #3b4351;">
        </figure>
      
    </div>

    <!-- Author title -->
    <div class="author-title">
      <div>Eric Yang</div>
      <div>2020-04-13</div>
    </div>
  </div>

  

  <div class="divider"></div>
</div>
  

  <div class="book-tocbot">
</div>
<div class="book-tocbot-menu">
  <a class="book-toc-expand" onclick="expand_toc()">Expand all</a>
  <a onclick="go_top()">Back to top</a>
  <a onclick="go_bottom()">Go to bottom</a>
</div>


<script src="/js/book-toc.js"></script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "\\(", right: "\\)", display: false},
                {left: "\\[", right: "\\]", display: true}
            ]
        });
    });
</script>

</div>

      </div>
    </div>
  </div>
  
  <a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>

</body>
</html>


<script src="/js/book.js"></script>
