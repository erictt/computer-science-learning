<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">

    
      <link rel="icon" href="/favicon.png" />
    

    <title>
        
          stanford-machine-learning/week-7 - Eric&#39;s CS Notes
        
    </title>

    <!-- Spectre.css framework -->
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-exp.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-icons.min.css">

    <!-- theme css & js -->
    
<link rel="stylesheet" href="/css/book.css">

    
<script src="/js/book.js"></script>


    <!-- tocbot -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css">
    
    <!-- katex -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">

    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/zooming/2.1.1/zooming.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    const zooming = new Zooming()
    zooming.listen('.book-content img')
})
</script>

<meta name="generator" content="Hexo 4.2.0"></head>

<body>

<div class="book-container">
  <div class="book-sidebar">
    <div class="book-brand">
  <a href="/">
    <img src="/favicon.png">
    <span>ERIC&#39;S CS NOTES</span>
  </a>
</div>
    <div class="book-menu">
  <h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2>
<ul>
<li><a href="/README/index.html">Introduction</a></li>
<li><a href="/SYMBOLS/index.html">Symbols of Mathematics</a></li>
</ul>
<!--
## Stanford Statistical Learning

* [Chapter 2 - Overview of Statistical Learning](/statistical-learning/chapter-2/index.html)
* Chapter 3 - Linear Regression
* Chapter 4 - Classification
* Chapter 5 - Resampling Methods
* Chapter 6 - Linear Model Selection and Regularization
* Chapter 7 - Moving Beyond Linearity
* Chapter 8 - Tree-Based Methods
* Chapter 9 - Support Vector Machines
* Chapter 10 - Unsupervised Learning
-->
<h2 id="introduction-to-probability"><a class="markdownIt-Anchor" href="#introduction-to-probability"></a> Introduction to Probability</h2>
<ul>
<li><a href="/introduction-to-probability/unit-1/index.html">Unit 1: Probability models and axioms</a></li>
<li><a href="/introduction-to-probability/unit-2/index.html">Unit 2: Conditioning and independence</a></li>
<li><a href="/introduction-to-probability/unit-3/index.html">Unit 3: Counting</a></li>
</ul>
<!--
* [Unit 4: Discrete random variables](/introduction-to-probability/unit-4/index.html)
* [Unit 5: Continuous random variables](/introduction-to-probability/unit-5/index.html)
* [Unit 6: Further topics on random variables](/introduction-to-probability/unit-6/index.html)
* [Unit 7: Bayesian inference](/introduction-to-probability/unit-7/index.html)
* [Unit 8: Limit theorems and classical statistics](/introduction-to-probability/unit-8/index.html)
* [Unit 9: Bernoulli and Poisson processes](/introduction-to-probability/unit-9/index.html)
* [Unit 10: Markov chains](/introduction-to-probability/unit-10/index.html)
-->
<h2 id="multivariable-calculus"><a class="markdownIt-Anchor" href="#multivariable-calculus"></a> Multivariable Calculus</h2>
<ul>
<li><a href="/multivariable-calculus/unit-1/index.html">Unit 1: Thinking about multivariable functions</a></li>
<li><a href="/multivariable-calculus/unit-2/index.html">Unit 2: Derivatives of multivariable functions</a></li>
</ul>
<!--
* [Unit 3: Applications of multivariable derivatives](/multivariable-calculus/unit-3/index.html)
* [Unit 4: Integrating multivariable functions](/multivariable-calculus/unit-4/index.html)
* [Unit 5: Green's, Stokes', and the divergence theorems](/multivariable-calculus/unit-5/index.html)
-->
<h2 id="algorithms-part-ii"><a class="markdownIt-Anchor" href="#algorithms-part-ii"></a> Algorithms: Part II</h2>
<ul>
<li><a href="/algorithms-2/week-1/index.html">Week 1 - Undirected Graph &amp; Directed Graph</a></li>
<li><a href="/algorithms-2/week-3/index.html">Week 3 - Maximum Flow and Minimum Cut &amp; Radix Sort</a></li>
<li><a href="/algorithms-2/week-4/index.html">Week 4 - Tries &amp; Substring Search</a></li>
</ul>
<h2 id="algorithms-part-i"><a class="markdownIt-Anchor" href="#algorithms-part-i"></a> Algorithms: Part I</h2>
<ul>
<li><a href="/algorithms-1/week-1/index.html">Week 1 - Union-Find &amp; Analysis of Algorithms</a></li>
<li><a href="/algorithms-1/week-2/index.html">Week 2 - Stacks and Queues &amp; Elementary Sorts</a></li>
<li><a href="/algorithms-1/week-3/index.html">Week 3 - Mergesort &amp; Quicksort</a></li>
<li><a href="/algorithms-1/week-4/index.html">Week 4 - Priority Queues &amp; Elementary Symbols</a></li>
<li><a href="/algorithms-1/week-5/index.html">Week 5 - Balanced Search Trees</a></li>
<li><a href="/algorithms-1/week-6/index.html">Week 6 - Hash Tables</a></li>
</ul>
<h2 id="introduction-to-software-design-and-architecture"><a class="markdownIt-Anchor" href="#introduction-to-software-design-and-architecture"></a> Introduction to Software Design and Architecture</h2>
<ul>
<li><a href="/introduction-to-software-design-and-architecture/design-pattern/index.html">Design Pattern</a></li>
</ul>
<h2 id="calculus-two-sequences-and-series"><a class="markdownIt-Anchor" href="#calculus-two-sequences-and-series"></a> Calculus Two: Sequences and Series</h2>
<ul>
<li><a href="/calculus-two/week-1/index.html">Week 1 - Sequences</a></li>
<li><a href="/calculus-two/week-2/index.html">Week 2 - Series</a></li>
<li><a href="/calculus-two/week-3/index.html">Week 3 - Convergence Tests</a></li>
<li><a href="/calculus-two/week-4/index.html">Week 4 - Alternating Series</a></li>
<li><a href="/calculus-two/week-5/index.html">Week 5 - Power Series</a></li>
<li><a href="/calculus-two/week-6/index.html">Week 6 - Taylor Series</a></li>
</ul>
<h2 id="laff-linear-algebra"><a class="markdownIt-Anchor" href="#laff-linear-algebra"></a> LAFF Linear Algebra</h2>
<ul>
<li><a href="/laff-linear-algebra/week-1/index.html">Week 1 - Vectors in Linear Algebra</a></li>
<li><a href="/laff-linear-algebra/week-2/index.html">Week 2 - Linear Transformations and Matrices</a></li>
<li><a href="/laff-linear-algebra/week-3/index.html">Week 3 - Matrix-Vector Operations</a></li>
<li><a href="/laff-linear-algebra/week-4/index.html">Week 4 - Matrix-Vector to Matrix-Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-5/index.html">Week 5 - Matrix- Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-6/index.html">Week 6 - Gaussian Elimination</a></li>
<li><a href="/laff-linear-algebra/week-7/index.html">Week 7 - More Gaussian Elimination and Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-8/index.html">Week 8 - More on Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-9/index.html">Week 9 - Vector Spaces</a></li>
<li><a href="/laff-linear-algebra/week-10/index.html">Week 10 - Vector Spaces, Orthogonality, and Linear Least-Squares</a></li>
<li><a href="/laff-linear-algebra/week-11/index.html">Week 11 - Orthogonal Projection, Low Rank Approximation, and Orthogonal Bases</a></li>
<li><a href="/laff-linear-algebra/week-12/index.html">Week 12 - Eigenvalues and Eigenvectors</a></li>
</ul>
<h2 id="stanford-machine-learning"><a class="markdownIt-Anchor" href="#stanford-machine-learning"></a> Stanford Machine Learning</h2>
<ul>
<li><a href="/stanford-machine-learning/week-1/index.html">Week 1 - Introduction</a></li>
<li><a href="/stanford-machine-learning/week-2/index.html">Week 2 - Linear Regression with Multiple Variables</a></li>
<li><a href="/stanford-machine-learning/week-3/index.html">Week 3 - Logistic Regression &amp; Regularization</a></li>
<li><a href="/stanford-machine-learning/week-4/index.html">Week 4 - Neural Networks: Representation</a></li>
<li><a href="/stanford-machine-learning/week-5/index.html">Week 5 - Neural Networks: Learning</a></li>
<li><a href="/stanford-machine-learning/week-6a/index.html">Week 6a - Advice for Applying Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-6b/index.html">Week 6b - Machine Learning System Design</a></li>
<li><a href="/stanford-machine-learning/week-7/index.html">Week 7 - Support Vector Machines</a></li>
<li><a href="/stanford-machine-learning/week-8/index.html">Week 8 - Unsupervised Learning &amp; Dimensionality Reduction</a></li>
<li><a href="/stanford-machine-learning/week-9a/index.html">Week 9a - Anomaly Detection</a></li>
<li><a href="/stanford-machine-learning/week-9b/index.html">Week 9b - Recommender Systems</a></li>
<li><a href="/stanford-machine-learning/week-10/index.html">Week 10 - Large Scale Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-11/index.html">Week 11 - Application Example: Photo OCR</a></li>
</ul>
<h2 id="calculus-one"><a class="markdownIt-Anchor" href="#calculus-one"></a> Calculus One</h2>
<ul>
<li><a href="/calculus-one/week-2-3/index.html">Week 2-3 - Functions &amp; Limits</a></li>
<li><a href="/calculus-one/week-4/index.html">Week 4 - The Beginning of Derivatives</a></li>
<li><a href="/calculus-one/week-5/index.html">Week 5 - Techniques of Differentiation</a></li>
<li><a href="/calculus-one/week-6/index.html">Week 6 - Chain Rule</a></li>
<li><a href="/calculus-one/week-7/index.html">Week 7 - Derivatives of Trigonometric Functions</a></li>
<li><a href="/calculus-one/week-8/index.html">Week 8 - Derivatives in the Real World</a></li>
<li><a href="/calculus-one/week-9/index.html">Week 9 - Optimization</a></li>
<li><a href="/calculus-one/week-10/index.html">Week 10 - Linear Approximation</a></li>
<li><a href="/calculus-one/week-11-12/index.html">Week 11-12 - Antidifferentiation &amp; Integration</a></li>
<li><a href="/calculus-one/week-13/index.html">Week 13 - Fundamental Theorem of Calculus</a></li>
<li><a href="/calculus-one/week-14/index.html">Week 14 - Substitution Rule</a></li>
<li><a href="/calculus-one/week-15/index.html">Week 15 - Techniques of Integration</a></li>
<li><a href="/calculus-one/week-16/index.html">Week 16 - Applications of Integration</a></li>
</ul>
<h2 id="computational-thinking"><a class="markdownIt-Anchor" href="#computational-thinking"></a> Computational Thinking</h2>
<ul>
<li><a href="/computational-thinking/lecture-1/index.html">Lecture 1 - Optimization and Knapsack Problem</a></li>
<li><a href="/computational-thinking/lecture-2/index.html">Lecture 2 - Decision Trees and Dynamic Programming</a>
<ul>
<li><a href="/computational-thinking/lecture-2-powerset/index.html">Exercise: Power Set Function</a></li>
</ul>
</li>
<li><a href="/computational-thinking/lecture-3/index.html">Lecture 3 - Graphs</a></li>
<li><a href="/computational-thinking/lecture-4-5/index.html">Lecture 4-5 - Plotting</a></li>
<li><a href="/computational-thinking/lecture-6-7/index.html">Lecture 6-7 - Stochastic Programs &amp; Inferential Statistics</a></li>
<li><a href="/computational-thinking/lecture-8/index.html">Lecture 8 - Monte Carlo Simulation</a></li>
<li><a href="/computational-thinking/lecture-9/index.html">Lecture 9 - Sampling and Standard Error</a></li>
<li><a href="/computational-thinking/lecture-10-11/index.html">Lecture 10-11 - Experimental Data</a></li>
<li><a href="/computational-thinking/lecture-12/index.html">Lecture 12 - Machine Learning</a></li>
<li><a href="/computational-thinking/lecture-13/index.html">Lecture 13 - Statistical Abuses</a></li>
</ul>
<h2 id="effective-thinking-through-mathematics"><a class="markdownIt-Anchor" href="#effective-thinking-through-mathematics"></a> Effective Thinking Through Mathematics</h2>
<ul>
<li><a href="/effective-thinking-through-mathematics/note/index.html">Note</a></li>
<li><a href="/effective-thinking-through-mathematics/week-4-telling-the-story-of-infinity/index.html">Week 4 (/Telling the Story of Infinity)</a></li>
<li><a href="/effective-thinking-through-mathematics/week-5-telling-the-story-of-the-euler-circuit-theorem/index.html">Week 5 (/Telling the Story of Euler Circuit Theorem)</a></li>
</ul>
<h2 id="cs50-introduction-to-computer-science"><a class="markdownIt-Anchor" href="#cs50-introduction-to-computer-science"></a> CS50 Introduction to Computer Science</h2>
<ul>
<li><a href="/cs50/week-1/index.html">Week 1 - C</a></li>
<li><a href="/cs50/week-2/index.html">Week 2 - Arrays</a></li>
<li><a href="/cs50/week-3/index.html">Week 3 - Algorithms</a></li>
<li><a href="/cs50/week-4/index.html">Week 4 - Memory</a></li>
<li><a href="/cs50/week-5/index.html">Week 5 - Data Structures</a></li>
<li><a href="/cs50/week-6/index.html">Week 6 - HTTP</a></li>
<li><a href="/cs50/week-7-10/index.html">Week 7-10 - Machine Learning/Python/SQL/Javascript</a></li>
</ul>

</div>


<script src="/js/book-menu.js"></script>

  </div>

  <div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseover="add_inner()" onmouseleave="remove_inner()">
  <div class="sidebar-toggle-inner"></div>
</div>

<script>
function add_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.add('show')  
}

function remove_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.remove('show')
}

function sidebar_toggle() {
    let sidebar_toggle = document.querySelector('.sidebar-toggle')
    let sidebar = document.querySelector('.book-sidebar')
    let content = document.querySelector('.off-canvas-content')
    if (sidebar_toggle.classList.contains('extend')) { // show
        sidebar_toggle.classList.remove('extend')
        sidebar.classList.remove('hide')
        content.classList.remove('extend')
    }
    else { // hide
        sidebar_toggle.classList.add('extend')
        sidebar.classList.add('hide')
        content.classList.add('extend')
    }
}
</script>

  <div class="off-canvas-content">
    <div class="columns">
      <div class="column col-10 col-lg-12">
        <div class="book-navbar">
          <!-- For Responsive Layout -->

<header class="navbar">
  <section class="navbar-section">
    <a onclick="open_sidebar()">
      <i class="icon icon-menu"></i>
    </a>
  </section>
</header>

        </div>
        <div class="book-content">
          <div class="book-post">
  <h1 id="week-7-support-vector-machinessvm"><a class="markdownIt-Anchor" href="#week-7-support-vector-machinessvm"></a> Week 7 - Support Vector Machines(SVM)</h1>
<ul>
<li>Compared to both logistic regression and neural networks, <strong>SVM</strong> sometimes gives a cleaner way of learning non-linear functions</li>
</ul>
<h2 id="optimization-objective"><a class="markdownIt-Anchor" href="#optimization-objective"></a> Optimization Objective</h2>
<ul>
<li>Start with logistic regression, and modify it a bit to get the SVM.</li>
<li>The logistic regression hypothesis is: \[h_{\theta}(x) = \frac{1}{1+e<sup>{-\theta</sup>{T}x} }\]</li>
<li>And the sigmoid activation function looks like:
<ul>
<li>
<img src="https://i.imgur.com/MqVJevd.jpg" style="width:250px" />
</li>
</ul>
</li>
<li>If \(y = 1\), we want \(h_{\theta}(x) \approx 1\), \(\theta^{T}x \gg  0\)</li>
<li>If \(y = 0\), we want \(h_{\theta}(x) \approx 0\), \(\theta^{T}x \ll  0\)</li>
<li>Cost of single example: \[\begin{aligned} &amp; -(y\log{h_{\theta}(x)} + (1 - y)\log{(1 - h_{\theta}(x)}))z \ = &amp;-(y\log{\frac{1}{1+e<sup>{-\theta</sup>{T}x} }} - (1 - y)\log{(1 - \frac{1}{1+e<sup>{-\theta</sup>{T}x} }}))\end{aligned}\]</li>
<li>If \(y = 1\) (want \(\theta^Tx \gg 0\)), then the cost will be: \(-\log\dfrac{1}{1+e^{-z} }\)
<ul>
<li>
<img src="https://i.imgur.com/E8pq3Bn.jpg" style="width:200px" />
</li>
</ul>
</li>
<li>If \(y = 0\) (want \(\theta^Tx \ll 0\)), then the cost will be: \(-\log{(1 - \dfrac{1}{1+e^{-z} }})\)
<ul>
<li>
<img src="https://i.imgur.com/cIqrK0l.jpg" style="width:200px" />
</li>
</ul>
</li>
<li>To build SVM, we redefine the cost functions:
<ul>
<li>If \(y = 1\):
<ul>
<li>
<img src="https://i.imgur.com/nXyoaBy.jpg" style="width:200px" />
</li>
<li>Instead of a curve line, we create two straight lines which acts as an approximation to the logistic regression.</li>
<li>We call this function \(cost_1(z)\).</li>
</ul>
</li>
<li>If \(y = 0\):
<ul>
<li>
<img src="https://i.imgur.com/x3FawPD.jpg" style="width:200px" />
</li>
<li>We call this function \(cost_0(z)\).</li>
</ul>
</li>
<li><strong>How to use formula to represent those two lines?</strong> At least to calculate the cost?
<ul>
<li>Everything Prof Ng said about SVM training was an intuition. The actual SVM training method provided in the <code>svmTrain()</code> function is the SMO method. That method is too complex to be included as part of the course. – from <a href="https://www.coursera.org/learn/machine-learning/discussions/weeks/7/threads/uCyF4elMEeWK_Q7eN25hdw" target="_blank" rel="noopener">Discuss Forms</a></li>
</ul>
</li>
</ul>
</li>
<li><strong>The complete SVM cost function</strong>
<ul>
<li>As a comparison we have logistic regression: \[{\underset{\theta}{\text{min} } } \ - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2\]</li>
<li>Replace the cost function with \(cost_0(z)\) and \(cost_1(z)\), we get: \[\underset{\theta}{\text{min} }\ \frac{1}{m} \sum_{i=1}^m \large[ y<sup>{(i)} cost_1(\theta</sup>Tx^{(i)}) + (1 - y<sup>{(i)}) cost_0(\theta</sup>Tx^{(i)})\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2\]</li>
<li>In convention with SVM notation, we adjust a little bit:
<ul>
<li>Get rid of \(\dfrac{1}{m}\) term.
<ul>
<li>Because \(\frac{1}{m}\) is a constant, so we should still end up with the same optimal value for \(\theta\).</li>
</ul>
</li>
<li>For logistic regression we have:
<ul>
<li>Training data set term <strong>A</strong>: \[- \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large]\]</li>
<li>and Regularization term <strong>B</strong>: \[\frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2\]</li>
<li>To conclude it, we get \(A + \lambda B \)</li>
<li>So \(\lambda\) is the trade-off between training data set and regularization terms.</li>
<li>Instead of using \(A + \lambda B \), In SVM, we rewrite it as \(CA + B\), which <strong>C</strong> is a constant.</li>
<li>We can think of the parameter <strong>C</strong> playing a role similar to \(\frac{1}{\lambda}\).</li>
</ul>
</li>
</ul>
</li>
<li>Overall optimization objective function for the SVM is: \[\underset{\theta}{\text{min} }\ C \sum_{i=1}^m \large[ y<sup>{(i)} \text{cost}_1(\theta</sup>Tx^{(i)}) + (1 - y<sup>{(i)}) \text{cost}_0(\theta</sup>Tx^{(i)})\large] + \frac{1}{2}\sum_{j=1}^n \theta_j^2\]</li>
</ul>
</li>
<li>SVM Hypothesis:
<ul>
<li>Unlike logistic, \(h_{\theta}(x)\) doesn’t give us a probability, instead we get a direct prediction of <strong>1</strong> or <strong>0</strong></li>
<li>\[h_{\theta}(x) = \left{ \begin{array}{rl}<br />
1 &amp; \text{, if } \theta^Tx \ge 0 \<br />
0 &amp; \text{, } \text{otherwise}.<br />
\end{array} \right.\]</li>
</ul>
</li>
</ul>
<h2 id="large-margin-intuition"><a class="markdownIt-Anchor" href="#large-margin-intuition"></a> Large Margin Intuition</h2>
<ul>
<li>Sometimes SVM called <strong>Large Margin Classifier</strong>.</li>
<li>Here are two plots for the cost function:
<ul>
<li>
<img src="https://i.imgur.com/XOGI1HA.jpg" style="width:500px" />
</li>
</ul>
</li>
<li>If we want the cost to be small, then we will need \(z\) to be more that <strong>1</strong> not just <strong>0</strong>. then:
<ul>
<li>If \(y = 1\), we want \(\theta^Tx \ge 1\) (not just \( \ge 0\)).</li>
<li>If \(y = 0\), we want \(\theta^Tx \le -1\) (not just \( \le 0\)).</li>
</ul>
</li>
</ul>
<h3 id="svm-decision-boundary"><a class="markdownIt-Anchor" href="#svm-decision-boundary"></a> SVM Decision Boundary</h3>
<ul>
<li>Use the simplified cost function \(\text{min}_{\theta} = CA+B\)</li>
<li>If <strong>C</strong> is a huge number, like <strong>100,000</strong>, then we will need to make <strong>A</strong> to be very small, best to be 0, and in the same time minimize <strong>B</strong>.
<ul>
<li>Whenever \(y^{(i)} = 1\): \(\theta<sup>Tx</sup>{(i)} \ge 1\).</li>
<li>Whenever \(y^{(i)} = 0\): \(\theta<sup>Tx</sup>{(i)} \le -1\).</li>
<li>\(\underset{\theta}{\text{min} }\ \dfrac{1}{2}\sum_{i=1}<sup>n\theta_j</sup>2\)</li>
</ul>
</li>
<li>Let’s check the result in a <strong>linearly separable case</strong>
<ul>
<li>
<img src="https://i.imgur.com/vAhBiwV.jpg" style="width:200px" />
</li>
<li>The green and magenta lines are functional decision boundaries which could be chosen by logistic regression</li>
<li>The black line, by contrast is chosen by the SVM because of this safety net imposed by the optimization graph</li>
<li>We can see that, there is a large margin between the black line and the training sets  which is called <strong>the margin of the support vector machine</strong>.
<ul>
<li>
<img src="https://i.imgur.com/GZhrXmy.jpg" style="width:200px" />
</li>
</ul>
</li>
<li>In another situation:
<ul>
<li>
<img src="https://i.imgur.com/xHnsmRF.jpg" style="width:200px" />
</li>
<li>We can’t get the result like the black line, since we set <strong>A</strong> to <strong>0</strong>, so the SVM is very sensitive to outliers. And we probably will get the magenta line below. Which lead to another way to fix this: set <strong>C</strong> to a small number, which means ignoring some outliers.
<ul>
<li>
<img src="https://i.imgur.com/K219yh2.jpg" style="width:360px" />
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="mathematics-behind-large-margin-classification"><a class="markdownIt-Anchor" href="#mathematics-behind-large-margin-classification"></a> Mathematics Behind Large Margin Classification</h2>
<h3 id="vector-inter-productions"><a class="markdownIt-Anchor" href="#vector-inter-productions"></a> Vector Inter Productions</h3>
<ul>
<li>
<img src="https://i.imgur.com/1hNa8cz.jpg" style="width:250px" />
</li>
<li>\(u = \begin{bmatrix} u_1 \ u_2 \end{bmatrix}\), \(v = \begin{bmatrix} v_1 \ v_2 \end{bmatrix}\)</li>
<li>length of vector: \(\lVert u \rVert = \sqrt{u_1<sup>2+u_2</sup>2}\)</li>
<li><strong>p</strong> = length of projection of <strong>v</strong> onto <strong>u</strong>.
<ul>
<li><strong>p</strong> is signed, which means it can be negative number.</li>
</ul>
</li>
<li>\(\begin{aligned}u^tv &amp;= p \cdot \lVert u \rVert \ &amp;= u_1v_1 + u_2v_2 \end{aligned}\)</li>
</ul>
<h3 id="svm-decision-boundary-2"><a class="markdownIt-Anchor" href="#svm-decision-boundary-2"></a> SVM Decision Boundary</h3>
<ul>
<li>\(\underset{\theta}{\text{min} }\ \dfrac{1}{2}\sum_{i=1}<sup>n\theta_j</sup>2\)</li>
<li>\(\begin{aligned}\text{s.t. } \theta<sup>Tx</sup>{(i)} &amp;\ge 1\ \text{ if }y^{(i)} = 1 \ \theta<sup>Tx</sup>{(i)} &amp;\le -1\ \text{ if } y^{(i)} = 0\end{aligned}\)</li>
<li><strong>Simplification</strong>: set \(\theta_0 = 0\text{, }n = 2\)(only 2 features). Then:
<ul>
<li>\(\underset{\theta}{\text{min} }\ \dfrac{1}{2}\sum_{i=1}<sup>n\theta_j</sup>2 = \frac{1}{2}(\theta_1<sup>2+\theta_2</sup>2) = \frac{1}{2}(\sqrt{\theta_1<sup>2+\theta_2</sup>2})^2 = \frac{1}{2}{\lVert \theta \rVert}^2\)</li>
<li>\(\theta<sup>Tx</sup>{(i)} = \theta_1x_1^{(i)} + \theta_2x_2^{(i)} = p^{(i)} \cdot {\lVert \theta \rVert}\)
<ul>
<li>
<img src="https://i.imgur.com/GIhb9bM.jpg" style="width:200px" />
</li>
</ul>
</li>
<li>Redefine these functions, we get:
<ul>
<li>\(\begin{aligned}\text{s.t. } p^{(i)} \cdot {\lVert \theta \rVert} &amp;\ge 1\ \text{ if }y^{(i)} = 1 \ p^{(i)} \cdot {\lVert \theta \rVert} &amp;\le -1\ \text{ if } y^{(i)} = 0\end{aligned}\)</li>
<li>where \(p^{(i)}\) is the projection of \(x^{(i)}\) onto the vector \(\theta\).</li>
</ul>
</li>
<li>So we want to maximize \(p^{(i)}\), so \(\lVert \theta \rVert\) can be small.</li>
<li>Note that, \(\theta_0 = 0\), so  the boundary has to pass through the origin (0,0).</li>
<li>Let’s consider the training examples like this:
<ul>
<li>
<img src="https://i.imgur.com/HAJLqSf.jpg" style="width:200px" />
</li>
</ul>
</li>
<li>And draw an option, to see if it’s possible that SVM would choose.
<ul>
<li>
<img src="https://i.imgur.com/fJk14pw.jpg" width = 200 />
</li>
<li>Note that <strong>θ is always at 90 degrees to the decision boundary</strong> (check linear algebra to find out why).</li>
<li>So vector \(\theta\) should be:
<ul>
<li>
<img src="https://i.imgur.com/erYtin7.jpg" style="width:200px" />
</li>
</ul>
</li>
<li>Then we can find the projection \(p^{(i)}\) of \(x^{(i)}\) onto \(\theta\):
<ul>
<li>
<img src="https://i.imgur.com/x2tqGMI.jpg" style="width:200px" />
</li>
<li>We know that we need to make \(p^{(i)} \cdot {\lVert \theta \rVert} \ge 1\), so:
<ul>
<li>if <strong>p</strong> is small, then \({\lVert \theta \rVert}\) should be very large</li>
</ul>
</li>
<li>Similarly, for the negative examples.</li>
</ul>
</li>
<li>But the optimization objective is trying to find a set of parameters where the norm of theta is small. So this doesn’t seem like a good choice.</li>
</ul>
</li>
<li>Let’s another option:
<ul>
<li>
<img src="https://i.imgur.com/B6DVnZs.jpg" style="width:200px" />
</li>
<li>Now if you look at the projection \(p^{(i)}\) of \(x^{(i)}\) onto \(\theta\), we find that <strong>p</strong> becomes large and \({\lVert \theta \rVert}\) can be small.</li>
<li>This is why the SVM choses this hypothesis as the better one, and how we generate the large margin.</li>
<li>
<img src="https://i.imgur.com/7ma5kh3.jpg" style="width:280px" />
</li>
</ul>
</li>
</ul>
</li>
<li>Finally, we did this derivation assuming \(\theta_0 = 0\).
<ul>
<li>It just means we are entertaining decision boundaries that pass through the origins of decision boundaries pass through the origin (0,0).</li>
<li>If you allow \(\theta_0\) to be other values then this simply means you can have decision boundaries which cross through the x and y values at points other than (0,0).</li>
</ul>
</li>
</ul>
<h2 id="kernels-i"><a class="markdownIt-Anchor" href="#kernels-i"></a> Kernels I</h2>
<ul>
<li>Kernels is used to adapt support vector machines in order to develop complex nonlinear classifiers.</li>
<li>Let’s see a example(find a non-linear boundary):
<ul>
<li>
<img src="https://i.imgur.com/7Lm10aX.jpg" style="width:300px" />
</li>
<li>One way to distinguish the positive and negative examples is to come up with a set of complex polynomial features: \[h_{\theta}(x) = \left{ \begin{array}{rl}<br />
1 &amp; \text{, if }\ \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_1 x_2 + \theta_4 x_1^2 + \theta_5 x_2^2 + \cdots \ge 0 \<br />
0 &amp; \text{, otherwise}.<br />
\end{array} \right.\]</li>
<li>Another way is using a new notation to denote \(x_1, x_2, x_1 x_2, x_1^2, x_2^2\) as \(f_1, f_2, f_3, f_4, f_5, \cdots\), so the hypothesis will be: \[h_{\theta}(x) = \left{ \begin{array}{rl}<br />
1 &amp; \text{, if }\ \theta_0 + \theta_1 f_1 + \theta_2 f_2 + \theta_3 f_3 + \theta_4 f_4 + \theta_5 f_5 + \cdots \ge 0 \<br />
0 &amp; \text{, otherwise}.<br />
\end{array} \right.\]</li>
<li>Is there a different/better choice of the features \(f_1, f_2, f_3, \cdots\) ?</li>
</ul>
</li>
</ul>
<h3 id="create-new-features"><a class="markdownIt-Anchor" href="#create-new-features"></a> Create New Features</h3>
<ul>
<li>First, manually pick a few points. In this case, we picked three points, and call them <strong>landmarks</strong> (\(l^{(1)}, l^{(2)}, l^{(3)}\)).
<ul>
<li>
<img src="https://i.imgur.com/sD8ZPt4.jpg" style="width:250px" />
</li>
<li>Later, will explain how to choose \(l^{(i)}\)</li>
</ul>
</li>
<li>Second, define \(f_1, f_2, f_3\) as the similarity between \(x\) and \(l^{(i)}\)(ignore \(x_0\)). Then: \[\begin{aligned}<br />
f_1 &amp;= \text{similarity}(x, l^{(1)}) = \text{exp}(-\frac{ {\lVert x - l^{(1)} \rVert}^2 }{2\sigma^2}) \<br />
f_2 &amp;= \text{similarity}(x, l^{(2)}) = \text{exp}(-\frac{ {\lVert x - l^{(2)} \rVert}^2 }{2\sigma^2}) \<br />
&amp;\vdots<br />
\end{aligned}\]
<ul>
<li>This similarity function is called a <strong>Kernel</strong>. And this <strong>exp</strong> function is a <strong>Gaussian Kernel</strong>.</li>
<li>So, instead of writing similarity between x and l we might write \[f_1 = k(x, l^{(1)})\]</li>
<li>My Note: \(\sigma\): the value of standard deviation. Gaussian Kernel is calculating the value correspond with mean(\(l^{(1)}\)) and \(\sigma\).</li>
</ul>
</li>
</ul>
<h4 id="kernels-and-similarity"><a class="markdownIt-Anchor" href="#kernels-and-similarity"></a> Kernels and Similarity</h4>
<ul>
<li>If \(x \approx l^{(1)}\):
<ul>
<li>\(f_1 \approx \text{exp}(-\frac{0<sup>2}{2\sigma</sup>2}) \approx 1\)</li>
</ul>
</li>
<li>If \(x\) is far from \(l^{(1)}\):
<ul>
<li>\(f_1 \approx \text{exp}(-\frac{(\text{large number})<sup>2}{2\sigma</sup>2}) \approx 0\)</li>
</ul>
</li>
<li>Example:
<ul>
<li>\(l^{(1)} = \begin{bmatrix}3 \ 5\end{bmatrix}\), \(f_1 = \text{exp}(-\dfrac{ {\lVert x - l^{(1)} \rVert}^2 }{2\sigma^2})\).</li>
<li>Plot \(f_1\) vs the kernel function, we get plots like:
<ul>
<li>
<img src="https://i.imgur.com/76D8ujL.jpg" style="width:450px" />
</li>
<li>Notice that when x = [3, 5], then \(f_1 = 1\).</li>
<li>As x moves away from [3,5], then the feature takes on values close to zero. So this measures how close <strong>x</strong> is to this <strong>landmark</strong>.</li>
<li>\(\sigma^2\) is parameter of the Gaussian Kernel, which defines the steepness of the rise around the landmark. We can see that the slop are getting smoother as \(\sigma^2\) are bigger.</li>
</ul>
</li>
</ul>
</li>
<li>Given this definition, what kind of hypothesis we can learn:
<ul>
<li>For example, let’s say we already run the algorithm and got \(\theta_0 = -0.5, \theta_1 = 1, \theta_2 = 1, \theta_3 = 0 \). What happens if we evaluate the <strong>magenta dot</strong> below?
<ul>
<li>
<img src="https://i.imgur.com/jhEkx7n.jpg" style="width:200px" />
</li>
<li>We know that \(x\) is close to \(l^{(1)}\), so \(f_1\) will be close to 1, and \(f_2\) and \(f_3\) and will be close to 0.</li>
<li>So \(\theta_0 + \theta_1 f_1 + \theta_2 f_2 + \theta_3 f_3 = -0.5 + 1 + 0 + 0 = 0.5 \ge 0\). Then we predict \(y = 1\).</li>
<li>After we tried different points, we will eventually get this non-linear boundary:
<ul>
<li>
<img src="https://i.imgur.com/hqZUuLM.jpg" style="width:250px" />
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Next segment will talk about:
<ul>
<li>How we choose the landmarks;</li>
<li>What other kernels we can use (other than the Gaussian Kernel).</li>
</ul>
</li>
</ul>
<h2 id="kernel-ii"><a class="markdownIt-Anchor" href="#kernel-ii"></a> Kernel II</h2>
<h3 id="choosing-the-landmarks"><a class="markdownIt-Anchor" href="#choosing-the-landmarks"></a> Choosing the Landmarks</h3>
<ul>
<li>
<p>Put landmarks as exactly the same locations as the training examples.</p>
</li>
<li>
<p>Then we will get <strong>m</strong> landmarks, which has the same number with the training examples.</p>
</li>
<li>
<p><strong>SVM with Kernels</strong></p>
<ul>
<li>
<p>Given \((x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots, (x^{(m)}, y^{(m)})\),</p>
</li>
<li>
<p>Choose \(l^{(1)} = x^{(1)}, l^{(2)} = x^{(2)}, \ldots, l^{(m)} = x^{(m)}\).</p>
</li>
<li>
<p>For training example (\(x^{(i)}, y^{(i)}\)): \[\begin{aligned}<br />
f_1^{(i)} &amp;= \text{similarity}(x^{(i)}, l^{(1)}) \<br />
f_2^{(i)} &amp;= \text{similarity}(x^{(i)}, l^{(2)}) \<br />
&amp;\vdots \<br />
f_i^{(i)} &amp;= \text{similarity}(x^{(i)}, l^{(i)}) = \text{similarity}(x^{(i)}, x^{(i)}) = \text{exp}(-\frac{0}{2\sigma^2}) = 1\<br />
&amp;\vdots \<br />
f_m^{(i)} &amp;= \text{similarity}(x^{(i)}, l^{(m)})<br />
\end{aligned}\]</p>
</li>
<li>
<p>We got \(f^{(i)} = \begin{bmatrix}f_0^{(i)} \ f_1^{(i)} \ \vdots \ f_m^{(i)} \ \end{bmatrix}\)(\(f_0^{(i)} = 1\)), which are our new training examples.</p>
<ul>
<li>Our training set become a \(m \times m\) matrix, because every sample have <strong>m</strong> features.</li>
<li>\(f^{(i)} \in \mathbb{R}^{m+1}\) (included \(f_0\)).</li>
</ul>
</li>
<li>
<p>Then, our hypothesis will be: Given \(x\), compute features \(f^{(i)} \in \mathbb{R}^{m+1}\). Predict \(y = 1\) if \(\theta^Tf \ge 0 \).</p>
</li>
<li>
<p>And the training: \[\underset{\theta}{\text{min} }\ C \sum_{i=1}^m \large[ y<sup>{(i)} cost_1(\theta</sup>Tf^{(i)}) + (1 - y<sup>{(i)}) cost_0(\theta</sup>Tf^{(i)})\large] + \frac{1}{2}\sum_{j=1}^m \theta_j^2\]</p>
<ul>
<li>Note that <strong>m = n</strong>, the number of features equals the number of training data examples.</li>
<li>Another mathematics detail about the training formula:
<ul>
<li>The regulation part: \(\displaystyle\sum_{j=1}^m \theta_j^2 = \theta^T\theta\)</li>
<li>What many implementations do is: \(\theta^TM\theta\)
<ul>
<li>Where the matrix <strong>M</strong> depends on the kernel you are using.</li>
<li>This is like a rescale version of the parameter vector theta.</li>
<li>Allows the SVM to run much more efficiently.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>You can apply kernels to other algorithms</p>
<ul>
<li>But they tend to be very computationally expensive</li>
<li>But the SVM is far more efficient - so more practical</li>
</ul>
</li>
</ul>
<h3 id="svm-parameters"><a class="markdownIt-Anchor" href="#svm-parameters"></a> SVM Parameters</h3>
<ul>
<li><strong>C</strong>(\(\frac{1}{\lambda}\))
<ul>
<li>Large C: Lower bias, high variance.</li>
<li>Small C: Higher bias, low variance.</li>
</ul>
</li>
<li>\(\sigma^2\):
<ul>
<li>Large \(\sigma^2\): Features \(f_i\) vary more smoothly. Higher bias, lower variance.</li>
<li>Small \(\sigma^2\): Features \(f_i\) vary less smoothly. Lower bias, higher variance.</li>
</ul>
</li>
</ul>
<h2 id="using-an-svm"><a class="markdownIt-Anchor" href="#using-an-svm"></a> Using an SVM</h2>
<ul>
<li>Use SVM software package(e.g. liblinear, libsvm,…) to solve for parameters \(\theta\).</li>
<li>What do you need to do, is to specify:
<ul>
<li>Choice of parameter <strong>C</strong></li>
<li>Choice of kernel</li>
</ul>
</li>
</ul>
<h3 id="choice-of-kernel"><a class="markdownIt-Anchor" href="#choice-of-kernel"></a> Choice of kernel</h3>
<ul>
<li>No kernel(“linear kernel”)
<ul>
<li>Predict “y = 1” if \(\theta^Tx \ge 0\)</li>
</ul>
</li>
<li>Gaussian kernel:
<ul>
<li>\(f_i = \text{exp}(-\frac{ {\lVert x - l^{(2)} \rVert}^2 }{2\sigma^2})\), where \(l^{(i)} = x^{(i)}\)</li>
<li>Need to choose \(\sigma^2\)</li>
<li>Note: Do perform <strong>feature scaling</strong> before using a Gaussian kernel.</li>
</ul>
</li>
<li>Other choice of kernel
<ul>
<li>Not all similarity functions make valid kernels</li>
<li>Need to satisfy technical condition called “<strong>Mercer’s Theorem</strong>” to make sure SVM packages’ optimizations run correctly, an do not diverge.</li>
<li>Many off-the-shelf kernels available:
<ul>
<li><strong>Polynomial kernel</strong>: \(k(x,l) = (x<sup>Tl+\text{constant})</sup>{\text{degree} }\)
<ul>
<li>For example: \(k(x,l) = (x<sup>Tl+1)</sup>2\)</li>
</ul>
</li>
<li>More esoteric: String kernel, chi-square kernel, histogram intersection kernel,…</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="multi-class-classification"><a class="markdownIt-Anchor" href="#multi-class-classification"></a> Multi-class Classification</h3>
<ul>
<li>Many SVM packages already have built-in multi-class classification functionality.</li>
<li>Otherwise, use one-vs-all method.</li>
</ul>
<h3 id="logistic-regression-vs-svms"><a class="markdownIt-Anchor" href="#logistic-regression-vs-svms"></a> Logistic Regression vs. SVMs</h3>
<ul>
<li>When should we use one algorithm versus the other?</li>
<li><strong>n</strong> = number of features (\(x \in \mathbb{R}^{n+1}\)), <strong>m</strong> = number of training examples</li>
<li>If <strong>n</strong> is large(relative to <strong>m</strong>) (e.g. n = 10,000, m = 10-1000):
<ul>
<li>Use logistic regression, or SVM without a kernel(“linear kernel”)</li>
</ul>
</li>
<li>If <strong>n</strong> is small, <strong>m</strong> is intermediate (e.g. n = 1 - 1000, m = 10 - 10,000):
<ul>
<li>Use SVM with Gaussian kernel</li>
</ul>
</li>
<li>If <strong>n</strong> is small, <strong>m</strong> is large (e.g. n = 1 - 1000, m = 10 - 50,000+):
<ul>
<li>Create/add more features, then use logistic regression or SVM without a kernel</li>
</ul>
</li>
<li>Neural network likely to work well for most of these settings, but may be slower to train.</li>
</ul>

</div>


  <div class="book-comments">
    




  </div>



<script src="/js/book-post.js"></script>

        </div>
      </div>
      <div class="column col-2 hide-lg">
        <div class="book-post-info">
  
    <div class="book-post-meta">

  <div class="author">

    <!-- Author image -->
    <div class="author-img">
      
        <figure
          class="avatar avatar-lg"
          data-initial="E"
          style="background-color: #3b4351;">
        </figure>
      
    </div>

    <!-- Author title -->
    <div class="author-title">
      <div>Eric Yang</div>
      <div>2020-04-13</div>
    </div>
  </div>

  

  <div class="divider"></div>
</div>
  

  <div class="book-tocbot">
</div>
<div class="book-tocbot-menu">
  <a class="book-toc-expand" onclick="expand_toc()">Expand all</a>
  <a onclick="go_top()">Back to top</a>
  <a onclick="go_bottom()">Go to bottom</a>
</div>


<script src="/js/book-toc.js"></script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "\\(", right: "\\)", display: false},
                {left: "\\[", right: "\\]", display: true}
            ]
        });
    });
</script>

</div>

      </div>
    </div>
  </div>
  
  <a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>

</body>
</html>


<script src="/js/book.js"></script>
