<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">

    
      <link rel="icon" href="/favicon.png" />
    

    <title>
        
          stanford-machine-learning/week-2 - Eric&#39;s CS Notes
        
    </title>

    <!-- Spectre.css framework -->
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-exp.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-icons.min.css">

    <!-- theme css & js -->
    
<link rel="stylesheet" href="/css/book.css">

    
<script src="/js/book.js"></script>


    <!-- tocbot -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css">
    
    <!-- katex -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">

    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/zooming/2.1.1/zooming.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    const zooming = new Zooming()
    zooming.listen('.book-content img')
})
</script>

<meta name="generator" content="Hexo 4.2.0"></head>

<body>

<div class="book-container">
  <div class="book-sidebar">
    <div class="book-brand">
  <a href="/">
    <img src="/favicon.png">
    <span>ERIC&#39;S CS NOTES</span>
  </a>
</div>
    <div class="book-menu">
  <h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2>
<ul>
<li><a href="/symbols/index.html">Symbols of Mathematics</a></li>
<li><a href="/glossary/index.html">Glossary</a></li>
</ul>
<!--
## Stanford Statistical Learning

* [Chapter 2 - Overview of Statistical Learning](/statistical-learning/chapter-2/index.html)
* Chapter 3 - Linear Regression
* Chapter 4 - Classification
* Chapter 5 - Resampling Methods
* Chapter 6 - Linear Model Selection and Regularization
* Chapter 7 - Moving Beyond Linearity
* Chapter 8 - Tree-Based Methods
* Chapter 9 - Support Vector Machines
* Chapter 10 - Unsupervised Learning
-->
<h2 id="introduction-to-probability"><a class="markdownIt-Anchor" href="#introduction-to-probability"></a> Introduction to Probability</h2>
<ul>
<li><a href="/introduction-to-probability/unit-1/index.html">Unit 1: Probability models and axioms</a></li>
<li><a href="/introduction-to-probability/unit-2/index.html">Unit 2: Conditioning and independence</a></li>
<li><a href="/introduction-to-probability/unit-3/index.html">Unit 3: Counting</a></li>
</ul>
<!--
* [Unit 4: Discrete random variables](/introduction-to-probability/unit-4/index.html)
* [Unit 5: Continuous random variables](/introduction-to-probability/unit-5/index.html)
* [Unit 6: Further topics on random variables](/introduction-to-probability/unit-6/index.html)
* [Unit 7: Bayesian inference](/introduction-to-probability/unit-7/index.html)
* [Unit 8: Limit theorems and classical statistics](/introduction-to-probability/unit-8/index.html)
* [Unit 9: Bernoulli and Poisson processes](/introduction-to-probability/unit-9/index.html)
* [Unit 10: Markov chains](/introduction-to-probability/unit-10/index.html)
-->
<h2 id="multivariable-calculus"><a class="markdownIt-Anchor" href="#multivariable-calculus"></a> Multivariable Calculus</h2>
<ul>
<li><a href="/multivariable-calculus/unit-1/index.html">Unit 1: Thinking about multivariable functions</a></li>
<li><a href="/multivariable-calculus/unit-2/index.html">Unit 2: Derivatives of multivariable functions</a></li>
</ul>
<!--
* [Unit 3: Applications of multivariable derivatives](/multivariable-calculus/unit-3/index.html)
* [Unit 4: Integrating multivariable functions](/multivariable-calculus/unit-4/index.html)
* [Unit 5: Green's, Stokes', and the divergence theorems](/multivariable-calculus/unit-5/index.html)
-->
<h2 id="algorithms-part-ii"><a class="markdownIt-Anchor" href="#algorithms-part-ii"></a> Algorithms: Part II</h2>
<ul>
<li><a href="/algorithms-2/week-1/index.html">Week 1 - Undirected Graph &amp; Directed Graph</a></li>
<li><a href="/algorithms-2/week-3/index.html">Week 3 - Maximum Flow and Minimum Cut &amp; Radix Sort</a></li>
<li><a href="/algorithms-2/week-4/index.html">Week 4 - Tries &amp; Substring Search</a></li>
</ul>
<h2 id="algorithms-part-i"><a class="markdownIt-Anchor" href="#algorithms-part-i"></a> Algorithms: Part I</h2>
<ul>
<li><a href="/algorithms-1/week-1/index.html">Week 1 - Union-Find &amp; Analysis of Algorithms</a></li>
<li><a href="/algorithms-1/week-2/index.html">Week 2 - Stacks and Queues &amp; Elementary Sorts</a></li>
<li><a href="/algorithms-1/week-3/index.html">Week 3 - Mergesort &amp; Quicksort</a></li>
<li><a href="/algorithms-1/week-4/index.html">Week 4 - Priority Queues &amp; Elementary Symbols</a></li>
<li><a href="/algorithms-1/week-5/index.html">Week 5 - Balanced Search Trees</a></li>
<li><a href="/algorithms-1/week-6/index.html">Week 6 - Hash Tables</a></li>
</ul>
<h2 id="introduction-to-software-design-and-architecture"><a class="markdownIt-Anchor" href="#introduction-to-software-design-and-architecture"></a> Introduction to Software Design and Architecture</h2>
<ul>
<li><a href="/introduction-to-software-design-and-architecture/design-pattern/index.html">Design Pattern</a></li>
</ul>
<h2 id="calculus-two-sequences-and-series"><a class="markdownIt-Anchor" href="#calculus-two-sequences-and-series"></a> Calculus Two: Sequences and Series</h2>
<ul>
<li><a href="/calculus-two/week-1/index.html">Week 1 - Sequences</a></li>
<li><a href="/calculus-two/week-2/index.html">Week 2 - Series</a></li>
<li><a href="/calculus-two/week-3/index.html">Week 3 - Convergence Tests</a></li>
<li><a href="/calculus-two/week-4/index.html">Week 4 - Alternating Series</a></li>
<li><a href="/calculus-two/week-5/index.html">Week 5 - Power Series</a></li>
<li><a href="/calculus-two/week-6/index.html">Week 6 - Taylor Series</a></li>
</ul>
<h2 id="laff-linear-algebra"><a class="markdownIt-Anchor" href="#laff-linear-algebra"></a> LAFF Linear Algebra</h2>
<ul>
<li><a href="/laff-linear-algebra/week-1/index.html">Week 1 - Vectors in Linear Algebra</a></li>
<li><a href="/laff-linear-algebra/week-2/index.html">Week 2 - Linear Transformations and Matrices</a></li>
<li><a href="/laff-linear-algebra/week-3/index.html">Week 3 - Matrix-Vector Operations</a></li>
<li><a href="/laff-linear-algebra/week-4/index.html">Week 4 - Matrix-Vector to Matrix-Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-5/index.html">Week 5 - Matrix- Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-6/index.html">Week 6 - Gaussian Elimination</a></li>
<li><a href="/laff-linear-algebra/week-7/index.html">Week 7 - More Gaussian Elimination and Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-8/index.html">Week 8 - More on Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-9/index.html">Week 9 - Vector Spaces</a></li>
<li><a href="/laff-linear-algebra/week-10/index.html">Week 10 - Vector Spaces, Orthogonality, and Linear Least-Squares</a></li>
<li><a href="/laff-linear-algebra/week-11/index.html">Week 11 - Orthogonal Projection, Low Rank Approximation, and Orthogonal Bases</a></li>
<li><a href="/laff-linear-algebra/week-12/index.html">Week 12 - Eigenvalues and Eigenvectors</a></li>
</ul>
<h2 id="stanford-machine-learning"><a class="markdownIt-Anchor" href="#stanford-machine-learning"></a> Stanford Machine Learning</h2>
<ul>
<li><a href="/stanford-machine-learning/week-1/index.html">Week 1 - Introduction</a></li>
<li><a href="/stanford-machine-learning/week-2/index.html">Week 2 - Linear Regression with Multiple Variables</a></li>
<li><a href="/stanford-machine-learning/week-3/index.html">Week 3 - Logistic Regression &amp; Regularization</a></li>
<li><a href="/stanford-machine-learning/week-4/index.html">Week 4 - Neural Networks: Representation</a></li>
<li><a href="/stanford-machine-learning/week-5/index.html">Week 5 - Neural Networks: Learning</a></li>
<li><a href="/stanford-machine-learning/week-6a/index.html">Week 6a - Advice for Applying Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-6b/index.html">Week 6b - Machine Learning System Design</a></li>
<li><a href="/stanford-machine-learning/week-7/index.html">Week 7 - Support Vector Machines</a></li>
<li><a href="/stanford-machine-learning/week-8/index.html">Week 8 - Unsupervised Learning &amp; Dimensionality Reduction</a></li>
<li><a href="/stanford-machine-learning/week-9a/index.html">Week 9a - Anomaly Detection</a></li>
<li><a href="/stanford-machine-learning/week-9b/index.html">Week 9b - Recommender Systems</a></li>
<li><a href="/stanford-machine-learning/week-10/index.html">Week 10 - Large Scale Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-11/index.html">Week 11 - Application Example: Photo OCR</a></li>
</ul>
<h2 id="calculus-one"><a class="markdownIt-Anchor" href="#calculus-one"></a> Calculus One</h2>
<ul>
<li><a href="/calculus-one/week-2-3/index.html">Week 2-3 - Functions &amp; Limits</a></li>
<li><a href="/calculus-one/week-4/index.html">Week 4 - The Beginning of Derivatives</a></li>
<li><a href="/calculus-one/week-5/index.html">Week 5 - Techniques of Differentiation</a></li>
<li><a href="/calculus-one/week-6/index.html">Week 6 - Chain Rule</a></li>
<li><a href="/calculus-one/week-7/index.html">Week 7 - Derivatives of Trigonometric Functions</a></li>
<li><a href="/calculus-one/week-8/index.html">Week 8 - Derivatives in the Real World</a></li>
<li><a href="/calculus-one/week-9/index.html">Week 9 - Optimization</a></li>
<li><a href="/calculus-one/week-10/index.html">Week 10 - Linear Approximation</a></li>
<li><a href="/calculus-one/week-11-12/index.html">Week 11-12 - Antidifferentiation &amp; Integration</a></li>
<li><a href="/calculus-one/week-13/index.html">Week 13 - Fundamental Theorem of Calculus</a></li>
<li><a href="/calculus-one/week-14/index.html">Week 14 - Substitution Rule</a></li>
<li><a href="/calculus-one/week-15/index.html">Week 15 - Techniques of Integration</a></li>
<li><a href="/calculus-one/week-16/index.html">Week 16 - Applications of Integration</a></li>
</ul>
<h2 id="computational-thinking"><a class="markdownIt-Anchor" href="#computational-thinking"></a> Computational Thinking</h2>
<ul>
<li><a href="/computational-thinking/lecture-1/index.html">Lecture 1 - Optimization and Knapsack Problem</a></li>
<li><a href="/computational-thinking/lecture-2/index.html">Lecture 2 - Decision Trees and Dynamic Programming</a>
<ul>
<li><a href="/computational-thinking/lecture-2-powerset/index.html">Exercise: Power Set Function</a></li>
</ul>
</li>
<li><a href="/computational-thinking/lecture-3/index.html">Lecture 3 - Graphs</a></li>
<li><a href="/computational-thinking/lecture-4-5/index.html">Lecture 4-5 - Plotting</a></li>
<li><a href="/computational-thinking/lecture-6-7/index.html">Lecture 6-7 - Stochastic Programs &amp; Inferential Statistics</a></li>
<li><a href="/computational-thinking/lecture-8/index.html">Lecture 8 - Monte Carlo Simulation</a></li>
<li><a href="/computational-thinking/lecture-9/index.html">Lecture 9 - Sampling and Standard Error</a></li>
<li><a href="/computational-thinking/lecture-10-11/index.html">Lecture 10-11 - Experimental Data</a></li>
<li><a href="/computational-thinking/lecture-12/index.html">Lecture 12 - Machine Learning</a></li>
<li><a href="/computational-thinking/lecture-13/index.html">Lecture 13 - Statistical Abuses</a></li>
</ul>
<h2 id="effective-thinking-through-mathematics"><a class="markdownIt-Anchor" href="#effective-thinking-through-mathematics"></a> Effective Thinking Through Mathematics</h2>
<ul>
<li><a href="/effective-thinking-through-mathematics/note/index.html">Note</a></li>
<li><a href="/effective-thinking-through-mathematics/week-4-telling-the-story-of-infinity/index.html">Week 4 (/Telling the Story of Infinity)</a></li>
<li><a href="/effective-thinking-through-mathematics/week-5-telling-the-story-of-the-euler-circuit-theorem/index.html">Week 5 (/Telling the Story of Euler Circuit Theorem)</a></li>
</ul>
<h2 id="cs50-introduction-to-computer-science"><a class="markdownIt-Anchor" href="#cs50-introduction-to-computer-science"></a> CS50 Introduction to Computer Science</h2>
<ul>
<li><a href="/cs50/week-1/index.html">Week 1 - C</a></li>
<li><a href="/cs50/week-2/index.html">Week 2 - Arrays</a></li>
<li><a href="/cs50/week-3/index.html">Week 3 - Algorithms</a></li>
<li><a href="/cs50/week-4/index.html">Week 4 - Memory</a></li>
<li><a href="/cs50/week-5/index.html">Week 5 - Data Structures</a></li>
<li><a href="/cs50/week-6/index.html">Week 6 - HTTP</a></li>
<li><a href="/cs50/week-7-10/index.html">Week 7-10 - Machine Learning/Python/SQL/Javascript</a></li>
</ul>

</div>


<script src="/js/book-menu.js"></script>

  </div>

  <div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseover="add_inner()" onmouseleave="remove_inner()">
  <div class="sidebar-toggle-inner"></div>
</div>

<script>
function add_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.add('show')  
}

function remove_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.remove('show')
}

function sidebar_toggle() {
    let sidebar_toggle = document.querySelector('.sidebar-toggle')
    let sidebar = document.querySelector('.book-sidebar')
    let content = document.querySelector('.off-canvas-content')
    if (sidebar_toggle.classList.contains('extend')) { // show
        sidebar_toggle.classList.remove('extend')
        sidebar.classList.remove('hide')
        content.classList.remove('extend')
    }
    else { // hide
        sidebar_toggle.classList.add('extend')
        sidebar.classList.add('hide')
        content.classList.add('extend')
    }
}
</script>

  <div class="off-canvas-content">
    <div class="columns">
      <div class="column col-10 col-lg-12">
        <div class="book-navbar">
          <!-- For Responsive Layout -->

<header class="navbar">
  <section class="navbar-section">
    <a onclick="open_sidebar()">
      <i class="icon icon-menu"></i>
    </a>
  </section>
</header>

        </div>
        <div class="book-content">
          <div class="book-post">
  <h1 id="week-2-linear-regression-with-multiple-variables"><a class="markdownIt-Anchor" href="#week-2-linear-regression-with-multiple-variables"></a> Week 2 - Linear Regression with Multiple Variables</h1>
<h2 id="multivariate-linear-regression"><a class="markdownIt-Anchor" href="#multivariate-linear-regression"></a> Multivariate Linear Regression</h2>
<h3 id="multiple-features"><a class="markdownIt-Anchor" href="#multiple-features"></a> Multiple features</h3>
<ul>
<li>In original version we had
<ul>
<li>\(X\) = house size, use this to predict</li>
<li>\(y\) = house price</li>
</ul>
</li>
<li>If in a new scheme we have more variables ( such as number of bedrooms, number floors, age of the house)
<ul>
<li>\(x_1, x_2, x_3, x_4\) are the four features
<ul>
<li>\(x_1\) - size (feet squared)</li>
<li>\(x_2\) - Number of bedrooms</li>
<li>\(x_3\) - Number of floors</li>
<li>\(x_4\) - Age of house (years)</li>
</ul>
</li>
<li>\(y\) is the output varible (price)</li>
</ul>
</li>
<li>Notations
<ul>
<li>\(n\) : number of features ( n =4 )</li>
<li>m : number of examples ( i.e. number of rows in a table )</li>
<li>\(x^{(i)}\) : input (features) of \(i^{th}\) training example.</li>
<li>\(x^{(i)}_{j}\) : value of feature \(j\) in \(i^{th}\) training example.</li>
<li><img src="https://i.imgur.com/OsFaou2.png" alt="week-2-1" /></li>
</ul>
</li>
<li>Cost function with multiple features
<ul>
<li>\(h_{\theta}(x) = \theta_{0}x_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{3} + \theta_{4}x_{4}\) (\(x_{0} = 1\) )
<ul>
<li>for convenience of notation, define \(x_0 = 1\)</li>
</ul>
</li>
<li>\(h_\theta(x) = \theta^{T}x\)
<ul>
<li>\(x = \begin{bmatrix} x_0 \ x_1 \ x_2 \ … \ x_n\end{bmatrix} (n+1)\)</li>
<li>\(\theta = \begin{bmatrix} \theta_0 \ \theta_1 \ \theta_2 \ … \ \theta_n\end{bmatrix} (n+1)\)</li>
<li>\(\theta^T\) is an \(1 * (n + 1)\) matrix</li>
</ul>
</li>
<li>\(h_{\theta}(X) = X\theta\)
<ul>
<li>The training examples are stored in \(X\) row-wise. The following example shows us the reason behind setting \(x^{(i)}_{0}=1\) :</li>
<li>\[\begin{aligned}X = \begin{bmatrix}x^{(1)}_0 &amp; x^{(1)}_1 \ x^{(2)}_0 &amp; x^{(2)}_1 \ x^{(3)}_0 &amp; x^{(3)}_1 \end{bmatrix}&amp;,\theta = \begin{bmatrix}\theta_0 \ \theta_1 \end{bmatrix}\end{aligned}\]</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="gradient-descent-for-multiple-variables"><a class="markdownIt-Anchor" href="#gradient-descent-for-multiple-variables"></a> Gradient Descent for Multiple Variables</h3>
<ul>
<li>
<p>Cost function: \(J(\theta_0, \theta_1,…, \theta_n) = \dfrac {1}{2m} \displaystyle \sum <em>{i=1}^m \left (h</em>\theta (x_{i}) - y_{i} \right)^2\)</p>
</li>
<li>
<p>Gradient descent:</p>
<ul>
<li>
<blockquote>
<p>\[\begin{aligned} \text{Repeat } \lbrace &amp; \ \theta_j := &amp; \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta) \ \rbrace&amp; \end{aligned}\]</p>
</blockquote>
<ul>
<li>\(J(\theta) = J(\theta_0, \theta_1,…, \theta_n)\)</li>
</ul>
</li>
</ul>
</li>
<li>
<p><img src="https://i.imgur.com/Yu3uUW3.png" alt="week-2-2" /></p>
</li>
</ul>
<h3 id="feature-scaling"><a class="markdownIt-Anchor" href="#feature-scaling"></a> Feature Scaling</h3>
<ul>
<li>If you have a problem with multiple features, you should make sure those features have a similar scale</li>
<li>e.g.
<ul>
<li>x1 = size(0 - 2000 feet)</li>
<li>x2 = number of bedrooms(1 - 5)</li>
<li>Means the contours generated if we plot \(\theta_1\) vs. \(\theta_2\) give a very tall and thin shape due to the huge range difference</li>
</ul>
</li>
<li>Running gradient descent on this kind of cost function can take a long time to find the globale minumum
<ul>
<li><img src="https://i.imgur.com/uSHJaTs.png" alt="week-2-3" /></li>
</ul>
</li>
<li>The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally:
<ul>
<li>\(-1 \le x_{(i)} \le 1\) or \(-0.5 \le x_{(i)} \le 0.5\)</li>
<li>The goal is to get all input variables into roughly one of these ranges, give or take a few.</li>
<li>Two techniques to help with this are <strong>feature scaling</strong> and <strong>mean normalization</strong>.</li>
<li><strong>Feature scaling</strong> involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1.</li>
<li><strong>Mean normalization</strong> involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.</li>
<li>To implement both of these techniques, adjust your input values as shown in this formula:
<ul>
<li>\(x_i := \dfrac{x_i - \mu_i}{s_i}\)</li>
<li>Where \(μ_i\) is the <strong>average</strong> of all the values for feature (i) and  \(s_i\) is the range of values (max - min), or \(s_i\) is the standard deviation.</li>
<li>For example, if \(x_i\) represents housing prices with a range of 100 to 2000 and a mean value of 1000, then, \(x_i := \dfrac{price-1000}{1900}\)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="learning-rate"><a class="markdownIt-Anchor" href="#learning-rate"></a> Learning Rate</h3>
<ul>
<li>Debugging: how to make sure gradient descent is working correctly.</li>
<li>How to choose learning rate \(\alpha\) .</li>
</ul>
<h4 id="debugging-gradient-descent"><a class="markdownIt-Anchor" href="#debugging-gradient-descent"></a> Debugging gradient descent</h4>
<ul>
<li>Make a plot with <em>number of iterations</em> on the x-axis. Now plot the cost function, \(J(θ)\) over the number of iterations of gradient descent. If \(J(θ)\) ever increases, then you probably need to decrease \(\alpha\).</li>
</ul>
<h4 id="automatic-convergence-test"><a class="markdownIt-Anchor" href="#automatic-convergence-test"></a> Automatic convergence test</h4>
<ul>
<li>Declare convergence if <strong>J(θ)</strong> decreases by less than <strong>E</strong> in one iteration, where <strong>E</strong> is some small value such as \(10^{-3}\). However in practice it’s difficult to choose this threshold value.</li>
<li><img src="https://i.imgur.com/XeUDjO7.png" alt="week-2-4" /></li>
<li>It has been proven that if learning rate \(\alpha\) is sufficiently small, then \(J(θ)\) will decrease on every iteration.</li>
<li><img src="https://i.imgur.com/TtstrDZ.png" alt="week-2-5" /></li>
</ul>
<h4 id="summarize"><a class="markdownIt-Anchor" href="#summarize"></a> Summarize</h4>
<ul>
<li>If \(\alpha\) is too small: slow convergence.</li>
<li>If \(\alpha\) is too large: may not decrease on every iteration and thus may not converge.</li>
<li>Typically
<ul>
<li>Try a range of alpha values</li>
<li>Plot \(J(\theta)\) vs number of iterations for each version of \(\alpha\)</li>
<li>Go for roughly threefold increases
<ul>
<li>0.001, 0.003, 0.01, 0.03. 0.1, 0.3</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="features-and-polynomial"><a class="markdownIt-Anchor" href="#features-and-polynomial"></a> Features and Polynomial</h3>
<ul>
<li>to improve our features and the form of our hypothesis function.</li>
</ul>
<h4 id="combine-multiple-features-into-one"><a class="markdownIt-Anchor" href="#combine-multiple-features-into-one"></a> Combine multiple features into one</h4>
<ul>
<li>We can combine \(x_1\) and \(x_2\) into a new feature \(x_3\) by taking \(x1 \cdot x2\) .
<ul>
<li>take the housing price as sample: \(x = x_1 * x_2\),\(x_1\) is frontage, and \(x_2\) is depth of the house.</li>
</ul>
</li>
</ul>
<h4 id="polynomial-regression"><a class="markdownIt-Anchor" href="#polynomial-regression"></a> Polynomial Regression</h4>
<ul>
<li>
<p>Our hypothesis function need <strong>NOT</strong> be linear (a straight line) if that does not fit the data well. And, choose new features to get a better model, called polynomial regression.</p>
</li>
<li>
<p>We can <strong>change the behavior or curve</strong> of our hypothesis function by making it a quadratic, cubic or square root function (or any other form). ​</p>
</li>
<li>
<p>For housing data could use a quadratic function: \(h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_1^2\)</p>
<ul>
<li>But may not fit data so well, because inflection point means housing prices decrease when size gets really big.</li>
<li>So instead must use a cubic function: \(h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_1^2 + \theta_3 x_1^3\)</li>
<li><img src="https://i.imgur.com/1fxm3mW.png" alt="week-2-6" /></li>
<li>In the cubic version, we have created new features \(x_2\) and \(x_3\) where.\(x_2 = x_1^2\) and \(x_3 = x_1^3\) .
<ul>
<li><img src="https://i.imgur.com/rsAi6DV.png" alt="week-2-10" /></li>
</ul>
</li>
<li>One important thing to keep in mind is, if you choose your features this way then feature scaling becomes very important.
<ul>
<li>eg. if \(x_1\) has range 1 - 1000 then range of \(x^2_1\) becomes 1 - 1000000 and that of \(x^3_1\) becomes 1 - 1000000000</li>
</ul>
</li>
<li>Or we can make it a square root function, we could do: \(h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 \sqrt{x_1}\) .</li>
</ul>
</li>
</ul>
<h2 id="computing-parameters-analytically"><a class="markdownIt-Anchor" href="#computing-parameters-analytically"></a> Computing Parameters Analytically</h2>
<h3 id="normal-equation"><a class="markdownIt-Anchor" href="#normal-equation"></a> Normal Equation</h3>
<ul>
<li>
<p>Method to solve for \(\theta\) analytically.Minimize J by explicitly taking its derivatives with respect to the θj ’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:</p>
<ul>
<li>\(\theta = (X^T X)<sup>{-1}X</sup>T y\)</li>
<li><img src="https://i.imgur.com/rbQ2fdS.png" alt="week-2-11" /></li>
</ul>
</li>
<li>
<p><strong>no need</strong> to do feature scaling with the normal equation.</p>
</li>
<li>
<p>The following is a comparison of gradient descent and the normal equation:</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Gradient Descent</th>
<th>Normal Equation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Need to choose alpha</td>
<td>No need to choose alpha</td>
</tr>
<tr>
<td>Needs many iterations</td>
<td>No need to iterate</td>
</tr>
<tr>
<td>O(\(kn^2\) )</td>
<td>O(\(n^3\) ), need to calculate inverse of \(X^TX\)</td>
</tr>
<tr>
<td>Works well when \(n\) is large</td>
<td>Slow if \(n\) is very large</td>
</tr>
</tbody>
</table>
<ul>
<li>With the normal equation, computing the inversion has complexity \(\mathcal{O}(n^3)\). So if we have a very large number of features, the normal equation will be slow. In practice, when exceeds 10,000 it might be a good time to go from a normal solution to an iterative process.</li>
</ul>
<h4 id="normal-equation-noninvertibility"><a class="markdownIt-Anchor" href="#normal-equation-noninvertibility"></a> Normal Equation Noninvertibility</h4>
<ul>
<li>If \(X^TX\) is <strong>noninvertible</strong> (singular/degenerate), the common causes might be having :
<ul>
<li>Redundant features, where two features are very closely related (i.e. they are linearly dependent)</li>
<li>Too many features (e.g. m ≤ n). In this case, delete some features or use “regularization”.</li>
</ul>
</li>
</ul>
<h2 id="words"><a class="markdownIt-Anchor" href="#words"></a> Words</h2>
<p><strong>semicolon</strong>   [,semi’kəulən, 'semikəulən] n. 分号</p>
<p><strong>decimal</strong>   ['desiməl] adj. 小数的；十进位的; n. 小数</p>
<p><strong>diagonal</strong>   [dai’æɡənəl] adj. 斜的；对角线的；斜纹的; n. 对角线；斜线</p>
<p><strong>vectorization</strong>   [,vektəri’zeiʃən] n. [数] 向量化</p>
<p><strong>numerical</strong>   [nju:'merikəl] adj. 数值的；数字的；用数字表示的</p>
<p><strong>pathological</strong>   [,pæθə’lɔdʒikəl] adj. 病理学的；病态的；由疾病引起的（等于pathologic）</p>
<p><strong>convergence</strong>   [kən’və:dʒəns] n. [数] 收敛；会聚，集合</p>
<p><strong>polynomial</strong>   [,pɔli’nəumiəl] n. [数] 多项式；</p>
<p><strong>quadratic</strong>   [kwɔ’drætik] adj. [数] 二次的 n. 二次方程式</p>
<p><strong>cubic</strong>   ['kju:bik] adj. 立方体的，立方的</p>
<p><strong>noninvertibility</strong> 可逆性</p>

</div>


  <div class="book-comments">
    




  </div>



<script src="/js/book-post.js"></script>

        </div>
      </div>
      <div class="column col-2 hide-lg">
        <div class="book-post-info">
  
    <div class="book-post-meta">

  <div class="author">

    <!-- Author image -->
    <div class="author-img">
      
        <figure
          class="avatar avatar-lg"
          data-initial="E"
          style="background-color: #3b4351;">
        </figure>
      
    </div>

    <!-- Author title -->
    <div class="author-title">
      <div>Eric Yang</div>
      <div>2020-04-13</div>
    </div>
  </div>

  

  <div class="divider"></div>
</div>
  

  <div class="book-tocbot">
</div>
<div class="book-tocbot-menu">
  <a class="book-toc-expand" onclick="expand_toc()">Expand all</a>
  <a onclick="go_top()">Back to top</a>
  <a onclick="go_bottom()">Go to bottom</a>
</div>


<script src="/js/book-toc.js"></script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "\\(", right: "\\)", display: false},
                {left: "\\[", right: "\\]", display: true}
            ]
        });
    });
</script>

</div>

      </div>
    </div>
  </div>
  
  <a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>

</body>
</html>


<script src="/js/book.js"></script>
