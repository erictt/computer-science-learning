<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">

    
      <link rel="icon" href="/favicon.png" />
    

    <title>
        
          stanford-machine-learning/week-3 - Eric&#39;s CS Notes
        
    </title>

    <!-- Spectre.css framework -->
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-exp.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-icons.min.css">

    <!-- theme css & js -->
    
<link rel="stylesheet" href="/css/book.css">

    
<script src="/js/book.js"></script>


    <!-- tocbot -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css">
    
    <!-- katex -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">

    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/zooming/2.1.1/zooming.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    const zooming = new Zooming()
    zooming.listen('.book-content img')
})
</script>

<meta name="generator" content="Hexo 4.2.0"></head>

<body>

<div class="book-container">
  <div class="book-sidebar">
    <div class="book-brand">
  <a href="/">
    <img src="/favicon.png">
    <span>ERIC&#39;S CS NOTES</span>
  </a>
</div>
    <div class="book-menu">
  <h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2>
<ul>
<li><a href="/README/index.html">Introduction</a></li>
<li><a href="/SYMBOLS/index.html">Symbols of Mathematics</a></li>
</ul>
<!--
## Stanford Statistical Learning

* [Chapter 2 - Overview of Statistical Learning](/statistical-learning/chapter-2/index.html)
* Chapter 3 - Linear Regression
* Chapter 4 - Classification
* Chapter 5 - Resampling Methods
* Chapter 6 - Linear Model Selection and Regularization
* Chapter 7 - Moving Beyond Linearity
* Chapter 8 - Tree-Based Methods
* Chapter 9 - Support Vector Machines
* Chapter 10 - Unsupervised Learning
-->
<h2 id="introduction-to-probability"><a class="markdownIt-Anchor" href="#introduction-to-probability"></a> Introduction to Probability</h2>
<ul>
<li><a href="/introduction-to-probability/unit-1/index.html">Unit 1: Probability models and axioms</a></li>
<li><a href="/introduction-to-probability/unit-2/index.html">Unit 2: Conditioning and independence</a></li>
<li><a href="/introduction-to-probability/unit-3/index.html">Unit 3: Counting</a></li>
</ul>
<!--
* [Unit 4: Discrete random variables](/introduction-to-probability/unit-4/index.html)
* [Unit 5: Continuous random variables](/introduction-to-probability/unit-5/index.html)
* [Unit 6: Further topics on random variables](/introduction-to-probability/unit-6/index.html)
* [Unit 7: Bayesian inference](/introduction-to-probability/unit-7/index.html)
* [Unit 8: Limit theorems and classical statistics](/introduction-to-probability/unit-8/index.html)
* [Unit 9: Bernoulli and Poisson processes](/introduction-to-probability/unit-9/index.html)
* [Unit 10: Markov chains](/introduction-to-probability/unit-10/index.html)
-->
<h2 id="multivariable-calculus"><a class="markdownIt-Anchor" href="#multivariable-calculus"></a> Multivariable Calculus</h2>
<ul>
<li><a href="/multivariable-calculus/unit-1/index.html">Unit 1: Thinking about multivariable functions</a></li>
<li><a href="/multivariable-calculus/unit-2/index.html">Unit 2: Derivatives of multivariable functions</a></li>
</ul>
<!--
* [Unit 3: Applications of multivariable derivatives](/multivariable-calculus/unit-3/index.html)
* [Unit 4: Integrating multivariable functions](/multivariable-calculus/unit-4/index.html)
* [Unit 5: Green's, Stokes', and the divergence theorems](/multivariable-calculus/unit-5/index.html)
-->
<h2 id="algorithms-part-ii"><a class="markdownIt-Anchor" href="#algorithms-part-ii"></a> Algorithms: Part II</h2>
<ul>
<li><a href="/algorithms-2/week-1/index.html">Week 1 - Undirected Graph &amp; Directed Graph</a></li>
<li><a href="/algorithms-2/week-3/index.html">Week 3 - Maximum Flow and Minimum Cut &amp; Radix Sort</a></li>
<li><a href="/algorithms-2/week-4/index.html">Week 4 - Tries &amp; Substring Search</a></li>
</ul>
<h2 id="algorithms-part-i"><a class="markdownIt-Anchor" href="#algorithms-part-i"></a> Algorithms: Part I</h2>
<ul>
<li><a href="/algorithms-1/week-1/index.html">Week 1 - Union-Find &amp; Analysis of Algorithms</a></li>
<li><a href="/algorithms-1/week-2/index.html">Week 2 - Stacks and Queues &amp; Elementary Sorts</a></li>
<li><a href="/algorithms-1/week-3/index.html">Week 3 - Mergesort &amp; Quicksort</a></li>
<li><a href="/algorithms-1/week-4/index.html">Week 4 - Priority Queues &amp; Elementary Symbols</a></li>
<li><a href="/algorithms-1/week-5/index.html">Week 5 - Balanced Search Trees</a></li>
<li><a href="/algorithms-1/week-6/index.html">Week 6 - Hash Tables</a></li>
</ul>
<h2 id="introduction-to-software-design-and-architecture"><a class="markdownIt-Anchor" href="#introduction-to-software-design-and-architecture"></a> Introduction to Software Design and Architecture</h2>
<ul>
<li><a href="/introduction-to-software-design-and-architecture/design-pattern/index.html">Design Pattern</a></li>
</ul>
<h2 id="calculus-two-sequences-and-series"><a class="markdownIt-Anchor" href="#calculus-two-sequences-and-series"></a> Calculus Two: Sequences and Series</h2>
<ul>
<li><a href="/calculus-two/week-1/index.html">Week 1 - Sequences</a></li>
<li><a href="/calculus-two/week-2/index.html">Week 2 - Series</a></li>
<li><a href="/calculus-two/week-3/index.html">Week 3 - Convergence Tests</a></li>
<li><a href="/calculus-two/week-4/index.html">Week 4 - Alternating Series</a></li>
<li><a href="/calculus-two/week-5/index.html">Week 5 - Power Series</a></li>
<li><a href="/calculus-two/week-6/index.html">Week 6 - Taylor Series</a></li>
</ul>
<h2 id="laff-linear-algebra"><a class="markdownIt-Anchor" href="#laff-linear-algebra"></a> LAFF Linear Algebra</h2>
<ul>
<li><a href="/laff-linear-algebra/week-1/index.html">Week 1 - Vectors in Linear Algebra</a></li>
<li><a href="/laff-linear-algebra/week-2/index.html">Week 2 - Linear Transformations and Matrices</a></li>
<li><a href="/laff-linear-algebra/week-3/index.html">Week 3 - Matrix-Vector Operations</a></li>
<li><a href="/laff-linear-algebra/week-4/index.html">Week 4 - Matrix-Vector to Matrix-Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-5/index.html">Week 5 - Matrix- Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-6/index.html">Week 6 - Gaussian Elimination</a></li>
<li><a href="/laff-linear-algebra/week-7/index.html">Week 7 - More Gaussian Elimination and Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-8/index.html">Week 8 - More on Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-9/index.html">Week 9 - Vector Spaces</a></li>
<li><a href="/laff-linear-algebra/week-10/index.html">Week 10 - Vector Spaces, Orthogonality, and Linear Least-Squares</a></li>
<li><a href="/laff-linear-algebra/week-11/index.html">Week 11 - Orthogonal Projection, Low Rank Approximation, and Orthogonal Bases</a></li>
<li><a href="/laff-linear-algebra/week-12/index.html">Week 12 - Eigenvalues and Eigenvectors</a></li>
</ul>
<h2 id="stanford-machine-learning"><a class="markdownIt-Anchor" href="#stanford-machine-learning"></a> Stanford Machine Learning</h2>
<ul>
<li><a href="/stanford-machine-learning/week-1/index.html">Week 1 - Introduction</a></li>
<li><a href="/stanford-machine-learning/week-2/index.html">Week 2 - Linear Regression with Multiple Variables</a></li>
<li><a href="/stanford-machine-learning/week-3/index.html">Week 3 - Logistic Regression &amp; Regularization</a></li>
<li><a href="/stanford-machine-learning/week-4/index.html">Week 4 - Neural Networks: Representation</a></li>
<li><a href="/stanford-machine-learning/week-5/index.html">Week 5 - Neural Networks: Learning</a></li>
<li><a href="/stanford-machine-learning/week-6a/index.html">Week 6a - Advice for Applying Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-6b/index.html">Week 6b - Machine Learning System Design</a></li>
<li><a href="/stanford-machine-learning/week-7/index.html">Week 7 - Support Vector Machines</a></li>
<li><a href="/stanford-machine-learning/week-8/index.html">Week 8 - Unsupervised Learning &amp; Dimensionality Reduction</a></li>
<li><a href="/stanford-machine-learning/week-9a/index.html">Week 9a - Anomaly Detection</a></li>
<li><a href="/stanford-machine-learning/week-9b/index.html">Week 9b - Recommender Systems</a></li>
<li><a href="/stanford-machine-learning/week-10/index.html">Week 10 - Large Scale Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-11/index.html">Week 11 - Application Example: Photo OCR</a></li>
</ul>
<h2 id="calculus-one"><a class="markdownIt-Anchor" href="#calculus-one"></a> Calculus One</h2>
<ul>
<li><a href="/calculus-one/week-2-3/index.html">Week 2-3 - Functions &amp; Limits</a></li>
<li><a href="/calculus-one/week-4/index.html">Week 4 - The Beginning of Derivatives</a></li>
<li><a href="/calculus-one/week-5/index.html">Week 5 - Techniques of Differentiation</a></li>
<li><a href="/calculus-one/week-6/index.html">Week 6 - Chain Rule</a></li>
<li><a href="/calculus-one/week-7/index.html">Week 7 - Derivatives of Trigonometric Functions</a></li>
<li><a href="/calculus-one/week-8/index.html">Week 8 - Derivatives in the Real World</a></li>
<li><a href="/calculus-one/week-9/index.html">Week 9 - Optimization</a></li>
<li><a href="/calculus-one/week-10/index.html">Week 10 - Linear Approximation</a></li>
<li><a href="/calculus-one/week-11-12/index.html">Week 11-12 - Antidifferentiation &amp; Integration</a></li>
<li><a href="/calculus-one/week-13/index.html">Week 13 - Fundamental Theorem of Calculus</a></li>
<li><a href="/calculus-one/week-14/index.html">Week 14 - Substitution Rule</a></li>
<li><a href="/calculus-one/week-15/index.html">Week 15 - Techniques of Integration</a></li>
<li><a href="/calculus-one/week-16/index.html">Week 16 - Applications of Integration</a></li>
</ul>
<h2 id="computational-thinking"><a class="markdownIt-Anchor" href="#computational-thinking"></a> Computational Thinking</h2>
<ul>
<li><a href="/computational-thinking/lecture-1/index.html">Lecture 1 - Optimization and Knapsack Problem</a></li>
<li><a href="/computational-thinking/lecture-2/index.html">Lecture 2 - Decision Trees and Dynamic Programming</a>
<ul>
<li><a href="/computational-thinking/lecture-2-powerset/index.html">Exercise: Power Set Function</a></li>
</ul>
</li>
<li><a href="/computational-thinking/lecture-3/index.html">Lecture 3 - Graphs</a></li>
<li><a href="/computational-thinking/lecture-4-5/index.html">Lecture 4-5 - Plotting</a></li>
<li><a href="/computational-thinking/lecture-6-7/index.html">Lecture 6-7 - Stochastic Programs &amp; Inferential Statistics</a></li>
<li><a href="/computational-thinking/lecture-8/index.html">Lecture 8 - Monte Carlo Simulation</a></li>
<li><a href="/computational-thinking/lecture-9/index.html">Lecture 9 - Sampling and Standard Error</a></li>
<li><a href="/computational-thinking/lecture-10-11/index.html">Lecture 10-11 - Experimental Data</a></li>
<li><a href="/computational-thinking/lecture-12/index.html">Lecture 12 - Machine Learning</a></li>
<li><a href="/computational-thinking/lecture-13/index.html">Lecture 13 - Statistical Abuses</a></li>
</ul>
<h2 id="effective-thinking-through-mathematics"><a class="markdownIt-Anchor" href="#effective-thinking-through-mathematics"></a> Effective Thinking Through Mathematics</h2>
<ul>
<li><a href="/effective-thinking-through-mathematics/note/index.html">Note</a></li>
<li><a href="/effective-thinking-through-mathematics/week-4-telling-the-story-of-infinity/index.html">Week 4 (/Telling the Story of Infinity)</a></li>
<li><a href="/effective-thinking-through-mathematics/week-5-telling-the-story-of-the-euler-circuit-theorem/index.html">Week 5 (/Telling the Story of Euler Circuit Theorem)</a></li>
</ul>
<h2 id="cs50-introduction-to-computer-science"><a class="markdownIt-Anchor" href="#cs50-introduction-to-computer-science"></a> CS50 Introduction to Computer Science</h2>
<ul>
<li><a href="/cs50/week-1/index.html">Week 1 - C</a></li>
<li><a href="/cs50/week-2/index.html">Week 2 - Arrays</a></li>
<li><a href="/cs50/week-3/index.html">Week 3 - Algorithms</a></li>
<li><a href="/cs50/week-4/index.html">Week 4 - Memory</a></li>
<li><a href="/cs50/week-5/index.html">Week 5 - Data Structures</a></li>
<li><a href="/cs50/week-6/index.html">Week 6 - HTTP</a></li>
<li><a href="/cs50/week-7-10/index.html">Week 7-10 - Machine Learning/Python/SQL/Javascript</a></li>
</ul>

</div>


<script src="/js/book-menu.js"></script>

  </div>

  <div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseover="add_inner()" onmouseleave="remove_inner()">
  <div class="sidebar-toggle-inner"></div>
</div>

<script>
function add_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.add('show')  
}

function remove_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.remove('show')
}

function sidebar_toggle() {
    let sidebar_toggle = document.querySelector('.sidebar-toggle')
    let sidebar = document.querySelector('.book-sidebar')
    let content = document.querySelector('.off-canvas-content')
    if (sidebar_toggle.classList.contains('extend')) { // show
        sidebar_toggle.classList.remove('extend')
        sidebar.classList.remove('hide')
        content.classList.remove('extend')
    }
    else { // hide
        sidebar_toggle.classList.add('extend')
        sidebar.classList.add('hide')
        content.classList.add('extend')
    }
}
</script>

  <div class="off-canvas-content">
    <div class="columns">
      <div class="column col-10 col-lg-12">
        <div class="book-navbar">
          <!-- For Responsive Layout -->

<header class="navbar">
  <section class="navbar-section">
    <a onclick="open_sidebar()">
      <i class="icon icon-menu"></i>
    </a>
  </section>
</header>

        </div>
        <div class="book-content">
          <div class="book-post">
  <h1 id="week-3-logistic-regression-regularization"><a class="markdownIt-Anchor" href="#week-3-logistic-regression-regularization"></a> Week 3 - Logistic Regression &amp; Regularization</h1>
<h2 id="classification-and-representation"><a class="markdownIt-Anchor" href="#classification-and-representation"></a> Classification and Representation</h2>
<h3 id="classification"><a class="markdownIt-Anchor" href="#classification"></a> Classification</h3>
<ul>
<li>to determine what class a new input should fall into, \(y \in {0, 1, …, n}\)</li>
<li>Classification problems
<ul>
<li>Email -&gt; spam/not spam?</li>
<li>Online transactions -&gt; fraudulent?</li>
<li>Tumor -&gt; Malignant/benign</li>
</ul>
</li>
<li>Sometimes, Linear Regression may work, like the sample below:
<ul>
<li>
<img src="https://i.imgur.com/ya0BsNY.jpg" style="width:400px" />
</li>
<li>But most time the training set won’t be so perfect</li>
</ul>
</li>
<li>Here we use <strong>Logistic regression</strong>, which generates a value where is always either 0 or 1</li>
</ul>
<h3 id="hypothesis-representation"><a class="markdownIt-Anchor" href="#hypothesis-representation"></a> Hypothesis Representation</h3>
<ul>
<li>To change our hypotheses \(h_θ(x)\) to satisfy \(0 \leq h_\theta (x) \leq 1\) .</li>
<li>Our new form uses the “<strong>Logistic Function</strong>” , also called the “Sigmoid Function”:
<ul>
<li>\(\begin{aligned}&amp; h_\theta (x) = g ( \theta^T x )\end{aligned}\)</li>
<li>\(\begin{aligned} z = \theta^T x \end{aligned}\)</li>
<li>\(\begin{aligned} g(z) = \dfrac{1}{1 + e^{-z}}\end{aligned}\)
<ul>
<li>\(e\) : Exponents</li>
</ul>
</li>
</ul>
</li>
<li>The following image shows us what the logistic function looks like:
<ul>
<li><img src="https://i.imgur.com/owUeGBY.png" alt="week-3-1" /></li>
</ul>
</li>
<li>Interpreting hypothesis output, we can use:
<ul>
<li>\(h_\theta(x) = P(y=1 | x ; \theta)\) , it give us the <strong>probability</strong> that output is 1.
<ul>
<li>probability that y = 1, given x, paramerterized by \(\theta\)</li>
<li>\(h_\theta(x) = P(y=1 | x ; \theta) + h_\theta(x) = P(y=0 | x ; \theta) = 1\)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="decision-boundary"><a class="markdownIt-Anchor" href="#decision-boundary"></a> Decision Boundary</h3>
<ul>
<li>In order to get our discrete 0 or 1 classification, we can translate the output of the hypothesis function as follows:
<ul>
<li>\(\begin{aligned}&amp; h_\theta(x) \geq 0.5 \rightarrow y = 1 \&amp; h_\theta(x) &lt; 0.5 \rightarrow y = 0 \\end{aligned}\)</li>
<li>so \(\begin{aligned}&amp; g(z) \geq 0.5 \text{, when } z \geq 0\end{aligned}\)
<ul>
<li>\(\begin{aligned}z=0, e^{0}=1 &amp;\Rightarrow g(z)=1/2\ z \to \infty, e^{-\infty} \to 0 &amp;\Rightarrow g(z)=1 \ z \to -\infty, e^{\infty}\to \infty &amp;\Rightarrow g(z)=0 \end{aligned}\)</li>
<li>so if our input to \(g\) is \(\theta^TX\) , then that means: \(\begin{aligned}&amp; h_\theta(x) = g(\theta^T x) \geq 0.5 \text{, when}\ \theta^T x \geq 0\end{aligned}\)</li>
</ul>
</li>
</ul>
</li>
<li>Sample:
<ul>
<li>\(h_\theta(x) = g(\theta_0 + \theta_1x_1 + \theta_2x_2)\) :</li>
<li><img src="https://i.imgur.com/xT54x7N.png" alt="week-3-2" /></li>
<li>\(\theta_0 = -3, \theta_1 = 1, \theta_2 = 1\)</li>
</ul>
</li>
<li>So our parameter vector is a column vector with the above values: \(\theta = \begin{bmatrix} -3\1\1\end{bmatrix}\)</li>
<li>Then \(z\) becomes \(\theta^TX\)</li>
<li>We predict &quot; \(y=1\) &quot; if
<ul>
<li>\[\begin{aligned}-3x_0 + 1x_1 + 1x_2 &amp;\geq 0 \<br />
-3 + x_1 + x_2 &amp;\geq 0 \<br />
x_1 + x_2 &amp;\geq 3\end{aligned}\]</li>
</ul>
</li>
<li>So \(x_1 + x_2 = 3\) we graphically plot our decision boundary:
<ul>
<li><img src="https://i.imgur.com/zjCbytD.png" alt="week-3-3" /></li>
<li>Means:
<ul>
<li>Blue = false, Magenta = true</li>
<li>Line = decision boundary</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="non-linear-decision-boundaries"><a class="markdownIt-Anchor" href="#non-linear-decision-boundaries"></a> Non-linear decision boundaries</h3>
<ul>
<li>Get logistic regression to fit a complex non-linear data set</li>
<li>\(h_\theta(x) = g(\theta_0 +\theta_1x_1 + \theta_2x_2 + \theta_3x_1^2 + \theta_4x_2^2)\)</li>
<li>Say \(\theta^T = \begin{bmatrix}-1, 0, 0, 1, 1\end{bmatrix}\) :</li>
<li>Predict that &quot; \(y = 1\) &quot;, if \(x_1^2 + x_2^2 \geq 1\)</li>
<li>If we plot \(x_1^2 + x_2^2 = 1\) , then this gives us a circle with a radius of 1 around 0:
<ul>
<li><img src="https://i.imgur.com/4NeARsm.png" alt="week-3-4" /></li>
</ul>
</li>
<li>Mean we can build more complex decision boundaries by fitting complex parameters to this (relatively) simple hypothesis</li>
</ul>
<h2 id="logistic-regression-model"><a class="markdownIt-Anchor" href="#logistic-regression-model"></a> Logistic Regression Model</h2>
<h3 id="cost-function"><a class="markdownIt-Anchor" href="#cost-function"></a> Cost function</h3>
<ul>
<li>Define the optimization object for the cost function we use to fit the parameters
<ul>
<li>Training set: \({(x^{(1)}, y ^ {(1)}), (x^{(2)}, y ^ {(2)}),… , (x^{(m)}, y ^ {(m)}) }\)</li>
<li>m example: \(x \in \begin{bmatrix}x_0\ x_1\ … \ x_n \end{bmatrix}; x_0 = 1, y \in {0, 1}\)</li>
<li>\(h_\theta(x) = \dfrac{1}{1 + e<sup>{-\theta</sup>Tx}}\)
<ul>
<li>Each example is a feature vector which is \(n+1\) dimensional</li>
</ul>
</li>
</ul>
</li>
<li>Linear regression uses the following function to determine \(\theta\)
<ul>
<li>\(J(\theta) = \dfrac {1}{m} \displaystyle \sum <em>{i=1}^m \dfrac{1}{2}\left (h</em>\theta (x^{(i)}) - y^{(i)} \right)^2\)</li>
<li>define <code>Cost</code> function to simplify the function:
<ul>
<li>\(Cost(h_\theta(x^{(i)}), y^{(i)}) = \dfrac{1}{2}(h_\theta(x^{(i)}) - y<sup>{(i)})</sup>2\)</li>
</ul>
</li>
<li>then we got:
<ul>
<li>\( J(\theta) = \dfrac{1}{m} \displaystyle \sum_{i=1}^m \mathrm{Cost}(h_\theta(x<sup>{(i)}),y</sup>{(i)})\)</li>
</ul>
</li>
<li>to further simplify it, we can get rid of the superscripts:
<ul>
<li>\( J(\theta) = \dfrac{1}{m} \displaystyle \sum_{i=1}^m \mathrm{Cost}(h_\theta(x),y)\)</li>
</ul>
</li>
</ul>
</li>
<li>If we use this function for logistic regression, it will be a <strong>non-convex function</strong> which has many local optimum. Like:
<ul>
<li><img src="https://i.imgur.com/6CSilDl.png" alt="week-3-5" /></li>
</ul>
</li>
<li>So we come out a new convex logistic regression cost function:
<ul>
<li>\(\begin{aligned} &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x)) ; &amp; \text{if y = 1} \ &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x)) ; &amp; \text{if y = 0}\end{aligned}\)</li>
<li>We only care \((0 \le h(x) \le 1)\), so:</li>
<li><img src="https://i.imgur.com/Ezj2ukK.png" alt="week-3-6" /></li>
<li><img src="https://i.imgur.com/0laO8CR.png" alt="week-3-7" /></li>
<li>\(\begin{aligned}&amp; \mathrm{Cost}(h_\theta(x),y) = 0 \text{ if } h_\theta(x) = y \ &amp; \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{ if } y = 0 ; \mathrm{and} ; h_\theta(x) \rightarrow 1 \ &amp; \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{ if } y = 1 ; \mathrm{and} ; h_\theta(x) \rightarrow 0 \ \end{aligned}\)</li>
</ul>
</li>
</ul>
<h3 id="simplified-cost-function-and-gradient-descent"><a class="markdownIt-Anchor" href="#simplified-cost-function-and-gradient-descent"></a> Simplified cost function and gradient descent</h3>
<ul>
<li>Compress cost function’s two conditional cases into one case:
<ul>
<li>\(\mathrm{Cost}(h_\theta(x),y) = - y ; \log(h_\theta(x)) - (1 - y) \log(1 - h_\theta(x))\)</li>
<li>Cause y can either be 0 or 1,
<ul>
<li>if y = 1, then \( (1 - y) \log(1 - h_\theta(x)) = 0\)</li>
<li>if y = 0, then \( y \log(h_\theta(x)) = 0\)</li>
</ul>
</li>
<li>this cost function can be derived from statistics using the principle of <strong>maximum likelihood estimation</strong>.
<ul>
<li>the idea of how to efficiently find parameters’ data for different models.</li>
</ul>
</li>
</ul>
</li>
<li>Then we can fully write out our entire cost function as follows:
<ul>
<li>\(J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]\)</li>
<li>and a vectorized implementation is:
<ul>
<li>\(\begin{aligned} &amp; h = g(X\theta)\ &amp; J(\theta) = \frac{1}{m} \cdot \left(-y<sup>{T}\log(h)-(1-y)</sup>{T}\log(1-h)\right) \end{aligned}\)</li>
</ul>
</li>
</ul>
</li>
<li><strong>Gradient Descent</strong>
<ul>
<li>the general form is:
<ul>
<li>\(\begin{aligned}&amp; Repeat ; \lbrace \ &amp; ; \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta) \ &amp; \rbrace\end{aligned}\)</li>
</ul>
</li>
<li>We can work out the derivative part using calculus to get:
<ul>
<li>\(\begin{aligned} &amp; Repeat ; \lbrace \ &amp; ; \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \ &amp; \rbrace \end{aligned}\)</li>
</ul>
</li>
<li>A vectorized implementation is:
<ul>
<li>\(\theta := \theta - \frac{\alpha}{m} X^{T} (g(X \theta ) - \vec{y})\)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="advanced-optimization"><a class="markdownIt-Anchor" href="#advanced-optimization"></a> Advanced Optimization</h3>
<ul>
<li>
<p>Alternatively, instead of gradient descent to minimize the cost function we could use</p>
<ul>
<li><strong>Conjugate gradient</strong></li>
<li><strong>BFGS</strong> (Broyden-Fletcher-Goldfarb-Shanno)</li>
<li><strong>L-BFGS</strong> (Limited memory - BFGS)</li>
</ul>
</li>
<li>
<p>Some properties</p>
<ul>
<li><strong>Advantages</strong>
<ul>
<li>No need to manually pick alpha (learning rate)</li>
<li>Have a clever inner loop (line search algorithm) which tries a bunch of alpha values and picks a good one</li>
<li>Often faster than gradient descent</li>
<li>Do more than just pick a good learning rate</li>
<li>Can be used successfully without understanding their complexity</li>
</ul>
</li>
<li><strong>Disadvantages</strong>
<ul>
<li>Could make debugging more difficult</li>
<li>Should not be implemented themselves</li>
<li>Different libraries may use different implementations - may hit performance</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Using advanced cost minimization algorithms</p>
<ul>
<li>
<p>Example:</p>
<ul>
<li>\(\theta = \begin{bmatrix}\theta_1\ \theta_2\end{bmatrix}\)</li>
<li>\(J(\theta) = (\theta_1 - 5)^2 + (\theta_2 - 5)^2\)</li>
<li>\(\dfrac{\partial}{\partial \theta_1}J(\theta) = 2(\theta_1 - 5)\)</li>
<li>\(\dfrac{\partial}{\partial \theta_2}J(\theta) = 2(\theta_2 - 5)\)</li>
<li>Example above
<ul>
<li>\(θ_1\) and \(θ_2\) (two parameters)</li>
<li>Cost function here is \(J(\theta) = (\theta_1 - 5)^2 + (\theta_2 - 5)^2\)</li>
<li>The derivatives of the \(J(θ)\) with respect to either \(θ_1\) and \(θ_2\) turns out to be the \(2(θ_i - 5)\)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>First, define our cost function:</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; function [jVal, gradient] &#x3D; costFunction(theta)</span><br><span class="line">&gt;     jVal &#x3D; [...code to compute J(theta)...];</span><br><span class="line">&gt;     gradient &#x3D; [...code to compute derivative of J(theta)...];</span><br><span class="line">&gt; end</span><br></pre></td></tr></table></figure>
<ul>
<li><code>jVal</code> = \((\theta_1 - 5)^2 + (\theta_2 - 5)^2\)</li>
<li><code>gradient</code> is a 2 by 1 vector, and 2 elements are the two partial derivative terms</li>
</ul>
</li>
<li>
<p>Then,</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; options &#x3D; optimset(&#39;GradObj&#39;, &#39;on&#39;, &#39;MaxIter&#39;, 100);</span><br><span class="line">&gt; initialTheta &#x3D; zeros(2,1);</span><br><span class="line">&gt; [optTheta, functionVal, exitFlag] &#x3D; fminunc(@costFunction, initialTheta, options);</span><br></pre></td></tr></table></figure>
<ul>
<li>Here,
<ul>
<li><strong>options</strong> is a data structure giving options for the algorithm</li>
<li><strong>fminunc</strong>
<ul>
<li>function minimize the cost function (<strong>f</strong>ind <strong>min</strong>imum of <strong>unc</strong>onstrained multivariable function)</li>
</ul>
</li>
<li><strong>@costFunction</strong> is a pointer to the costFunction function to be used</li>
</ul>
</li>
<li>For the octave implementation
<ul>
<li><strong>initialTheta</strong> must be a matrix of at least two dimensions</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="multiclass-classification-one-vs-all"><a class="markdownIt-Anchor" href="#multiclass-classification-one-vs-all"></a> Multiclass Classification: One-vs-all</h2>
<ul>
<li>Divide our problem into n+1 (+1 because the index starts at 0) binary classification problems; in each one, we predict the probability that <code>y</code> is a member of one of our classes.
<ul>
<li>\(\begin{aligned}&amp; y \in \lbrace0, 1 … n\rbrace \&amp; h_\theta^{(0)}(x) = P(y = 0 | x ; \theta) \&amp; h_\theta^{(1)}(x) = P(y = 1 | x ; \theta) \&amp; \cdots \&amp; h_\theta^{(n)}(x) = P(y = n | x ; \theta) \&amp; \mathrm{prediction} = \max_i( h_\theta ^{(i)}(x) )\\end{aligned}\)</li>
<li>The following image show how one could classify 3 classes:</li>
<li><img src="https://i.imgur.com/dUx6MP7.png" alt="week-3-8" /></li>
</ul>
</li>
<li>Overall
<ul>
<li>Train a logistic regression classifier \(h_{θ}^{(i)}(x)\) for each class i to predict the probability that \(y = i\)</li>
<li>On a new input, \(x\) to make a prediction, pick the class \(i\) that maximizes the probability that \(h_θ^{(i)}(x) = 1\)</li>
</ul>
</li>
</ul>
<h2 id="the-problem-of-overfitting"><a class="markdownIt-Anchor" href="#the-problem-of-overfitting"></a> The Problem of Overfitting</h2>
<h3 id="problems"><a class="markdownIt-Anchor" href="#problems"></a> Problems:</h3>
<ul>
<li>Three figures to shows that <strong>underfitting</strong>, <strong>fitting</strong> and <strong>overfitting</strong>: (take housing price as sample)
<ul>
<li><img src="https://i.imgur.com/1vYilQz.png" alt="week-3-9" /></li>
<li>under-fitting or high bias: leftmost, \(y = θ_0 + θ_1x\) , doesn’t really lie on straight line.</li>
<li>overfitting: rightmost, \(y = \sum_{j=0} ^5 \theta_j x^j\) , not a good predictor.</li>
<li>fitting one: \(y = \theta_0 + \theta_1x + \theta_2x^2\) , obtain a slightly better fit to the data.</li>
</ul>
</li>
</ul>
<h3 id="addressing-overfitting"><a class="markdownIt-Anchor" href="#addressing-overfitting"></a> Addressing overfitting</h3>
<ul>
<li>Reduce the number of features:
<ul>
<li>Manually select which features to keep.</li>
<li>Use a model selection algorithm.</li>
</ul>
</li>
<li>Regularization
<ul>
<li>Keep all the features, but reduce the magnitude of parameters \(\theta_j\) .</li>
<li>Regularization works well when we have a lot of slightly useful features.</li>
</ul>
</li>
</ul>
<h3 id="cost-function-2"><a class="markdownIt-Anchor" href="#cost-function-2"></a> Cost Function</h3>
<ul>
<li>if we have overfitting from our hypothesis function , we can reduce the weight that some of the terms in our function carry by increasing their cost.</li>
<li>Say we wanted to make the following function more quadratic:
<ul>
<li>\(\theta_0 + \theta_1x + \theta_2x^2 + \theta_3x^3 + \theta_4x^4\)</li>
</ul>
</li>
<li>We’ll want to eliminate the influence of \(\theta_3x^4\) and \(\theta_4x^4\) . Without actually getting rid of these features or changing the form of our hypothesis, we can instead modify our cost function:
<ul>
<li>\(min_\theta\ \dfrac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)}) - y<sup>{(i)})</sup>2 + 1000\cdot\theta_3^2 + 1000\cdot\theta_4^2\)</li>
<li>Add two extra terms at the end to inflate the cost of \(\theta_3\) and \(\theta_4\) . This will in turn greatly reduce the values of \(\theta_3x^4\) and \(\theta_4x^4\) in our hypothesis function.</li>
</ul>
</li>
<li>We could also regularize all of our theta parameters in a single summation as:
<ul>
<li>\(min_\theta\ \dfrac{1}{2m}\ \left[ \sum_{i=1}^m (h_\theta(x^{(i)}) - y<sup>{(i)})</sup>2 + \lambda\ \sum_{j=1}^n \theta_j^2 \right]\)</li>
<li>\(\lambda\) (lambda), is the <strong>regularization parameter</strong>. It determines how much the costs of our theta parameters are inflated.</li>
<li>But if lambda is chosen to be too large, it may smooth out the function too much and cause under-fitting.</li>
</ul>
</li>
</ul>
<h3 id="regularized-linear-regression"><a class="markdownIt-Anchor" href="#regularized-linear-regression"></a> Regularized Linear Regression</h3>
<ul>
<li>
<p>Gradient Descent</p>
</li>
<li>
<blockquote>
<p>\(\begin{aligned} &amp; \text{Repeat}\ \lbrace \ &amp; \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y<sup>{(i)})x_0</sup>{(i)} \ &amp; \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y<sup>{(i)})x_j</sup>{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2…n\rbrace\ &amp; \rbrace \end{aligned}\)</p>
</blockquote>
<ul>
<li>The term \(\frac{\lambda}{m}\theta_j\) performs our regularization. With some manipulation our update rule can also be represented as:
<ul>
<li>\(\theta_j := \theta_j(1 - \alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\sum_{i=1}<sup>m(h_\theta(x</sup>{(i)}) - y<sup>{(i)})x_j</sup>{(i)}\)</li>
</ul>
</li>
<li>The first term in the above equation, \(1 - \alpha\frac{\lambda}{m}\) will always be less than 1. Intuitively you can see it as reducing the value of \(θ_j\) by some amount on every update. Notice that the second term is now exactly the same as it was before.</li>
</ul>
</li>
</ul>
<h4 id="normoal-equation"><a class="markdownIt-Anchor" href="#normoal-equation"></a> Normoal Equation</h4>
<ul>
<li>To add in reguarlization, the equation is the same as our original, except that we add another term inside the parentheses:
<ul>
<li>\(\begin{aligned}&amp; \theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty \&amp; \text{where}\ \ L = \begin{bmatrix} 0 &amp; &amp; &amp; &amp; \ &amp; 1 &amp; &amp; &amp; \ &amp; &amp; 1 &amp; &amp; \ &amp; &amp; &amp; \ddots &amp; \ &amp; &amp; &amp; &amp; 1\end{bmatrix}\end{aligned}\)</li>
</ul>
</li>
<li>L is a matrix with 0 at the top left and 1’s down the diagonal, with 0’s everywhere else. It should have dimension (n+1)×(n+1). Intuitively, this is the identity matrix (though we are not including \(x_0\) ), multiplied with a single real number \(\lambda \) .</li>
<li>Recall that if \(m \le n\) , then \(X^TX\) is non-invertible. However, when we add the term \(\lambda \cdot L\) , then \(X^TX + \lambda \cdot L\) becomes invertible.</li>
</ul>
<h3 id="regularized-logistic-regression"><a class="markdownIt-Anchor" href="#regularized-logistic-regression"></a> Regularized Logistic Regression</h3>
<ul>
<li>Cost function:
<ul>
<li>\(J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)})) \large]\)</li>
</ul>
</li>
<li>Regularize this equation by adding a term to the end:
<ul>
<li>\(J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2\)</li>
<li>The second sum, \(\sum_{j=1}^n \theta_j^2\) means to explicitly exclude the bias term, \(\theta_0\) . I.e. the \(\theta\) vector is indexed from 0 to n (holding n+1 values, \(\theta_0\) through \(\theta_n\) ), and this sum explicitly skips \(\theta_0\) , by running from 1 to n, skipping 0 (This is because for regularization we don’t penalize \(θ_0\) so treat it slightly differently). Thus, when computing the equation, we should continuously update the two following equations:</li>
<li><img src="https://i.imgur.com/vfaLQ8g.png" alt="week-3-10" /></li>
</ul>
</li>
</ul>
<h2 id="words"><a class="markdownIt-Anchor" href="#words"></a> Words</h2>
<p><strong>logistic</strong> [ləu’dʒistik] adj. [数] 符号逻辑的</p>
<p><strong>fraudulent</strong> ['frɔ:djulənt] adj. 欺骗性的；不正的</p>
<p><strong>malignant</strong> [mə’liɡnənt] adj. [医] 恶性的；</p>
<p><strong>benign</strong> [bi’nain] adj. 良性的；</p>
<p><strong>polynomial</strong> [,pɔli’nəumiəl] n. [数] 多项式；</p>
<p><strong>quadratic</strong> [kwɔ’drætik] adj. [数] 二次的 n. 二次方程式;</p>
<p><strong>penalize</strong>  ['pi:nəlaiz] vt. 处罚；处刑；使不利</p>
<p><strong>sigmoid function</strong> ['sigmɔid] 双弯曲函数</p>

</div>


  <div class="book-comments">
    




  </div>



<script src="/js/book-post.js"></script>

        </div>
      </div>
      <div class="column col-2 hide-lg">
        <div class="book-post-info">
  
    <div class="book-post-meta">

  <div class="author">

    <!-- Author image -->
    <div class="author-img">
      
        <figure
          class="avatar avatar-lg"
          data-initial="E"
          style="background-color: #3b4351;">
        </figure>
      
    </div>

    <!-- Author title -->
    <div class="author-title">
      <div>Eric Yang</div>
      <div>2020-04-13</div>
    </div>
  </div>

  

  <div class="divider"></div>
</div>
  

  <div class="book-tocbot">
</div>
<div class="book-tocbot-menu">
  <a class="book-toc-expand" onclick="expand_toc()">Expand all</a>
  <a onclick="go_top()">Back to top</a>
  <a onclick="go_bottom()">Go to bottom</a>
</div>


<script src="/js/book-toc.js"></script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "\\(", right: "\\)", display: false},
                {left: "\\[", right: "\\]", display: true}
            ]
        });
    });
</script>

</div>

      </div>
    </div>
  </div>
  
  <a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>

</body>
</html>


<script src="/js/book.js"></script>
