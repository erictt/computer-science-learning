<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">

    
      <link rel="icon" href="/favicon.png" />
    

    <title>
        
          stanford-machine-learning/week-9a - Eric&#39;s CS Notes
        
    </title>

    <!-- Spectre.css framework -->
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-exp.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-icons.min.css">

    <!-- theme css & js -->
    
<link rel="stylesheet" href="/css/book.css">

    
<script src="/js/book.js"></script>


    <!-- tocbot -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css">
    
    <!-- katex -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">

    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/zooming/2.1.1/zooming.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    const zooming = new Zooming()
    zooming.listen('.book-content img')
})
</script>

<meta name="generator" content="Hexo 4.2.0"></head>

<body>

<div class="book-container">
  <div class="book-sidebar">
    <div class="book-brand">
  <a href="/">
    <img src="/favicon.png">
    <span>ERIC&#39;S CS NOTES</span>
  </a>
</div>
    <div class="book-menu">
  <h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2>
<ul>
<li><a href="/README/index.html">Introduction</a></li>
<li><a href="/SYMBOLS/index.html">Symbols of Mathematics</a></li>
</ul>
<!--
## Stanford Statistical Learning

* [Chapter 2 - Overview of Statistical Learning](/statistical-learning/chapter-2/index.html)
* Chapter 3 - Linear Regression
* Chapter 4 - Classification
* Chapter 5 - Resampling Methods
* Chapter 6 - Linear Model Selection and Regularization
* Chapter 7 - Moving Beyond Linearity
* Chapter 8 - Tree-Based Methods
* Chapter 9 - Support Vector Machines
* Chapter 10 - Unsupervised Learning
-->
<h2 id="introduction-to-probability"><a class="markdownIt-Anchor" href="#introduction-to-probability"></a> Introduction to Probability</h2>
<ul>
<li><a href="/introduction-to-probability/unit-1/index.html">Unit 1: Probability models and axioms</a></li>
<li><a href="/introduction-to-probability/unit-2/index.html">Unit 2: Conditioning and independence</a></li>
<li><a href="/introduction-to-probability/unit-3/index.html">Unit 3: Counting</a></li>
</ul>
<!--
* [Unit 4: Discrete random variables](/introduction-to-probability/unit-4/index.html)
* [Unit 5: Continuous random variables](/introduction-to-probability/unit-5/index.html)
* [Unit 6: Further topics on random variables](/introduction-to-probability/unit-6/index.html)
* [Unit 7: Bayesian inference](/introduction-to-probability/unit-7/index.html)
* [Unit 8: Limit theorems and classical statistics](/introduction-to-probability/unit-8/index.html)
* [Unit 9: Bernoulli and Poisson processes](/introduction-to-probability/unit-9/index.html)
* [Unit 10: Markov chains](/introduction-to-probability/unit-10/index.html)
-->
<h2 id="multivariable-calculus"><a class="markdownIt-Anchor" href="#multivariable-calculus"></a> Multivariable Calculus</h2>
<ul>
<li><a href="/multivariable-calculus/unit-1/index.html">Unit 1: Thinking about multivariable functions</a></li>
<li><a href="/multivariable-calculus/unit-2/index.html">Unit 2: Derivatives of multivariable functions</a></li>
</ul>
<!--
* [Unit 3: Applications of multivariable derivatives](/multivariable-calculus/unit-3/index.html)
* [Unit 4: Integrating multivariable functions](/multivariable-calculus/unit-4/index.html)
* [Unit 5: Green's, Stokes', and the divergence theorems](/multivariable-calculus/unit-5/index.html)
-->
<h2 id="algorithms-part-ii"><a class="markdownIt-Anchor" href="#algorithms-part-ii"></a> Algorithms: Part II</h2>
<ul>
<li><a href="/algorithms-2/week-1/index.html">Week 1 - Undirected Graph &amp; Directed Graph</a></li>
<li><a href="/algorithms-2/week-3/index.html">Week 3 - Maximum Flow and Minimum Cut &amp; Radix Sort</a></li>
<li><a href="/algorithms-2/week-4/index.html">Week 4 - Tries &amp; Substring Search</a></li>
</ul>
<h2 id="algorithms-part-i"><a class="markdownIt-Anchor" href="#algorithms-part-i"></a> Algorithms: Part I</h2>
<ul>
<li><a href="/algorithms-1/week-1/index.html">Week 1 - Union-Find &amp; Analysis of Algorithms</a></li>
<li><a href="/algorithms-1/week-2/index.html">Week 2 - Stacks and Queues &amp; Elementary Sorts</a></li>
<li><a href="/algorithms-1/week-3/index.html">Week 3 - Mergesort &amp; Quicksort</a></li>
<li><a href="/algorithms-1/week-4/index.html">Week 4 - Priority Queues &amp; Elementary Symbols</a></li>
<li><a href="/algorithms-1/week-5/index.html">Week 5 - Balanced Search Trees</a></li>
<li><a href="/algorithms-1/week-6/index.html">Week 6 - Hash Tables</a></li>
</ul>
<h2 id="introduction-to-software-design-and-architecture"><a class="markdownIt-Anchor" href="#introduction-to-software-design-and-architecture"></a> Introduction to Software Design and Architecture</h2>
<ul>
<li><a href="/introduction-to-software-design-and-architecture/design-pattern/index.html">Design Pattern</a></li>
</ul>
<h2 id="calculus-two-sequences-and-series"><a class="markdownIt-Anchor" href="#calculus-two-sequences-and-series"></a> Calculus Two: Sequences and Series</h2>
<ul>
<li><a href="/calculus-two/week-1/index.html">Week 1 - Sequences</a></li>
<li><a href="/calculus-two/week-2/index.html">Week 2 - Series</a></li>
<li><a href="/calculus-two/week-3/index.html">Week 3 - Convergence Tests</a></li>
<li><a href="/calculus-two/week-4/index.html">Week 4 - Alternating Series</a></li>
<li><a href="/calculus-two/week-5/index.html">Week 5 - Power Series</a></li>
<li><a href="/calculus-two/week-6/index.html">Week 6 - Taylor Series</a></li>
</ul>
<h2 id="laff-linear-algebra"><a class="markdownIt-Anchor" href="#laff-linear-algebra"></a> LAFF Linear Algebra</h2>
<ul>
<li><a href="/laff-linear-algebra/week-1/index.html">Week 1 - Vectors in Linear Algebra</a></li>
<li><a href="/laff-linear-algebra/week-2/index.html">Week 2 - Linear Transformations and Matrices</a></li>
<li><a href="/laff-linear-algebra/week-3/index.html">Week 3 - Matrix-Vector Operations</a></li>
<li><a href="/laff-linear-algebra/week-4/index.html">Week 4 - Matrix-Vector to Matrix-Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-5/index.html">Week 5 - Matrix- Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-6/index.html">Week 6 - Gaussian Elimination</a></li>
<li><a href="/laff-linear-algebra/week-7/index.html">Week 7 - More Gaussian Elimination and Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-8/index.html">Week 8 - More on Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-9/index.html">Week 9 - Vector Spaces</a></li>
<li><a href="/laff-linear-algebra/week-10/index.html">Week 10 - Vector Spaces, Orthogonality, and Linear Least-Squares</a></li>
<li><a href="/laff-linear-algebra/week-11/index.html">Week 11 - Orthogonal Projection, Low Rank Approximation, and Orthogonal Bases</a></li>
<li><a href="/laff-linear-algebra/week-12/index.html">Week 12 - Eigenvalues and Eigenvectors</a></li>
</ul>
<h2 id="stanford-machine-learning"><a class="markdownIt-Anchor" href="#stanford-machine-learning"></a> Stanford Machine Learning</h2>
<ul>
<li><a href="/stanford-machine-learning/week-1/index.html">Week 1 - Introduction</a></li>
<li><a href="/stanford-machine-learning/week-2/index.html">Week 2 - Linear Regression with Multiple Variables</a></li>
<li><a href="/stanford-machine-learning/week-3/index.html">Week 3 - Logistic Regression &amp; Regularization</a></li>
<li><a href="/stanford-machine-learning/week-4/index.html">Week 4 - Neural Networks: Representation</a></li>
<li><a href="/stanford-machine-learning/week-5/index.html">Week 5 - Neural Networks: Learning</a></li>
<li><a href="/stanford-machine-learning/week-6a/index.html">Week 6a - Advice for Applying Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-6b/index.html">Week 6b - Machine Learning System Design</a></li>
<li><a href="/stanford-machine-learning/week-7/index.html">Week 7 - Support Vector Machines</a></li>
<li><a href="/stanford-machine-learning/week-8/index.html">Week 8 - Unsupervised Learning &amp; Dimensionality Reduction</a></li>
<li><a href="/stanford-machine-learning/week-9a/index.html">Week 9a - Anomaly Detection</a></li>
<li><a href="/stanford-machine-learning/week-9b/index.html">Week 9b - Recommender Systems</a></li>
<li><a href="/stanford-machine-learning/week-10/index.html">Week 10 - Large Scale Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-11/index.html">Week 11 - Application Example: Photo OCR</a></li>
</ul>
<h2 id="calculus-one"><a class="markdownIt-Anchor" href="#calculus-one"></a> Calculus One</h2>
<ul>
<li><a href="/calculus-one/week-2-3/index.html">Week 2-3 - Functions &amp; Limits</a></li>
<li><a href="/calculus-one/week-4/index.html">Week 4 - The Beginning of Derivatives</a></li>
<li><a href="/calculus-one/week-5/index.html">Week 5 - Techniques of Differentiation</a></li>
<li><a href="/calculus-one/week-6/index.html">Week 6 - Chain Rule</a></li>
<li><a href="/calculus-one/week-7/index.html">Week 7 - Derivatives of Trigonometric Functions</a></li>
<li><a href="/calculus-one/week-8/index.html">Week 8 - Derivatives in the Real World</a></li>
<li><a href="/calculus-one/week-9/index.html">Week 9 - Optimization</a></li>
<li><a href="/calculus-one/week-10/index.html">Week 10 - Linear Approximation</a></li>
<li><a href="/calculus-one/week-11-12/index.html">Week 11-12 - Antidifferentiation &amp; Integration</a></li>
<li><a href="/calculus-one/week-13/index.html">Week 13 - Fundamental Theorem of Calculus</a></li>
<li><a href="/calculus-one/week-14/index.html">Week 14 - Substitution Rule</a></li>
<li><a href="/calculus-one/week-15/index.html">Week 15 - Techniques of Integration</a></li>
<li><a href="/calculus-one/week-16/index.html">Week 16 - Applications of Integration</a></li>
</ul>
<h2 id="computational-thinking"><a class="markdownIt-Anchor" href="#computational-thinking"></a> Computational Thinking</h2>
<ul>
<li><a href="/computational-thinking/lecture-1/index.html">Lecture 1 - Optimization and Knapsack Problem</a></li>
<li><a href="/computational-thinking/lecture-2/index.html">Lecture 2 - Decision Trees and Dynamic Programming</a>
<ul>
<li><a href="/computational-thinking/lecture-2-powerset/index.html">Exercise: Power Set Function</a></li>
</ul>
</li>
<li><a href="/computational-thinking/lecture-3/index.html">Lecture 3 - Graphs</a></li>
<li><a href="/computational-thinking/lecture-4-5/index.html">Lecture 4-5 - Plotting</a></li>
<li><a href="/computational-thinking/lecture-6-7/index.html">Lecture 6-7 - Stochastic Programs &amp; Inferential Statistics</a></li>
<li><a href="/computational-thinking/lecture-8/index.html">Lecture 8 - Monte Carlo Simulation</a></li>
<li><a href="/computational-thinking/lecture-9/index.html">Lecture 9 - Sampling and Standard Error</a></li>
<li><a href="/computational-thinking/lecture-10-11/index.html">Lecture 10-11 - Experimental Data</a></li>
<li><a href="/computational-thinking/lecture-12/index.html">Lecture 12 - Machine Learning</a></li>
<li><a href="/computational-thinking/lecture-13/index.html">Lecture 13 - Statistical Abuses</a></li>
</ul>
<h2 id="effective-thinking-through-mathematics"><a class="markdownIt-Anchor" href="#effective-thinking-through-mathematics"></a> Effective Thinking Through Mathematics</h2>
<ul>
<li><a href="/effective-thinking-through-mathematics/note/index.html">Note</a></li>
<li><a href="/effective-thinking-through-mathematics/week-4-telling-the-story-of-infinity/index.html">Week 4 (/Telling the Story of Infinity)</a></li>
<li><a href="/effective-thinking-through-mathematics/week-5-telling-the-story-of-the-euler-circuit-theorem/index.html">Week 5 (/Telling the Story of Euler Circuit Theorem)</a></li>
</ul>
<h2 id="cs50-introduction-to-computer-science"><a class="markdownIt-Anchor" href="#cs50-introduction-to-computer-science"></a> CS50 Introduction to Computer Science</h2>
<ul>
<li><a href="/cs50/week-1/index.html">Week 1 - C</a></li>
<li><a href="/cs50/week-2/index.html">Week 2 - Arrays</a></li>
<li><a href="/cs50/week-3/index.html">Week 3 - Algorithms</a></li>
<li><a href="/cs50/week-4/index.html">Week 4 - Memory</a></li>
<li><a href="/cs50/week-5/index.html">Week 5 - Data Structures</a></li>
<li><a href="/cs50/week-6/index.html">Week 6 - HTTP</a></li>
<li><a href="/cs50/week-7-10/index.html">Week 7-10 - Machine Learning/Python/SQL/Javascript</a></li>
</ul>

</div>


<script src="/js/book-menu.js"></script>

  </div>

  <div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseover="add_inner()" onmouseleave="remove_inner()">
  <div class="sidebar-toggle-inner"></div>
</div>

<script>
function add_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.add('show')  
}

function remove_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.remove('show')
}

function sidebar_toggle() {
    let sidebar_toggle = document.querySelector('.sidebar-toggle')
    let sidebar = document.querySelector('.book-sidebar')
    let content = document.querySelector('.off-canvas-content')
    if (sidebar_toggle.classList.contains('extend')) { // show
        sidebar_toggle.classList.remove('extend')
        sidebar.classList.remove('hide')
        content.classList.remove('extend')
    }
    else { // hide
        sidebar_toggle.classList.add('extend')
        sidebar.classList.add('hide')
        content.classList.add('extend')
    }
}
</script>

  <div class="off-canvas-content">
    <div class="columns">
      <div class="column col-10 col-lg-12">
        <div class="book-navbar">
          <!-- For Responsive Layout -->

<header class="navbar">
  <section class="navbar-section">
    <a onclick="open_sidebar()">
      <i class="icon icon-menu"></i>
    </a>
  </section>
</header>

        </div>
        <div class="book-content">
          <div class="book-post">
  <h1 id="week-9a-anomaly-detection"><a class="markdownIt-Anchor" href="#week-9a-anomaly-detection"></a> Week 9a - Anomaly Detection</h1>
<h2 id="density-estimation"><a class="markdownIt-Anchor" href="#density-estimation"></a> Density Estimation</h2>
<h3 id="problem-motivation"><a class="markdownIt-Anchor" href="#problem-motivation"></a> Problem Motivation</h3>
<ul>
<li>Fraud detection:
<ul>
<li>\(x^{(i)}\) = features of user \(i\)'s activities on the website.</li>
<li>Model \(p(x)\) from data.</li>
<li>Identify unusual users by checking which have \(p(x) &lt; \epsilon \)
<ul>
<li>if \(p(x) &lt; \epsilon\) flag this as an anomaly</li>
<li>otherwise, this is OK</li>
</ul>
</li>
</ul>
</li>
<li>Manufacturing
<ul>
<li>Aircraft engine manufacturer
<ul>
<li>Check if the new engine is anomalous</li>
</ul>
</li>
</ul>
</li>
<li>Monitoring computers in a data center.
<ul>
<li>\(x^{(i)}\) = features of machine \(i\)</li>
<li>\(x_1\) = memory use,</li>
<li>\(x_2\) = number of disk accesses/sec,</li>
<li>\(x_3\) = CPU load,</li>
<li>\(x_4\) = CPU load/network traffic.</li>
<li>…</li>
</ul>
</li>
</ul>
<h3 id="gaussiannormal-distribution"><a class="markdownIt-Anchor" href="#gaussiannormal-distribution"></a> Gaussian(Normal) Distribution</h3>
<ul>
<li>
<p>Have learned it from <a href="https://cs.ericyy.me/computational-thinking/lecture-6-7/index.html" target="_blank" rel="noopener">Lecture 6(Computational Thinking)</a></p>
</li>
<li>
<p>Notation <a href="https://en.wikipedia.org/wiki/Normal_distribution" target="_blank" rel="noopener">From Wikipedia</a></p>
<ul>
<li>When a random variable \(X\) is distributed normally with mean \(\mu\)  and variance \(\sigma ^{2}\), one may write \[{\displaystyle X\ \sim \ {\mathcal {N}}(\mu ,\sigma ^{2}).}\]</li>
<li>The probability of x can be written as \[p(x;\mu, \sigma^2) = \frac{1}{\sqrt{2\pi} \sigma} \exp(-\frac{x-\mu<sup>2}{2\sigma</sup>2})\]</li>
</ul>
</li>
</ul>
<h3 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h3>
<ol>
<li>Choose features \(x_i\) that you think might be indicative of anomalous examples.</li>
<li>Fit parameters \(\mu_1, \ldots, \mu_n, \sigma_1^2, \ldots, \sigma_n^2 \)
<ul>
<li>\[\mu_j = \frac{1}{m}\sum_{i=1}^m x_j^{(i)}\]</li>
<li>\[\sigma_j^2 = \frac{1}{m}\sum_{i=1}^m (x_j^{(i)} - \mu_j)^2\]</li>
</ul>
</li>
<li>Given new example x, compute \(p(x)\):
<ul>
<li>\[\begin{aligned}p(x) &amp;= p(x_1;\mu_1,\sigma_1^2) \cdot p(x_2;\mu_2,\sigma_2^2) \cdot p(x_3;\mu_3,\sigma_3^2) \cdot \ldots \cdot p(x_n;\mu_n,\sigma_n^2) \<br />
&amp;= \prod_{j=1}^n p(x_j;\mu_j, \sigma_j^2)<br />
\end{aligned}\]</li>
<li>Anomaly if \(p(x) &lt; \epsilon\).</li>
</ul>
</li>
</ol>
<ul>
<li>The problem of estimating this distribution <strong>p</strong> of <strong>x</strong>, sometimes called <strong>the problem of density estimation</strong>.</li>
</ul>
<h4 id="example"><a class="markdownIt-Anchor" href="#example"></a> Example</h4>
<ul>
<li>
<img src="https://i.imgur.com/Hh2szfM.jpg" style="width:500px" />
</li>
</ul>
<h2 id="building-an-anomaly-detection-system"><a class="markdownIt-Anchor" href="#building-an-anomaly-detection-system"></a> Building an Anomaly Detection System</h2>
<h3 id="developing-and-evaluating-an-anomaly-detection-system"><a class="markdownIt-Anchor" href="#developing-and-evaluating-an-anomaly-detection-system"></a> Developing and Evaluating an Anomaly Detection System</h3>
<ul>
<li>Assume we have some labeled data, of anomalous and non-anomalous examples. (y =0 if normal, y = 1 if anomalous).</li>
<li>Training set: \(x^{(1)}, x^{(2)}, \ldots, x^{(m)}\) (assume normal examples/not anomalous)</li>
<li>Cross validation set: \((x^{(1)}<em>{cv}, y^{(1)}</em>{cv}), \ldots, (x^{(m_{cv})}<em>{cv}, y^{(m</em>{cv})}_{cv})\)</li>
<li>Test set: \((x^{(1)}<em>{test}, y^{(1)}</em>{test}), \ldots, (x^{(m_{test})}<em>{test}, y^{(m</em>{test})}_{test})\)</li>
</ul>
<h4 id="aircraft-engines-motivating-example"><a class="markdownIt-Anchor" href="#aircraft-engines-motivating-example"></a> Aircraft engines motivating example</h4>
<ul>
<li>Data set:
<ul>
<li>10000 good(normal) engines</li>
<li>20 flawed engines (anomalous)</li>
</ul>
</li>
<li>Separate Data set:
<ul>
<li>Training set: 6000 good engines</li>
<li>CV: 2000 good engines (y = 0), 10 anomalous (y = 1)</li>
<li>Test: 2000 good engines (y = 0), 10 anomalous (y = 1)</li>
</ul>
</li>
<li>Algorithm evaluation
<ul>
<li>Fit model \(p(x)\) on training set \({ x^{(1)}, \ldots, x^{(m)} }\)</li>
<li>On a cross validation/test examples \(x\), predict \[y = \begin{cases}<br />
1 &amp;\text{if}\ p(x) &lt; \epsilon \text{(anomaly)}\<br />
0 &amp;\text{if}\ p(x) \ge \epsilon \text{(normal)}<br />
\end{cases}\]</li>
<li>Possible evaluation metrics:
<ul>
<li>True positive, false positive, false negative, true negative</li>
<li>Precision/Recall</li>
<li>\(F_1\)-score</li>
</ul>
</li>
<li>Can also use cross validation set to choose parameter \(\epsilon\)</li>
</ul>
</li>
</ul>
<h3 id="anomaly-detection-vs-supervised-learning"><a class="markdownIt-Anchor" href="#anomaly-detection-vs-supervised-learning"></a> Anomaly Detection vs Supervised Learning</h3>
<ul>
<li>Anomaly Detection
<ul>
<li>Very small number of positive examples (y=1). (0-20 is common).</li>
<li>Large number of negative (y=0) examples.</li>
<li>Many different “types” of anomalies. Hard for any algorithm to learn from positive examples what the anomalies look like;</li>
<li>Further anomalies may look nothing like any of the anomalous examples we’ve seen so far.</li>
</ul>
</li>
<li>Supervised Learning
<ul>
<li>Large number of positive and negatives.</li>
<li>Enough positive examples for algorithm to get a sense of what positive examples are like, future positive examples likely to be similar to ones in training set.</li>
</ul>
</li>
<li>Examples can use Anomaly detection
<ul>
<li>Fraud detection</li>
<li>Manufacturing (e.g. aircraft engines)</li>
<li>Monitering machines in a data center</li>
</ul>
</li>
<li>Examples can use Supervised learning
<ul>
<li>Email spam classification</li>
<li>Weather prediction</li>
<li>Cancer classification</li>
</ul>
</li>
</ul>
<h3 id="choosing-what-features-to-use"><a class="markdownIt-Anchor" href="#choosing-what-features-to-use"></a> Choosing What Features to Use</h3>
<ul>
<li>
<p><strong>Non-gaussian features</strong></p>
<ul>
<li>Plot a histogram of data to check if it has a Gaussian description. If it doesn’t, we can transfer the feature with:
<ul>
<li>\(\log(x+C)\), where <strong>C</strong> is a constant</li>
<li>\(x^{\frac{1}{2}}\)</li>
<li>\(x^{\frac{1}{3}}\)</li>
<li>…</li>
<li>to make it more Gaussian
<ul>
<li>
<img src="https://i.imgur.com/TcIZjMZ.jpg" style="width:500px" />
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Error analysis for anomaly detection</strong></p>
<ul>
<li>
<p>When we get a wrong result on CV test, need to find out why the algorithm doesn’t work.</p>
</li>
<li>
<p>For example:</p>
<ul>
<li>We have one dimension, and our anomalous value is sort of buried in it (in green - Gaussian superimposed in blue)</li>
<li>So we need to come out a new feature to make this data set in two dimensions, and hopefully, the green spot will be out of our normal range.</li>
<li>
<img src="https://i.imgur.com/PtfZCmU.jpg" style="width:450px" />
</li>
</ul>
</li>
<li>
<p>Another example:</p>
<ul>
<li>Monitoring computers in a data center</li>
<li>Choose features that might take on unusually large or small values in the event of an anomaly.
<ul>
<li>\(x_1\) = memory use of computer</li>
<li>\(x_2\) = number of disk accesses/sec</li>
<li>\(x_3\) = CPU load</li>
<li>\(x_4\) = network traffic</li>
</ul>
</li>
<li>If some program are running infinity, we will get a high CPU load, and low network traffic. But the features above can’t detect this issue, so we come out a new feature = \(\frac{\text{CPU load}}{\text{network traffic}}\)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="multivariate-gaussian-distribution"><a class="markdownIt-Anchor" href="#multivariate-gaussian-distribution"></a> Multivariate Gaussian Distribution</h2>
<ul>
<li>\(x \in \mathbb{R}^n\).</li>
<li>Formula: \[p(x;\mu,\Sigma)= \frac{1}{\sqrt { (2\pi)^n| \Sigma| } }  \exp\left(-{1 \over 2} (\mathbf{x}-\mu)^{\rm T} \Sigma^{-1} ({\mathbf x}-\mu)\right)\]
<ul>
<li>\(\mu \in \mathbb{R}^n, \Sigma \in \mathbb{R}^{n \times n}\) (covariance matrix)</li>
<li>\(\lvert \Sigma \rvert\): matrix determinant. In Matlab: <code>det(Sigma)</code></li>
</ul>
</li>
<li>Examples
<ul>
<li>
<img src="https://i.imgur.com/6ol1UBo.jpg" style="width:500px" />
</li>
<li>
<img src="https://i.imgur.com/hdC1t0i.jpg" style="width:500px" />
</li>
<li>
<img src="https://i.imgur.com/vL0WPC5.jpg" style="width:500px" />
</li>
</ul>
</li>
</ul>
<h3 id="anomaly-detection-using-the-multivariate-gaussian-distribution"><a class="markdownIt-Anchor" href="#anomaly-detection-using-the-multivariate-gaussian-distribution"></a> Anomaly Detection using the Multivariate Gaussian Distribution</h3>
<ol start="0">
<li>Given training set \({ x^{(1)}, x^{(2)}, \ldots, x^{(m)} }\)</li>
<li>Fit model \(p(x)\) by setting
<ul>
<li>\(\displaystyle \mu = \frac{1}{m} \sum_{i=1}^m x^{(i)}\)</li>
<li>\(\displaystyle \Sigma = \frac{1}{m}\sum_{i=1}^m (x^{(i)} - \mu)(x^{(i)} - \mu)^T\)</li>
</ul>
</li>
<li>Given a new example \(x\), compute \[p(x) = \frac{1}{\sqrt { (2\pi)^n| \Sigma| } }  \exp\left(-{1 \over 2} (\mathbf{x}-\mu)^{\rm T} \Sigma^{-1} ({\mathbf x}-\mu)\right)\]
<ul>
<li>Flag an anomaly if \(p(x) &lt; \epsilon\)</li>
</ul>
</li>
</ol>
<h4 id="relationship-to-original-model"><a class="markdownIt-Anchor" href="#relationship-to-original-model"></a> Relationship to original model</h4>
<ul>
<li>Original model: \[p(x) = p(x_1;\mu_1,\sigma_1^2) \cdot p(x_2;\mu_2,\sigma_2^2) \cdot p(x_3;\mu_3,\sigma_3^2) \cdot \ldots \cdot p(x_n;\mu_n,\sigma_n^2)\] Corresponds to multivariate Gaussian \[p(x;\mu,\Sigma)= \frac{1}{\sqrt { (2\pi)^n| \Sigma| } }  \exp\left(-{1 \over 2} (\mathbf{x}-\mu)^{\rm T} \Sigma^{-1} ({\mathbf x}-\mu)\right)\] where \[\Sigma = \begin{bmatrix}\sigma_1^2 &amp; 0 &amp; 0 &amp; 0 \ 0 &amp; \sigma_2^2 &amp; 0 &amp; 0 \ 0 &amp; 0 &amp; \ddots &amp; 0 \ 0 &amp; 0 &amp; 0 &amp; \sigma_n^2 \end{bmatrix}\]</li>
</ul>
<h3 id="original-model-vs-multivariate-gaussian"><a class="markdownIt-Anchor" href="#original-model-vs-multivariate-gaussian"></a> Original model vs Multivariate Gaussian</h3>
<ul>
<li>Original model
<ul>
<li>[Disadvantage] Manually create features to capture anomalies where \(x_1, x_2\) take unusual combinations of values.</li>
<li>[Advantage] Computationally cheaper</li>
<li>[Advantage] Works fine even <strong>m</strong>(training set size) is small.</li>
</ul>
</li>
<li>Multivariate Gaussian
<ul>
<li>[Advantage] Automatically captures correlations between features</li>
<li>[Disadvantage] Computationally more expensive
<ul>
<li>try to reduce the feature size</li>
</ul>
</li>
<li>[Disadvantage] Must have \(m &gt; n\), or \(\Sigma\) is not-invertible.
<ul>
<li>In practice, we should make sure \(m \ge 10n\).</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="words"><a class="markdownIt-Anchor" href="#words"></a> Words</h2>
<ul>
<li><strong>covariance matrix</strong> 协方差矩阵</li>
<li><strong>off-diagonal</strong> adj. [数] 非对角的，[数] 对角线外的</li>
</ul>

</div>


  <div class="book-comments">
    




  </div>



<script src="/js/book-post.js"></script>

        </div>
      </div>
      <div class="column col-2 hide-lg">
        <div class="book-post-info">
  
    <div class="book-post-meta">

  <div class="author">

    <!-- Author image -->
    <div class="author-img">
      
        <figure
          class="avatar avatar-lg"
          data-initial="E"
          style="background-color: #3b4351;">
        </figure>
      
    </div>

    <!-- Author title -->
    <div class="author-title">
      <div>Eric Yang</div>
      <div>2020-04-13</div>
    </div>
  </div>

  

  <div class="divider"></div>
</div>
  

  <div class="book-tocbot">
</div>
<div class="book-tocbot-menu">
  <a class="book-toc-expand" onclick="expand_toc()">Expand all</a>
  <a onclick="go_top()">Back to top</a>
  <a onclick="go_bottom()">Go to bottom</a>
</div>


<script src="/js/book-toc.js"></script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "\\(", right: "\\)", display: false},
                {left: "\\[", right: "\\]", display: true}
            ]
        });
    });
</script>

</div>

      </div>
    </div>
  </div>
  
  <a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>

</body>
</html>


<script src="/js/book.js"></script>
