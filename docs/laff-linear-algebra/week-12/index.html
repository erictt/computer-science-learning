<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">

    
      <link rel="icon" href="/favicon.png" />
    

    <title>
        
          laff-linear-algebra/week-12 - Eric&#39;s CS Notes
        
    </title>

    <!-- Spectre.css framework -->
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-exp.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-icons.min.css">

    <!-- theme css & js -->
    
<link rel="stylesheet" href="/css/book.css">

    
<script src="/js/book.js"></script>


    <!-- tocbot -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css">
    
    <!-- katex -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/zooming/2.1.1/zooming.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    const zooming = new Zooming()
    zooming.listen('.book-content img')
})
</script>

<meta name="generator" content="Hexo 4.2.0"></head>


<body>

<div class="book-container">
  <div class="book-sidebar">
    <div class="book-brand">
  <a href="/">
    <img src="/favicon.png">
    <span>ERIC&#39;S CS NOTES</span>
  </a>
</div>
    <div class="book-menu">
  <h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2>
<ul>
<li><a href="/symbols/index.html">Symbols of Mathematics</a></li>
<li><a href="/glossary/index.html">Glossary</a></li>
</ul>
<!--
## Stanford Statistical Learning

* [Chapter 2 - Overview of Statistical Learning](/statistical-learning/chapter-2/index.html)
* Chapter 3 - Linear Regression
* Chapter 4 - Classification
* Chapter 5 - Resampling Methods
* Chapter 6 - Linear Model Selection and Regularization
* Chapter 7 - Moving Beyond Linearity
* Chapter 8 - Tree-Based Methods
* Chapter 9 - Support Vector Machines
* Chapter 10 - Unsupervised Learning
-->
<h2 id="introduction-to-probability"><a class="markdownIt-Anchor" href="#introduction-to-probability"></a> Introduction to Probability</h2>
<ul>
<li><a href="/introduction-to-probability/unit-1/index.html">Unit 1: Probability models and axioms</a></li>
<li><a href="/introduction-to-probability/unit-2/index.html">Unit 2: Conditioning and independence</a></li>
<li><a href="/introduction-to-probability/unit-3/index.html">Unit 3: Counting</a></li>
</ul>
<!--
* [Unit 4: Discrete random variables](/introduction-to-probability/unit-4/index.html)
* [Unit 5: Continuous random variables](/introduction-to-probability/unit-5/index.html)
* [Unit 6: Further topics on random variables](/introduction-to-probability/unit-6/index.html)
* [Unit 7: Bayesian inference](/introduction-to-probability/unit-7/index.html)
* [Unit 8: Limit theorems and classical statistics](/introduction-to-probability/unit-8/index.html)
* [Unit 9: Bernoulli and Poisson processes](/introduction-to-probability/unit-9/index.html)
* [Unit 10: Markov chains](/introduction-to-probability/unit-10/index.html)
-->
<h2 id="multivariable-calculus"><a class="markdownIt-Anchor" href="#multivariable-calculus"></a> Multivariable Calculus</h2>
<ul>
<li><a href="/multivariable-calculus/unit-1/index.html">Unit 1: Thinking about multivariable functions</a></li>
<li><a href="/multivariable-calculus/unit-2/index.html">Unit 2: Derivatives of multivariable functions</a></li>
</ul>
<!--
* [Unit 3: Applications of multivariable derivatives](/multivariable-calculus/unit-3/index.html)
* [Unit 4: Integrating multivariable functions](/multivariable-calculus/unit-4/index.html)
* [Unit 5: Green's, Stokes', and the divergence theorems](/multivariable-calculus/unit-5/index.html)
-->
<h2 id="algorithms-part-ii"><a class="markdownIt-Anchor" href="#algorithms-part-ii"></a> Algorithms: Part II</h2>
<ul>
<li><a href="/algorithms-2/week-1/index.html">Week 1 - Undirected Graph &amp; Directed Graph</a></li>
<li><a href="/algorithms-2/week-3/index.html">Week 3 - Maximum Flow and Minimum Cut &amp; Radix Sort</a></li>
<li><a href="/algorithms-2/week-4/index.html">Week 4 - Tries &amp; Substring Search</a></li>
</ul>
<h2 id="algorithms-part-i"><a class="markdownIt-Anchor" href="#algorithms-part-i"></a> Algorithms: Part I</h2>
<ul>
<li><a href="/algorithms-1/week-1/index.html">Week 1 - Union-Find &amp; Analysis of Algorithms</a></li>
<li><a href="/algorithms-1/week-2/index.html">Week 2 - Stacks and Queues &amp; Elementary Sorts</a></li>
<li><a href="/algorithms-1/week-3/index.html">Week 3 - Mergesort &amp; Quicksort</a></li>
<li><a href="/algorithms-1/week-4/index.html">Week 4 - Priority Queues &amp; Elementary Symbols</a></li>
<li><a href="/algorithms-1/week-5/index.html">Week 5 - Balanced Search Trees</a></li>
<li><a href="/algorithms-1/week-6/index.html">Week 6 - Hash Tables</a></li>
</ul>
<h2 id="introduction-to-software-design-and-architecture"><a class="markdownIt-Anchor" href="#introduction-to-software-design-and-architecture"></a> Introduction to Software Design and Architecture</h2>
<ul>
<li><a href="/introduction-to-software-design-and-architecture/design-pattern/index.html">Design Pattern</a></li>
</ul>
<h2 id="calculus-two-sequences-and-series"><a class="markdownIt-Anchor" href="#calculus-two-sequences-and-series"></a> Calculus Two: Sequences and Series</h2>
<ul>
<li><a href="/calculus-two/week-1/index.html">Week 1 - Sequences</a></li>
<li><a href="/calculus-two/week-2/index.html">Week 2 - Series</a></li>
<li><a href="/calculus-two/week-3/index.html">Week 3 - Convergence Tests</a></li>
<li><a href="/calculus-two/week-4/index.html">Week 4 - Alternating Series</a></li>
<li><a href="/calculus-two/week-5/index.html">Week 5 - Power Series</a></li>
<li><a href="/calculus-two/week-6/index.html">Week 6 - Taylor Series</a></li>
</ul>
<h2 id="laff-linear-algebra"><a class="markdownIt-Anchor" href="#laff-linear-algebra"></a> LAFF Linear Algebra</h2>
<ul>
<li><a href="/laff-linear-algebra/week-1/index.html">Week 1 - Vectors in Linear Algebra</a></li>
<li><a href="/laff-linear-algebra/week-2/index.html">Week 2 - Linear Transformations and Matrices</a></li>
<li><a href="/laff-linear-algebra/week-3/index.html">Week 3 - Matrix-Vector Operations</a></li>
<li><a href="/laff-linear-algebra/week-4/index.html">Week 4 - Matrix-Vector to Matrix-Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-5/index.html">Week 5 - Matrix- Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-6/index.html">Week 6 - Gaussian Elimination</a></li>
<li><a href="/laff-linear-algebra/week-7/index.html">Week 7 - More Gaussian Elimination and Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-8/index.html">Week 8 - More on Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-9/index.html">Week 9 - Vector Spaces</a></li>
<li><a href="/laff-linear-algebra/week-10/index.html">Week 10 - Vector Spaces, Orthogonality, and Linear Least-Squares</a></li>
<li><a href="/laff-linear-algebra/week-11/index.html">Week 11 - Orthogonal Projection, Low Rank Approximation, and Orthogonal Bases</a></li>
<li><a href="/laff-linear-algebra/week-12/index.html">Week 12 - Eigenvalues and Eigenvectors</a></li>
</ul>
<h2 id="stanford-machine-learning"><a class="markdownIt-Anchor" href="#stanford-machine-learning"></a> Stanford Machine Learning</h2>
<ul>
<li><a href="/stanford-machine-learning/week-1/index.html">Week 1 - Introduction</a></li>
<li><a href="/stanford-machine-learning/week-2/index.html">Week 2 - Linear Regression with Multiple Variables</a></li>
<li><a href="/stanford-machine-learning/week-3/index.html">Week 3 - Logistic Regression &amp; Regularization</a></li>
<li><a href="/stanford-machine-learning/week-4/index.html">Week 4 - Neural Networks: Representation</a></li>
<li><a href="/stanford-machine-learning/week-5/index.html">Week 5 - Neural Networks: Learning</a></li>
<li><a href="/stanford-machine-learning/week-6a/index.html">Week 6a - Advice for Applying Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-6b/index.html">Week 6b - Machine Learning System Design</a></li>
<li><a href="/stanford-machine-learning/week-7/index.html">Week 7 - Support Vector Machines</a></li>
<li><a href="/stanford-machine-learning/week-8/index.html">Week 8 - Unsupervised Learning &amp; Dimensionality Reduction</a></li>
<li><a href="/stanford-machine-learning/week-9a/index.html">Week 9a - Anomaly Detection</a></li>
<li><a href="/stanford-machine-learning/week-9b/index.html">Week 9b - Recommender Systems</a></li>
<li><a href="/stanford-machine-learning/week-10/index.html">Week 10 - Large Scale Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-11/index.html">Week 11 - Application Example: Photo OCR</a></li>
</ul>
<h2 id="calculus-one"><a class="markdownIt-Anchor" href="#calculus-one"></a> Calculus One</h2>
<ul>
<li><a href="/calculus-one/week-2-3/index.html">Week 2-3 - Functions &amp; Limits</a></li>
<li><a href="/calculus-one/week-4/index.html">Week 4 - The Beginning of Derivatives</a></li>
<li><a href="/calculus-one/week-5/index.html">Week 5 - Techniques of Differentiation</a></li>
<li><a href="/calculus-one/week-6/index.html">Week 6 - Chain Rule</a></li>
<li><a href="/calculus-one/week-7/index.html">Week 7 - Derivatives of Trigonometric Functions</a></li>
<li><a href="/calculus-one/week-8/index.html">Week 8 - Derivatives in the Real World</a></li>
<li><a href="/calculus-one/week-9/index.html">Week 9 - Optimization</a></li>
<li><a href="/calculus-one/week-10/index.html">Week 10 - Linear Approximation</a></li>
<li><a href="/calculus-one/week-11-12/index.html">Week 11-12 - Antidifferentiation &amp; Integration</a></li>
<li><a href="/calculus-one/week-13/index.html">Week 13 - Fundamental Theorem of Calculus</a></li>
<li><a href="/calculus-one/week-14/index.html">Week 14 - Substitution Rule</a></li>
<li><a href="/calculus-one/week-15/index.html">Week 15 - Techniques of Integration</a></li>
<li><a href="/calculus-one/week-16/index.html">Week 16 - Applications of Integration</a></li>
</ul>
<h2 id="computational-thinking"><a class="markdownIt-Anchor" href="#computational-thinking"></a> Computational Thinking</h2>
<ul>
<li><a href="/computational-thinking/lecture-1/index.html">Lecture 1 - Optimization and Knapsack Problem</a></li>
<li><a href="/computational-thinking/lecture-2/index.html">Lecture 2 - Decision Trees and Dynamic Programming</a>
<ul>
<li><a href="/computational-thinking/lecture-2-powerset/index.html">Exercise: Power Set Function</a></li>
</ul>
</li>
<li><a href="/computational-thinking/lecture-3/index.html">Lecture 3 - Graphs</a></li>
<li><a href="/computational-thinking/lecture-4-5/index.html">Lecture 4-5 - Plotting</a></li>
<li><a href="/computational-thinking/lecture-6-7/index.html">Lecture 6-7 - Stochastic Programs &amp; Inferential Statistics</a></li>
<li><a href="/computational-thinking/lecture-8/index.html">Lecture 8 - Monte Carlo Simulation</a></li>
<li><a href="/computational-thinking/lecture-9/index.html">Lecture 9 - Sampling and Standard Error</a></li>
<li><a href="/computational-thinking/lecture-10-11/index.html">Lecture 10-11 - Experimental Data</a></li>
<li><a href="/computational-thinking/lecture-12/index.html">Lecture 12 - Machine Learning</a></li>
<li><a href="/computational-thinking/lecture-13/index.html">Lecture 13 - Statistical Abuses</a></li>
</ul>
<h2 id="effective-thinking-through-mathematics"><a class="markdownIt-Anchor" href="#effective-thinking-through-mathematics"></a> Effective Thinking Through Mathematics</h2>
<ul>
<li><a href="/effective-thinking-through-mathematics/note/index.html">Note</a></li>
<li><a href="/effective-thinking-through-mathematics/week-4-telling-the-story-of-infinity/index.html">Week 4 (/Telling the Story of Infinity)</a></li>
<li><a href="/effective-thinking-through-mathematics/week-5-telling-the-story-of-the-euler-circuit-theorem/index.html">Week 5 (/Telling the Story of Euler Circuit Theorem)</a></li>
</ul>
<h2 id="cs50-introduction-to-computer-science"><a class="markdownIt-Anchor" href="#cs50-introduction-to-computer-science"></a> CS50 Introduction to Computer Science</h2>
<ul>
<li><a href="/cs50/week-1/index.html">Week 1 - C</a></li>
<li><a href="/cs50/week-2/index.html">Week 2 - Arrays</a></li>
<li><a href="/cs50/week-3/index.html">Week 3 - Algorithms</a></li>
<li><a href="/cs50/week-4/index.html">Week 4 - Memory</a></li>
<li><a href="/cs50/week-5/index.html">Week 5 - Data Structures</a></li>
<li><a href="/cs50/week-6/index.html">Week 6 - HTTP</a></li>
<li><a href="/cs50/week-7-10/index.html">Week 7-10 - Machine Learning/Python/SQL/Javascript</a></li>
</ul>

</div>


<script src="/js/book-menu.js"></script>

  </div>

  <div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseover="add_inner()" onmouseleave="remove_inner()">
  <div class="sidebar-toggle-inner"></div>
</div>

<script>
function add_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.add('show')  
}

function remove_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.remove('show')
}

function sidebar_toggle() {
    let sidebar_toggle = document.querySelector('.sidebar-toggle')
    let sidebar = document.querySelector('.book-sidebar')
    let content = document.querySelector('.off-canvas-content')
    if (sidebar_toggle.classList.contains('extend')) { // show
        sidebar_toggle.classList.remove('extend')
        sidebar.classList.remove('hide')
        content.classList.remove('extend')
    }
    else { // hide
        sidebar_toggle.classList.add('extend')
        sidebar.classList.add('hide')
        content.classList.add('extend')
    }
}
</script>

  <div class="off-canvas-content">
    <div class="columns">
      <div class="column col-10 col-lg-12">
        <div class="book-navbar">
          <!-- For Responsive Layout -->

<header class="navbar">
  <section class="navbar-section">
    <a onclick="open_sidebar()">
      <i class="icon icon-menu"></i>
    </a>
  </section>
</header>

        </div>
        <div class="book-content">
          <div class="book-post">
  <h1 id="week-12-eigenvalues-and-eigenvectors"><a class="markdownIt-Anchor" href="#week-12-eigenvalues-and-eigenvectors"></a> Week 12 - Eigenvalues and Eigenvectors</h1>
<h2 id="the-algebraic-eigenvalue-problem"><a class="markdownIt-Anchor" href="#the-algebraic-eigenvalue-problem"></a> The Algebraic Eigenvalue Problem</h2>
<ul>
<li>
<p>The algebraic eigenvalue problem is given by \[Ax = \lambda x\]</p>
</li>
<li>
<p>where \(A \in \mathbb{R}^{n \times n}\) is a square matrix, \(\lambda\) is a scalar, and \(x\) is a nonzero vector.</p>
<ul>
<li>If \(x \ne 0\), then \(\lambda\) is said to be an eigenvalue and x is said to be an <strong>eigenvector</strong> associated with the <strong>eigenvalue</strong> \(\lambda\).</li>
<li>The tuple \((\lambda, x)\) is said to be an <strong>eigenpair</strong>.</li>
<li>The set of all vectors <strong>x</strong> that satisfy \(Ax = \lambda x\) is a subspace, called <strong>eigenspace</strong>.</li>
</ul>
</li>
<li>
<p>Equivalent statements:</p>
<ul>
<li>\(Ax = \lambda x\), where \(x \ne 0\).</li>
<li>\((A - \lambda I) x = 0\), where \(x \ne 0\).</li>
<li>\(A - \lambda I\) is singular.</li>
<li>\(\mathcal{N}(A - \lambda I)\) contains a nonzero vector x.
<ul>
<li>This is a consequence of there being a vector \(x \ne 0\) such that \((A - \lambda I)x = 0\).</li>
</ul>
</li>
<li>\(\text{dim}(\mathcal{N}(A - \lambda I)) &gt; 0\).</li>
<li>\(\text{det}(A - \lambda I) = 0\).
<ul>
<li>=&gt; \(\mathcal{N}(A - \lambda I) \ne {0}\)</li>
<li>A is a square matrix =&gt; \((A - \lambda I)\) is not invertible</li>
<li>=&gt; \((A - \lambda I) x = 0\) has many solutions.</li>
<li>More proves in <strong>Week 7#Showing that A^T A is invertible</strong></li>
</ul>
</li>
</ul>
</li>
<li>
<p>If we ﬁnd a vector \(x \ne 0\) such that \(Ax = \lambda x\), it is certainly not unique.</p>
<ul>
<li>For any scalar \(\alpha\), \(A(\alpha x) = \lambda (\alpha x)\) also holds.</li>
<li>If \(Ax = \lambda x\) and \(Ay = \lambda y\), then \(A(x + y) = Ax + Ay = \lambda x + \lambda y = \lambda (x + y)\)</li>
</ul>
</li>
<li>
<p>We conclude that the set of all vectors \(x\) that satisfy \(Ax = \lambda x\) is a subspace.</p>
</li>
</ul>
<h2 id="simple-cases"><a class="markdownIt-Anchor" href="#simple-cases"></a> Simple cases</h2>
<ul>
<li>The eigenvalue of the <strong>zero</strong> matrix is the scalar \(\lambda = 0\). All nonzero vectors are eigenvectors.</li>
<li>The eigenvalue of the <strong>identity</strong> matrix is the scalar \(\lambda = 1\). All nonzero vectors are eigenvectors.</li>
<li>The eigenvalues of a <strong>diagonal</strong> matrix are its elements on the diagonal. The unit basis vectors are eigenvectors.</li>
<li>The eigenvalues of a <strong>triangular</strong> matrix are its elements on the diagonal.
<ul>
<li>Because if there is a zero on the diagonal, it is singular.</li>
</ul>
</li>
<li>The eigenvalues of a 2 × 2 matrix can be found by finding the roots of \(p_2(\lambda) = \text{det}(A - \lambda I) = 0\)</li>
<li>The eigenvalues of a 3 × 3 matrix can be found by finding the roots of \(p_3(\lambda) = \text{det}(A - \lambda I) = 0\)</li>
</ul>
<h3 id="compute-the-eigenvalues-and-eigenvectors-of-22-matrices"><a class="markdownIt-Anchor" href="#compute-the-eigenvalues-and-eigenvectors-of-22-matrices"></a> Compute the eigenvalues and eigenvectors of 2×2 matrices</h3>
<ul>
<li>Compute \[\text{det}(\begin{pmatrix} (\alpha_{0,0} - \lambda) &amp; \alpha_{0,1} \ \alpha_{1,0} &amp; (\alpha_{1,1} - \lambda)\end{pmatrix}) = (\alpha_{0,0} - \lambda)(\alpha_{1,1} - \lambda) - \alpha_{0,1}\alpha_{1,0} = 0\]</li>
<li>Recognize that this is a second degree polynomial in \(\lambda\).</li>
<li>It is called the <strong>characteristic polynomial</strong> of the matrix \(A, p_2(\lambda)\).</li>
<li>Compute the coefficients of \(p_2(\lambda)\) so that \[p_2(\lambda) = - \lambda^2 + \beta \lambda + \gamma\]</li>
<li>Solve \[- \lambda^2 + \beta \lambda + \gamma = 0\]</li>
<li>for its roots. You can do this either by examination, or by using the quadratic formula: \[\lambda = \frac{-\beta \pm \sqrt{\beta^2 + 4 \gamma} }{-2}\]</li>
<li>Find all of the eigenvectors that satisﬁes \[\begin{pmatrix} (\alpha_{0,0} - \lambda) &amp; \alpha_{0,1} \ \alpha_{1,0} &amp; (\alpha_{1,1} - \lambda)\end{pmatrix}\begin{pmatrix} \chi_0 \ \chi_1\end{pmatrix} = \begin{pmatrix} 0 \ 0\end{pmatrix}\]
<ul>
<li>Transform \(\begin{pmatrix} (\alpha_{0,0} - \lambda) &amp; \alpha_{0,1} \ \alpha_{1,0} &amp; (\alpha_{1,1} - \lambda)\end{pmatrix}\) to row-echelon form with different \(\lambda\)s, find the eigenspaces.</li>
</ul>
</li>
<li>Check your answer! It is a matter of plugging it into \(Ax = \lambda x\) and seeing if the computed \(\lambda\) and \(x\) satisfy the equation.</li>
</ul>
<h4 id="example"><a class="markdownIt-Anchor" href="#example"></a> Example</h4>
<ul>
<li>\(A = \begin{bmatrix} 1 &amp; 2 \ 4 &amp; 3 \end{bmatrix}\)</li>
<li>\(\text{det}(\begin{bmatrix} 1 - \lambda &amp; 2 \ 4 &amp; 3 - \lambda \end{bmatrix}) = 0\)</li>
<li>\((1 - \lambda) (3 - \lambda)- 8 = 0\)</li>
<li>=&gt; \(\lambda = 5 \text{ or } \lambda = -1\)</li>
<li>For any eigenvalues \(\lambda\), \(\mathcal{E}_A(\lambda) = \mathcal{N}(\lambda I_n - A)\)
<ul>
<li>\(\mathcal{E}_A(\lambda)\): <strong>eigenspace</strong>.</li>
</ul>
</li>
<li>when \(\lambda = 5\), then \(\mathcal{E}_A(5) = \mathcal{N}(\begin{bmatrix} 4 &amp; -2 \ -4 &amp; 2 \end{bmatrix})\).
<ul>
<li>Transform to row-echelon form, we get \(\begin{bmatrix} 1 &amp; -1/2 \ 0 &amp; 0 \end{bmatrix} \begin{bmatrix} \chi_0 \ \chi_1 \end{bmatrix} = \begin{bmatrix} 0 \ 0 \end{bmatrix}\)</li>
</ul>
</li>
<li>then \(\chi_0 = \frac{1}{2} \chi_1\)</li>
<li>\(\mathcal{E}_A(5) = {\begin{bmatrix} \chi_0 \ \chi_1 \end{bmatrix} = \epsilon \begin{bmatrix} 1/2 \ 1 \end{bmatrix}, \epsilon \in \mathbb{R}}\)</li>
<li>\(\mathcal{E}_A(5) = \text{Span}(\begin{bmatrix} 1/2 \ 1 \end{bmatrix})\)</li>
<li>Same way, we get \(\mathcal{E}_A(-1) = \text{Span}(\begin{bmatrix} -1 \ 1 \end{bmatrix})\)</li>
</ul>
<h2 id="diagonalization"><a class="markdownIt-Anchor" href="#diagonalization"></a> Diagonalization</h2>
<ul>
<li><strong>Theorem</strong>: Let \(A \in \mathbb{R}^{n \times n}\). Then there exists a nonsingular matrix \(X\) such that \(X^{-1} A X = \Lambda\) iff \(A\) has <strong>n</strong> <strong>linearly independent eigenvectors</strong>. Then \[\begin{aligned} X^{-1} A X &amp;= \Lambda \ A X &amp;= X \Lambda \ A &amp;= X \Lambda X^{-1}\end{aligned}\]
<ul>
<li>\[\Lambda = \begin{pmatrix} \lambda_1 &amp; &amp; &amp; \ &amp; \lambda_2 &amp; &amp; \ &amp; &amp; \ddots &amp; \ &amp; &amp; &amp; \lambda_n \end{pmatrix}\]</li>
</ul>
</li>
<li>If \(\Lambda\) is in addition diagonal, then the diagonal elements of \(\Lambda\) are <strong>eigenvalues</strong> of <strong>A</strong> and the columns of <strong>X</strong> are <strong>eigenvectors</strong> of <strong>A</strong>.</li>
<li>For example:
<ul>
<li>\(A = \begin{pmatrix} 1 &amp; -1 \ 2 &amp; 4 \end{pmatrix}\)</li>
<li>the eigenpairs are \((2, \begin{pmatrix} -1 \ 1 \end{pmatrix}), (3 \begin{pmatrix} -1 \ 2 \end{pmatrix})\)</li>
<li>Then:</li>
<li>
<img src="https://i.imgur.com/UYfYgSZ.jpg" style="width:600px" />
</li>
<li><strong>The matrix A can be diagonalized</strong>.</li>
</ul>
</li>
</ul>
<h3 id="defective-matrices"><a class="markdownIt-Anchor" href="#defective-matrices"></a> Defective matrices</h3>
<ul>
<li>A <strong>defective matrix</strong> is a square matrix that does not have a complete basis of eigenvectors, and is therefore not <strong>diagonalizable</strong>. In particular, an n × n matrix is defective if and only if it does <strong>not</strong> have <strong>n linearly independent eigenvectors</strong>.</li>
</ul>
<h4 id="jordan-block"><a class="markdownIt-Anchor" href="#jordan-block"></a> Jordan Block</h4>
<ul>
<li>
<p>In general, the k ×k matrix \(J_k(\lambda)\) given by \[J_k(\lambda) = \begin{pmatrix} \lambda &amp; 1 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \ 0 &amp; \lambda &amp; 1 &amp; \cdots &amp; 0 &amp; 0 \ 0 &amp; 0 &amp; \lambda &amp; \cdots &amp; 0 &amp; 0 \ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \ 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \lambda &amp; 1 \ 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; \lambda \end{pmatrix}\]</p>
</li>
<li>
<p>a simple example: \[\begin{pmatrix} \lambda &amp; 1 \ 0 &amp; \lambda \end{pmatrix}\]</p>
</li>
<li>
<p>Any nontrivial Jordan block of size 2×2 or larger (that is, not completely diagonal) is defective.</p>
</li>
<li>
<p><strong>Example</strong></p>
<ul>
<li>A simple example of a defective matrix is: \[{ {\begin{bmatrix}3&amp;1\0&amp;3\end{bmatrix} } }\]</li>
<li>which has a double eigenvalue of <strong>3</strong> but only <strong>one</strong> distinct eigenvector \[{\begin{bmatrix}1\0\end{bmatrix} }\]</li>
</ul>
</li>
</ul>
<h2 id="general-case"><a class="markdownIt-Anchor" href="#general-case"></a> General case</h2>
<ul>
<li>
<p><strong>Theorem</strong>: The matrix \(A \in \mathbb{R}^{n \times n}\) is <strong>nonsingular</strong> iff \(\text{det}(A) \ne 0\).</p>
</li>
<li>
<p><strong>Theorem</strong>: Given \(A \in \mathbb{R}^{n \times n}\), \[p_n(\lambda) = \text{det}(A - \lambda I) = \lambda^n + \gamma_{n-1} \lambda^{n-1} + \cdots + \gamma_1 \lambda + \gamma_0\] for some <strong>coefficients</strong> \(\gamma_1, \ldots, \gamma_{n-1} \in \mathbb{R}\)</p>
</li>
<li>
<p><strong>Definition</strong>: Given \(A \in \mathbb{R}^{n \times n}\), \(p_n(\lambda) = \text{det}(A - \lambda I)\) is called the <strong>characteristic polynomial</strong>.</p>
</li>
<li>
<img src="https://i.imgur.com/kcwPmSK.jpg" style="width:650px" />
</li>
</ul>
<h2 id="properties-of-eigenvalues-and-eigenvectors"><a class="markdownIt-Anchor" href="#properties-of-eigenvalues-and-eigenvectors"></a> Properties of eigenvalues and eigenvectors</h2>
<ul>
<li><strong>Definition</strong>: Given \(A \in \mathbb{R}^{n \times n}\) and nonzero vector \(x \in \mathbb{R}^{n}\) , the scalar \(x^T Ax/x^T x\) is known as the <strong>Rayleigh quotient</strong>.</li>
<li><strong>Theorem</strong>: Let \(A \in \mathbb{R}^{n \times n}\) and x equal an eigenvector of A. Assume that x is real valued as is the T eigenvalue λ with \(Ax = \lambda x\). Then \(\lambda = x x^T Ax x\) is the eigenvalue associated with the eigenvector x.</li>
<li><strong>Theorem</strong>: Let \(A \in \mathbb{R}^{n \times n}\) , β be a scalar, and \(\lambda \in \Lambda(A)\). Then \(\beta \lambda \in \Lambda(\beta A)\).</li>
<li><strong>Theorem</strong>: Let \(A \in \mathbb{R}^{n \times n}\) be nonsingular, \(\lambda \in \Lambda(A)\), and \(Ax = \lambda x\). Then \(A^{-1} x = \frac{1}{\lambda} x\).</li>
<li><strong>Theorem</strong>: Let \(A \in \mathbb{R}^{n \times n}\), \(\lambda \in \Lambda(A)\),  Then \((\lambda - \mu ) \in \Lambda(A - \mu I)\).</li>
</ul>
<h2 id="relative-definitions"><a class="markdownIt-Anchor" href="#relative-definitions"></a> Relative Definitions</h2>
<h3 id="eigenspaces"><a class="markdownIt-Anchor" href="#eigenspaces"></a> Eigenspaces</h3>
<ul>
<li>the nullspace \(A - I\lambda\) is the <strong>eigenspace</strong> of A for λ denoted by \(\mathcal{E}_A(\lambda)\). In other words, \(\mathcal{E}_A(\lambda)\) consists of all the eigenvectors of A for λ and the zero vector.</li>
</ul>
<h3 id="algebraic-and-geometric-multiplicity"><a class="markdownIt-Anchor" href="#algebraic-and-geometric-multiplicity"></a> Algebraic and Geometric Multiplicity</h3>
<ul>
<li>Example: Let \(A = \begin{bmatrix} 1 &amp; 2 \ 1 &amp; 0 \end{bmatrix}\)
<ul>
<li>-1 is an eigenvalue of A. and the correspond eigenvector is \(\begin{bmatrix} -1 \ 1 \end{bmatrix}\)</li>
</ul>
</li>
<li>\(\mathcal{E}_A(-1) = \text{Span}(\begin{bmatrix} -1 \ 1\end{bmatrix})\)</li>
<li>The <strong>geometric multiplicity</strong> of an eigenvalue λ of A is the dimension of \(\mathcal{E}_A(\lambda)\)
<ul>
<li>the geometric multiplicity of −1 is 1.</li>
</ul>
</li>
<li>The <strong>algebraic multiplicity</strong> of an eigenvalue λ of A is the number of times λ appears as a root of \(p_A\).
<ul>
<li>−1  appears only <strong>once</strong> as a root.  the algebraic multiplicity of -1 is 1.</li>
</ul>
</li>
<li>In general, the algebraic multiplicity and geometric multiplicity of an eigenvalue can differ. However, the geometric multiplicity can <strong>never exceed</strong> the algebraic multiplicity.</li>
<li>If for every eigenvalue of A, the <strong>geometric multiplicity</strong> equals the <strong>algebraic multiplicity</strong>, then A is said to be <strong>diagonalizable</strong>.</li>
</ul>
<h3 id="singular-matrix"><a class="markdownIt-Anchor" href="#singular-matrix"></a> Singular Matrix</h3>
<ul>
<li><strong>A matrix is singular</strong> if and only if 0 is one of its <strong>eigenvalues</strong>. A singular matrix can be either diagonalizable or not diagonalizable. For example:
<ul>
<li>\(\left(\begin{array}{c c} 1 &amp; 0 \ 0 &amp; 0\end{array}\right)\) is diagonalizable</li>
<li>\(\left(\begin{array}{c c} 0 &amp; 1 \ 0 &amp; 0\end{array}\right)\) is not diagonalizable.</li>
</ul>
</li>
</ul>
<h3 id="polynomial-roots"><a class="markdownIt-Anchor" href="#polynomial-roots"></a> Polynomial Roots</h3>
<ul>
<li>A root of a polynomial \(P(z)\) is a number \(z_i\) such that \(P(z_i)=0\). The fundamental theorem of algebra states that a polynomial \(P(z)\) of degree n has n roots, some of which may be degenerate.</li>
<li>For example, the roots of the polynomial \(x<sup>3-2x</sup>2-x+2=(x-2)(x-1)(x+1)\) are -1, 1, and 2.</li>
</ul>
<h2 id="refers"><a class="markdownIt-Anchor" href="#refers"></a> Refers</h2>
<ul>
<li><a href="http://mathworld.wolfram.com/SingularMatrix.html" target="_blank" rel="noopener">http://mathworld.wolfram.com/SingularMatrix.html</a></li>
<li><a href="https://en.wikipedia.org/wiki/Invertible_matrix" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Invertible_matrix</a></li>
<li><a href="http://mathworld.wolfram.com/Determinant.html" target="_blank" rel="noopener">http://mathworld.wolfram.com/Determinant.html</a></li>
<li><a href="https://en.wikipedia.org/wiki/Defective_matrix" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Defective_matrix</a></li>
<li><a href="http://people.math.carleton.ca/~kcheung/math/notes/MATH1107/wk10/10_algebraic_and_geometric_multiplicities.html" target="_blank" rel="noopener">http://people.math.carleton.ca/~kcheung/math/notes/MATH1107/wk10/10_algebraic_and_geometric_multiplicities.html</a></li>
<li><a href="http://mathworld.wolfram.com/PolynomialRoots.html" target="_blank" rel="noopener">http://mathworld.wolfram.com/PolynomialRoots.html</a></li>
</ul>
<h2 id="words"><a class="markdownIt-Anchor" href="#words"></a> Words</h2>
<ul>
<li><strong>eigenvalue</strong> ['aiɡən,vælju:] n. [数] 特征值</li>
<li><strong>eigenvector</strong> ['aiɡən,vektə] n. [数] 特征向量；本征矢量</li>
<li><strong>diagonalization</strong> [dai,æɡənəlai’zeiʃən, -li’z-] n. [数] 对角化；对角线化</li>
<li><strong>multiplicity</strong> [,mʌlti’plisəti] n. 多样性；[物] 多重性</li>
<li><strong>algebraic and geometric multiplicity</strong> 代数重数与几何重数</li>
<li><strong>companion matrix</strong> 友（矩）[数] 阵</li>
<li><strong>spectrum</strong> ['spektrəm] n. 光谱；频谱；范围；余象</li>
</ul>

</div>


  <div class="book-comments">
    




  </div>



<script src="/js/book-post.js"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>


        </div>
      </div>
      <div class="column col-2 hide-lg">
        <div class="book-post-info">
  
    <div class="book-post-meta">

  <div class="author">

    <!-- Author image -->
    <div class="author-img">
      
        <figure
          class="avatar avatar-lg"
          data-initial="E"
          style="background-color: #3b4351;">
        </figure>
      
    </div>

    <!-- Author title -->
    <div class="author-title">
      <div>Eric Yang</div>
      <div>2020-04-14</div>
    </div>
  </div>

  

  <div class="divider"></div>
</div>
  

  <div class="book-tocbot">
</div>
<div class="book-tocbot-menu">
  <a class="book-toc-expand" onclick="expand_toc()">Expand all</a>
  <a onclick="go_top()">Back to top</a>
  <a onclick="go_bottom()">Go to bottom</a>
</div>


<script src="/js/book-toc.js"></script>

</div>

      </div>
    </div>
  </div>
  
  <a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>

</body>
</html>


<script src="/js/book.js"></script>
