<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">

    
      <link rel="icon" href="/favicon.png" />
    

    <title>
        
          Eric&#39;s CS Notes
        
    </title>

    <!-- Spectre.css framework -->
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-exp.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-icons.min.css">

    <!-- theme css & js -->
    
<link rel="stylesheet" href="/css/book.css">

    
<script src="/js/book.js"></script>


    <!-- tocbot -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.18.2/tocbot.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.18.2/tocbot.css">
    
    <!-- katex css -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css" integrity="sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/" crossorigin="anonymous">

    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/zooming/2.1.1/zooming.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    const zooming = new Zooming()
    zooming.listen('.book-content img')
})
</script>

<meta name="generator" content="Hexo 6.3.0"></head>


<body>

<div class="book-container">
  <div class="book-sidebar">
    <div class="book-brand">
  <a href="/">
    <img src="/favicon.png">
    <span>ERIC&#39;S CS NOTES</span>
  </a>
</div>

    <div class="book-menu">
  <!--
## Introduction to Probability

* [Unit 1: Probability models and axioms](/introduction-to-probability/unit-1/index.html)
* [Unit 2: Conditioning and independence](/introduction-to-probability/unit-2/index.html)
* [Unit 3: Counting](/introduction-to-probability/unit-3/index.html)
* [Unit 4: Discrete random variables](/introduction-to-probability/unit-4/index.html)
* [Unit 5: Continuous random variables](/introduction-to-probability/unit-5/index.html)
* [Unit 6: Further topics on random variables](/introduction-to-probability/unit-6/index.html)
* [Unit 7: Bayesian inference](/introduction-to-probability/unit-7/index.html)
* [Unit 8: Limit theorems and classical statistics](/introduction-to-probability/unit-8/index.html)
* [Unit 9: Bernoulli and Poisson processes](/introduction-to-probability/unit-9/index.html)
* [Unit 10: Markov chains](/introduction-to-probability/unit-10/index.html)
-->
<!--
## Multivariable Calculus

* [Unit 1: Thinking about multivariable functions](/multivariable-calculus/unit-1/index.html)
* [Unit 2: Derivatives of multivariable functions](/multivariable-calculus/unit-2/index.html)

* [Unit 3: Applications of multivariable derivatives](/multivariable-calculus/unit-3/index.html)
* [Unit 4: Integrating multivariable functions](/multivariable-calculus/unit-4/index.html)
* [Unit 5: Green's, Stokes', and the divergence theorems](/multivariable-calculus/unit-5/index.html)
-->
<h2 id="cs6210-advanced-operating-systems"><a class="markdownIt-Anchor" href="#cs6210-advanced-operating-systems"></a> CS6210: Advanced Operating Systems</h2>
<ul>
<li><a href="/cs6210/lesson-02/index.html">Lesson 2: OS Structure</a></li>
<li><a href="/cs6210/lesson-03/index.html">Lesson 3: Virtualization</a></li>
<li><a href="/cs6210/lesson-04a/index.html">Lesson 4: Parallel Systems - Part 1</a></li>
<li><a href="/cs6210/lesson-04b/index.html">Lesson 4: Parallel Systems - Part 2</a></li>
<li><a href="/cs6210/lesson-05/index.html">Lesson 5: Distributed Systems</a></li>
<li><a href="/cs6210/lesson-06/index.html">Lesson 6: Distributed Objects and Middleware</a></li>
<li><a href="/cs6210/lesson-07a/index.html">Lesson 7: Distributed Subsystems - GMS</a></li>
<li><a href="/cs6210/lesson-07b/index.html">Lesson 7: Distributed Subsystems - DSM</a></li>
<li><a href="/cs6210/lesson-07c/index.html">Lesson 7: Distributed Subsystems - DFS</a></li>
</ul>
<!--
* [Lesson 9: Internet Computing](/cs6210/lesson-09/index.html)
* [Lesson 10: RT and Multimedia](/cs6210/lesson-10/index.html)
* [Lesson 8: Failures and Recovery](/cs6210/lesson-08/index.html)
* [Lesson 11: Security](/cs6210/lesson-11/index.html)
-->
<h2 id="cs6250-computer-networks"><a class="markdownIt-Anchor" href="#cs6250-computer-networks"></a> CS6250 Computer Networks</h2>
<ul>
<li><a href="/cs6250/week-1-internet-architecture/index.html">Week 1 - Internet Architecture</a></li>
<li><a href="/cs6250/week-2-transport-and-application-layers/index.html">Week 2 - Transport and Application Layers</a></li>
<li><a href="/cs6250/week-3-intradomain-routing/index.html">Week 3 - Intradomain Routing</a></li>
<li><a href="/cs6250/week-4-as-relationships-and-interdomain-routing/index.html">Week 4 - AS Relationships and Interdomain Routing</a></li>
<li><a href="/cs6250/week-5-router-design-and-algorithems-part-1/index.html">Week 5 - Router Design and Algorithms (Part 1)</a></li>
<li><a href="/cs6250/week-6-router-design-and-algorithems-part-2/index.html">Week 6 - Router Design and Algorithms (Part 2)</a></li>
<li><a href="/cs6250/week-7-software-defined-networking-part-1/index.html">Week 7 - Software Defined Networking (Part 1)</a></li>
<li><a href="/cs6250/week-8-software-defined-networking-part-2/index.html">Week 8 - Software Defined Networking (Part 2)</a></li>
<li><a href="/cs6250/week-9-internet-security/index.html">Week 9 - Internet Security</a></li>
<li><a href="/cs6250/week-10-internet-surveillance-and-censorship/index.html">Week 10 - Internet Surveillance and Censorship</a></li>
<li><a href="/cs6250/week-11-applications-video/index.html">Week 11 - Applications Videos</a></li>
<li><a href="/cs6250/week-12-applications-cdns-and-overlay-networks/index.html">Week 12 - Applications CDNs and Overlay Networks</a></li>
</ul>
<h2 id="cs6200-graduate-introduction-to-operating-systems"><a class="markdownIt-Anchor" href="#cs6200-graduate-introduction-to-operating-systems"></a> CS6200 Graduate Introduction to Operating Systems</h2>
<ul>
<li><a href="/cs6200/p1-preparation/index.html">P0 - Preparation</a></li>
<li><a href="/cs6200/p1l2-introduction/index.html">P1L2 - Introduction</a></li>
<li><a href="/cs6200/p2l1-processes-and-process-management/index.html">P2L1 - Processes and Process Management</a></li>
<li><a href="/cs6200/p2l2-threads-and-concurrency/index.html">P2L2 - Threads and Concurrency</a></li>
<li><a href="/cs6200/p2l3-pthread/index.html">P2L3 - PThread</a></li>
<li><a href="/cs6200/p2l4-thread-design-consideration/index.html">P2L4 - Thread Design Considerations</a></li>
<li><a href="/cs6200/p2l5-thread-performance-consideration/index.html">P2L5 - Thread Performance Considerations</a></li>
<li><a href="/cs6200/p3l1-scheduling/index.html">P3L1 - Scheduling</a></li>
<li><a href="/cs6200/p3l2-memory-management/index.html">P3L2 - Memory Management</a></li>
<li><a href="/cs6200/p3l3-inter-process-communication/index.html">P3L3 - Inter-Process Communication</a></li>
<li><a href="/cs6200/p3l4-synchronization-constructs/index.html">P3L4 - Synchronization Constructs</a></li>
<li><a href="/cs6200/p3l5-io-management/index.html">P3L5 - I/O Management</a></li>
<li><a href="/cs6200/p3l6-virtualization/index.html">P3L6 - Virtualization</a></li>
</ul>
<!--
* [P4L1 - Remote Procedure Calls](/cs6200/p4l1-remote-procedure-calls/index.html) 
* [P4L2 - Distributed File Systems](/cs6200/p4l2-distributed-file-systems/index.html) 
* [P4L3 - Distributed Shared Memory](/cs6200/p4l3-distributed-shared-memory/index.html) 
* [P4L4 - Datacenter Technologies](/cs6200/p4l4-datacenter-technologies/index.html) 
-->
<h2 id="algorithms-part-ii"><a class="markdownIt-Anchor" href="#algorithms-part-ii"></a> Algorithms: Part II</h2>
<ul>
<li><a href="/algorithms-2/week-1/index.html">Week 1 - Undirected Graph &amp; Directed Graph</a></li>
<li><a href="/algorithms-2/week-2/index.html">Week 2 - Minimum Spanning Trees &amp; Shortest Path</a></li>
<li><a href="/algorithms-2/week-3/index.html">Week 3 - Maximum Flow and Minimum Cut &amp; String Sort</a></li>
<li><a href="/algorithms-2/week-4/index.html">Week 4 - Tries &amp; Substring Search</a></li>
<li><a href="/algorithms-2/week-5/index.html">Week 5 - Regular Expressions</a></li>
<li><a href="/algorithms-2/week-6/index.html">Week 6 - Reductions</a></li>
</ul>
<h2 id="algorithms-part-i"><a class="markdownIt-Anchor" href="#algorithms-part-i"></a> Algorithms: Part I</h2>
<ul>
<li><a href="/algorithms-1/week-1/index.html">Week 1 - Union-Find &amp; Analysis of Algorithms</a></li>
<li><a href="/algorithms-1/week-2/index.html">Week 2 - Stacks and Queues &amp; Elementary Sorts</a></li>
<li><a href="/algorithms-1/week-3/index.html">Week 3 - Mergesort &amp; Quicksort</a></li>
<li><a href="/algorithms-1/week-4/index.html">Week 4 - Priority Queues &amp; Elementary Symbols</a></li>
<li><a href="/algorithms-1/week-5/index.html">Week 5 - Balanced Search Trees</a></li>
<li><a href="/algorithms-1/week-6/index.html">Week 6 - Hash Tables</a></li>
</ul>
<h2 id="introduction-to-software-design-and-architecture"><a class="markdownIt-Anchor" href="#introduction-to-software-design-and-architecture"></a> Introduction to Software Design and Architecture</h2>
<ul>
<li><a href="/introduction-to-software-design-and-architecture/design-pattern/index.html">Design Pattern</a></li>
</ul>
<h2 id="calculus-two-sequences-and-series"><a class="markdownIt-Anchor" href="#calculus-two-sequences-and-series"></a> Calculus Two: Sequences and Series</h2>
<ul>
<li><a href="/calculus-two/week-1/index.html">Week 1 - Sequences</a></li>
<li><a href="/calculus-two/week-2/index.html">Week 2 - Series</a></li>
<li><a href="/calculus-two/week-3/index.html">Week 3 - Convergence Tests</a></li>
<li><a href="/calculus-two/week-4/index.html">Week 4 - Alternating Series</a></li>
<li><a href="/calculus-two/week-5/index.html">Week 5 - Power Series</a></li>
<li><a href="/calculus-two/week-6/index.html">Week 6 - Taylor Series</a></li>
</ul>
<h2 id="laff-linear-algebra"><a class="markdownIt-Anchor" href="#laff-linear-algebra"></a> LAFF Linear Algebra</h2>
<ul>
<li><a href="/laff-linear-algebra/week-1/index.html">Week 1 - Vectors in Linear Algebra</a></li>
<li><a href="/laff-linear-algebra/week-2/index.html">Week 2 - Linear Transformations and Matrices</a></li>
<li><a href="/laff-linear-algebra/week-3/index.html">Week 3 - Matrix-Vector Operations</a></li>
<li><a href="/laff-linear-algebra/week-4/index.html">Week 4 - Matrix-Vector to Matrix-Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-5/index.html">Week 5 - Matrix- Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-6/index.html">Week 6 - Gaussian Elimination</a></li>
<li><a href="/laff-linear-algebra/week-7/index.html">Week 7 - More Gaussian Elimination and Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-8/index.html">Week 8 - More on Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-9/index.html">Week 9 - Vector Spaces</a></li>
<li><a href="/laff-linear-algebra/week-10/index.html">Week 10 - Vector Spaces, Orthogonality, and Linear Least-Squares</a></li>
<li><a href="/laff-linear-algebra/week-11/index.html">Week 11 - Orthogonal Projection, Low Rank Approximation, and Orthogonal Bases</a></li>
<li><a href="/laff-linear-algebra/week-12/index.html">Week 12 - Eigenvalues and Eigenvectors</a></li>
</ul>
<h2 id="stanford-machine-learning"><a class="markdownIt-Anchor" href="#stanford-machine-learning"></a> Stanford Machine Learning</h2>
<ul>
<li><a href="/stanford-machine-learning/week-1/index.html">Week 1 - Introduction</a></li>
<li><a href="/stanford-machine-learning/week-2/index.html">Week 2 - Linear Regression with Multiple Variables</a></li>
<li><a href="/stanford-machine-learning/week-3/index.html">Week 3 - Logistic Regression &amp; Regularization</a></li>
<li><a href="/stanford-machine-learning/week-4/index.html">Week 4 - Neural Networks: Representation</a></li>
<li><a href="/stanford-machine-learning/week-5/index.html">Week 5 - Neural Networks: Learning</a></li>
<li><a href="/stanford-machine-learning/week-6a/index.html">Week 6a - Advice for Applying Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-6b/index.html">Week 6b - Machine Learning System Design</a></li>
<li><a href="/stanford-machine-learning/week-7/index.html">Week 7 - Support Vector Machines</a></li>
<li><a href="/stanford-machine-learning/week-8/index.html">Week 8 - Unsupervised Learning &amp; Dimensionality Reduction</a></li>
<li><a href="/stanford-machine-learning/week-9a/index.html">Week 9a - Anomaly Detection</a></li>
<li><a href="/stanford-machine-learning/week-9b/index.html">Week 9b - Recommender Systems</a></li>
<li><a href="/stanford-machine-learning/week-10/index.html">Week 10 - Large Scale Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-11/index.html">Week 11 - Application Example: Photo OCR</a></li>
</ul>
<h2 id="calculus-one"><a class="markdownIt-Anchor" href="#calculus-one"></a> Calculus One</h2>
<ul>
<li><a href="/calculus-one/week-2-3/index.html">Week 2-3 - Functions &amp; Limits</a></li>
<li><a href="/calculus-one/week-4/index.html">Week 4 - The Beginning of Derivatives</a></li>
<li><a href="/calculus-one/week-5/index.html">Week 5 - Techniques of Differentiation</a></li>
<li><a href="/calculus-one/week-6/index.html">Week 6 - Chain Rule</a></li>
<li><a href="/calculus-one/week-7/index.html">Week 7 - Derivatives of Trigonometric Functions</a></li>
<li><a href="/calculus-one/week-8/index.html">Week 8 - Derivatives in the Real World</a></li>
<li><a href="/calculus-one/week-9/index.html">Week 9 - Optimization</a></li>
<li><a href="/calculus-one/week-10/index.html">Week 10 - Linear Approximation</a></li>
<li><a href="/calculus-one/week-11-12/index.html">Week 11-12 - Antidifferentiation &amp; Integration</a></li>
<li><a href="/calculus-one/week-13/index.html">Week 13 - Fundamental Theorem of Calculus</a></li>
<li><a href="/calculus-one/week-14/index.html">Week 14 - Substitution Rule</a></li>
<li><a href="/calculus-one/week-15/index.html">Week 15 - Techniques of Integration</a></li>
<li><a href="/calculus-one/week-16/index.html">Week 16 - Applications of Integration</a></li>
</ul>
<h2 id="computational-thinking"><a class="markdownIt-Anchor" href="#computational-thinking"></a> Computational Thinking</h2>
<ul>
<li><a href="/computational-thinking/lecture-1/index.html">Lecture 1 - Optimization and Knapsack Problem</a></li>
<li><a href="/computational-thinking/lecture-2/index.html">Lecture 2 - Decision Trees and Dynamic Programming</a>
<ul>
<li><a href="/computational-thinking/lecture-2-powerset/index.html">Exercise: Power Set Function</a></li>
</ul>
</li>
<li><a href="/computational-thinking/lecture-3/index.html">Lecture 3 - Graphs</a></li>
<li><a href="/computational-thinking/lecture-4-5/index.html">Lecture 4-5 - Plotting</a></li>
<li><a href="/computational-thinking/lecture-6-7/index.html">Lecture 6-7 - Stochastic Programs &amp; Inferential Statistics</a></li>
<li><a href="/computational-thinking/lecture-8/index.html">Lecture 8 - Monte Carlo Simulation</a></li>
<li><a href="/computational-thinking/lecture-9/index.html">Lecture 9 - Sampling and Standard Error</a></li>
<li><a href="/computational-thinking/lecture-10-11/index.html">Lecture 10-11 - Experimental Data</a></li>
<li><a href="/computational-thinking/lecture-12/index.html">Lecture 12 - Machine Learning</a></li>
<li><a href="/computational-thinking/lecture-13/index.html">Lecture 13 - Statistical Abuses</a></li>
</ul>
<h2 id="effective-thinking-through-mathematics"><a class="markdownIt-Anchor" href="#effective-thinking-through-mathematics"></a> Effective Thinking Through Mathematics</h2>
<ul>
<li><a href="/effective-thinking-through-mathematics/note/index.html">Note</a></li>
<li><a href="/effective-thinking-through-mathematics/week-4-telling-the-story-of-infinity/index.html">Week 4 (/Telling the Story of Infinity)</a></li>
<li><a href="/effective-thinking-through-mathematics/week-5-telling-the-story-of-the-euler-circuit-theorem/index.html">Week 5 (/Telling the Story of Euler Circuit Theorem)</a></li>
</ul>
<h2 id="cs50-introduction-to-computer-science"><a class="markdownIt-Anchor" href="#cs50-introduction-to-computer-science"></a> CS50 Introduction to Computer Science</h2>
<ul>
<li><a href="/cs50/week-1/index.html">Week 1 - C</a></li>
<li><a href="/cs50/week-2/index.html">Week 2 - Arrays</a></li>
<li><a href="/cs50/week-3/index.html">Week 3 - Algorithms</a></li>
<li><a href="/cs50/week-4/index.html">Week 4 - Memory</a></li>
<li><a href="/cs50/week-5/index.html">Week 5 - Data Structures</a></li>
<li><a href="/cs50/week-6/index.html">Week 6 - HTTP</a></li>
<li><a href="/cs50/week-7-10/index.html">Week 7-10 - Machine Learning/Python/SQL/Javascript</a></li>
</ul>
<h2 id="others"><a class="markdownIt-Anchor" href="#others"></a> Others</h2>
<ul>
<li><a href="/symbols/index.html">Symbols of Mathematics</a></li>
<li><a href="/glossary/index.html">Glossary</a></li>
</ul>
<h2 id="about-me"><a class="markdownIt-Anchor" href="#about-me"></a> <a target="_blank" rel="noopener" href="https://ericyy.me/about/">About Me</a></h2>

</div>


<script src="/js/book-menu.js"></script>

  </div>

  <div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseover="add_inner()" onmouseleave="remove_inner()">
  <div class="sidebar-toggle-inner"></div>
</div>

<script>
function add_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.add('show')  
}

function remove_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.remove('show')
}

function sidebar_toggle() {
    let sidebar_toggle = document.querySelector('.sidebar-toggle')
    let sidebar = document.querySelector('.book-sidebar')
    let content = document.querySelector('.off-canvas-content')
    if (sidebar_toggle.classList.contains('extend')) { // show
        sidebar_toggle.classList.remove('extend')
        sidebar.classList.remove('hide')
        content.classList.remove('extend')
    }
    else { // hide
        sidebar_toggle.classList.add('extend')
        sidebar.classList.add('hide')
        content.classList.add('extend')
    }
}
</script>

  <div class="off-canvas-content">
    <div class="columns">
      <div class="column col-10 col-lg-12">
        <div class="book-navbar">
          <!-- For Responsive Layout -->

<header class="navbar">
  <section class="navbar-section">
    <a onclick="open_sidebar()">
      <i class="icon icon-menu"></i>
    </a>
  </section>
</header>

        </div>
        <div class="book-content">
          <div class="book-post">
  <h1 id="lecture-09-internet-computing"><a class="markdownIt-Anchor" href="#lecture-09-internet-computing"></a> Lecture 09: Internet Computing</h1>
<h2 id="instruction"><a class="markdownIt-Anchor" href="#instruction"></a> Instruction</h2>
<p>You are a teach assistant of the course of advanced operating system. This is the lecture of Internet Computing, which teaches Giant Scale Services, MapReduce, Content Delivery Networks. I will ask you a sequence of questions regarding the course. Use bullet points to answer all questions accurately, don’t make up anything. And also make your answer easy to understand and easy to review. Provide more details if necessary.</p>
<h2 id="end-of-instruction"><a class="markdownIt-Anchor" href="#end-of-instruction"></a> End of Instruction</h2>
<h2 id="l09a-giant-scale-services"><a class="markdownIt-Anchor" href="#l09a-giant-scale-services"></a> L09a: Giant Scale Services</h2>
<h3 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h3>
<ul>
<li>The module focuses on managing large data centers and internet scale computing.</li>
<li>The module addresses systems issues in managing large data centers, programming big data applications, and disseminating content on the web in a scalable manner.</li>
<li>The module builds on previous lessons to harden distributed systems issues and handle scale on the order of thousands of processes and failures.</li>
<li>Failures are inevitable, and the module will address how to handle them.</li>
</ul>
<h3 id="generic-service-model-of-giant-scale-services"><a class="markdownIt-Anchor" href="#generic-service-model-of-giant-scale-services"></a> Generic Service Model of Giant Scale Services</h3>
<p><img src="https://i.imgur.com/mZmtQ2W.png" alt=""></p>
<ul>
<li>The web portal example used in the module is Gmail, a popular email service provided by Google.</li>
<li>The architecture within a site typically consists of thousands of servers, all interconnected through a high-bandwidth communication backplane, and connected to data stores to process incoming client requests.</li>
<li>The servers may optionally use a backplane that allows them to talk to one another for servicing any particular request, which helps to distribute the load evenly.</li>
<li>The load manager plays a crucial role in ensuring that the client traffic is balanced among all the servers, and no server is overloaded, which would cause the service to slow down or fail.</li>
<li>In addition to load balancing, the load manager also monitors the state of the servers and shields incoming client requests from any partial failures that may happen internally within a particular site.</li>
<li>The load manager typically uses various algorithms and techniques to balance the load, such as round-robin, least-connections, or IP hashing.</li>
<li>Embarrassingly parallel refers to the fact that the incoming client requests are all independent of one another and can be handled in parallel as long as there is enough server capacity to meet all the incoming requests. This is a characteristic of most giant scale services.</li>
</ul>
<ol start="5">
<li>Clusters as Workhorses</li>
</ol>
<p><img src="https://i.imgur.com/0XOda7V.png" alt=""></p>
<ul>
<li>Computational clusters are the workhorses of giant scale services and are employed in modern data centers.</li>
<li>Each node in the cluster may itself be an SMP, and the advantages of structuring computational resources as a cluster of machines include absolute scalability, cost and performance control, and incremental scalability.</li>
<li>Computational clusters offer incremental scalability by adding more nodes to the cluster to increase performance, or scaling back when the volume of requests decreases.</li>
</ul>
<ol start="7">
<li>Load Management Choices:</li>
</ol>
<p><img src="https://i.imgur.com/DmJCSlP.png" alt=""></p>
<ul>
<li>Load management can be done at any of the seven layers of the OSI reference model, with higher layers offering more functionality in terms of dealing with server failures and directing incoming client requests.</li>
<li>Load managers operating at the transport level or higher can dynamically isolate down server nodes from the external world, have service-specific front end nodes, and co-opt client devices in load management.</li>
</ul>
<ol start="8">
<li>Load Management at Network Level:</li>
</ol>
<ul>
<li>Load management at the network level is done using round-robin DNS servers, which assign different IP addresses corresponding to different servers to incoming client requests for good load balance.</li>
<li>The assumption in this model is that all servers are identical and that data is fully replicated, so any incoming request can be sent to any server, and the data needed to satisfy the request is available.</li>
<li>The advantage of using round-robin DNS servers is good load balance, but the disadvantage is that it cannot hide down server nodes from the external world.</li>
</ul>
<h3 id="dq-principle"><a class="markdownIt-Anchor" href="#dq-principle"></a> DQ principle</h3>
<p><img src="https://i.imgur.com/6kVTPLF.png" alt=""></p>
<ul>
<li>The DQ principle is used to manage incoming client requests and the data set available for handling those requests on a server.</li>
<li>The server has all the data required for dealing with incoming client queries, called the full data set (Df).</li>
<li>The offered load to the server is called Q0, which is the amount of requests hitting the server per unit time.</li>
<li>The yield (Q) is the ratio of completed requests to the offered load, and ideally should be one, but may be less than one if the server is not able to deal with the offered load entirely.</li>
<li>The available data set for processing each query may be less than the full data set due to failures of some of the data servers or the load on the server, and is called the harvest (Dv/Df).</li>
<li>The product DQ, representing the data server query and the rate of query coming into the server, is a constant for a given server capacity.</li>
<li>To increase the number of clients being served, the harvest can be decreased while keeping the yield the same.</li>
<li>To give the complete data that is needed for serving a query, the yield can be decreased while keeping the harvest constant.</li>
<li>DQ represents a system constant for the server’s capacity, and the system administrator can choose to sacrifice yield for harvest or harvest for yield.</li>
<li>For network-bound applications, DQ is much more intuitive than traditional measures like I/O operations per second (IOOPS).</li>
<li>Uptime is another metric that system administrators use, but it is not very intuitive for giant-scale services because if there are no queries during the mean-time-to-repair (MTTR), then the uptime is not a good measure of how well a server is performing.</li>
<li>The DQ principle is powerful in advising the system administrator on how to architect the system, including how much to replicate, how much to partition the data set, and how to gracefully degrade the servers when the volume of incoming traffic increases beyond a server capacity.</li>
</ul>
<p><img src="https://i.imgur.com/4lfmAdl.png" alt=""><br>
<img src="https://i.imgur.com/XfUMKgn.png" alt=""></p>
<p>Section 12: Online Evolution and Growth</p>
<p><img src="https://i.imgur.com/VjbFQX7.png" alt=""></p>
<ul>
<li>Services are continuously evolving, requiring upgrades to servers in data centers.</li>
<li>Fast reboot involves bringing down all servers at once to upgrade, resulting in complete loss of service for the duration of upgrade.</li>
<li>Rolling upgrade involves upgrading servers one at a time, resulting in service availability but with periodic DQ loss.</li>
<li>Big flip involves bringing down half the nodes at once to upgrade, resulting in 50% service availability for the duration of upgrade.</li>
<li>DQ loss is a constant, and system administrators have a choice in how they dish out the loss to the user community.</li>
</ul>
<p>Section 13: Online Evolution and Growth (cont)</p>
<ul>
<li>The DQ principle helps system designers optimize for yield or harvest for a given system capacity.</li>
<li>It also helps in coming up with explicit policies for graceful degradation of services during server failure, load saturation, or planned upgrades.</li>
</ul>
<p>Section 14: Giant Scale Services Conclusion</p>
<ul>
<li>Giant scale services are network bound, not disk I/O bound.</li>
<li>The DQ principle helps optimize service and plan for graceful degradation during various scenarios.</li>
</ul>
<h2 id="l09b-mapreduce"><a class="markdownIt-Anchor" href="#l09b-mapreduce"></a> L09b: MapReduce</h2>
<ol>
<li>MapReduce Introduction:</li>
</ol>
<ul>
<li>Big data refers to large data sets that take a long time to compute.</li>
<li>Applications that work on big data need to exploit parallelism available in data centers.</li>
<li>Programming in the large requires parallelizing an application across many machines and handling data distribution and failure handling.</li>
</ul>
<ol start="2">
<li>MapReduce:</li>
</ol>
<p><img src="https://i.imgur.com/uh0tUTR.png" alt=""></p>
<ul>
<li>MapReduce is a programming environment for dealing with big data applications.</li>
<li>The input to MapReduce is considered as a set of records identified by a key-value pair.</li>
<li>The MapReduce framework requires two user-defined functions: map and reduce.</li>
<li>Both map and reduce functions take key-value pairs as inputs and produce key-value pairs as outputs.</li>
<li>The example of finding specific names of individuals in a corpus of documents is used to explain the MapReduce framework.</li>
<li>The map function looks for unique names in the corpus of documents and outputs the number of times each name occurs in a file.</li>
<li>The reduce function aggregates the values from the map function and outputs the total number of occurrences for each unique name.</li>
<li>The programming environment handles the plumbing between the output of the map function and input of the reduce function, as well as other details such as the number of mapper and reducer instances required.</li>
</ul>
<p>Section 3: Why MapReduce</p>
<p><img src="https://i.imgur.com/6QszE4y.png" alt=""></p>
<ul>
<li>MapReduce is a programming framework for big data applications.</li>
<li>Many processing steps in giant-scale services are expressible as MapReduce, such as seat availability searches, website frequency analysis, word indexing, and page ranking.</li>
<li>These applications are embarrassingly parallel and work on big datasets, making them ideal for taking advantage of the computation resources in a data center.</li>
<li>Domain expertise in the form of map and reduce functions is required from the app developer, but the programming system handles the heavy lifting such as instantiating mappers and reducers and data movement.</li>
</ul>
<p>Section 4: Heavy Lifting Done by the Runtime</p>
<p><img src="https://i.imgur.com/789SyjC.png" alt=""></p>
<ul>
<li>The programming library splits the input key-value pairs into M splits and spawns a master and worker threads for map and reduce functions.</li>
<li>Mappers are assigned to worker threads and produce intermediate key-value pairs that are buffered in memory and periodically written to local disks.</li>
<li>Reducers are assigned to worker threads based on the number of unique keys specified by the app developer and pull data from all mappers using remote read.</li>
<li>Once all reducers have completed their work, the map reduce computation is complete.</li>
</ul>
<p>Section 5: Heavy Lifting Done by the Runtime (cont)</p>
<ul>
<li>The programming framework sorts input data from mappers and calls the user-supplied reduce function for each key with intermediate values.</li>
<li>Each reduce function writes to a final output file for its partition and notifies the master when it’s done.</li>
<li>The master manages available resources to handle m and R splits and assigns workers to handle each split.</li>
</ul>
<p>Section 6: Issues to be handled by the Runtime</p>
<ul>
<li>Runtime system manages and map_reduce computation.</li>
<li>Master data structures include location of files created by mappers and namespace of files created by mappers.</li>
<li>Scoreboard keeps track of mappers and reducers currently assigned to work on different splits.</li>
<li>Fault tolerance is important, as a mapper may not respond in a timely manner, and master may assume it’s dead and restart on a different node.</li>
<li>Locality management ensures that the working set of computations fits in the closest level of the memory hierarchy of a process.</li>
<li>Task granularity is important for good load balance of computational resources.</li>
<li>Programming framework offers several refinements to the basic model, such as overriding the partitioning function and combining function for mapping and reduce functions.</li>
</ul>
<p>Section 7: MapReduce Conclusion</p>
<ul>
<li>The power of MapReduce is its simplicity.</li>
<li>The domain expert only needs to write the Map and Reduce functions for their application.</li>
<li>The runtime system manages all the heavy lifting under the covers.</li>
</ul>
<h2 id="l09c-content-delivery-networks"><a class="markdownIt-Anchor" href="#l09c-content-delivery-networks"></a> L09c: Content Delivery Networks</h2>
<p>Introduction</p>
<ul>
<li>The internet and World Wide Web provide vast amounts of information to users.</li>
<li>Content creation happens both by individuals and businesses like CNN, BBC, and NBC.</li>
<li>This section will focus on content delivery networks (CDNs) and how information is stored and distributed.</li>
<li>Previous sections focused on the server end of giant scale services, data organization, and programming models for big data.</li>
<li>CDNs deal with the issue of scale in distributing information worldwide to users.</li>
</ul>
<p>Summary of Section 3: DHT Introduction<br>
<img src="https://i.imgur.com/6reJ4A9.png" alt=""></p>
<ul>
<li>Content distribution networks allow for the distribution and storage of content on the internet</li>
<li>Key-value pairs are used to identify unique content and the location of the node where the content is stored</li>
<li>DHT, or distributed hash table, is a distributed solution to store key-value pairs on the internet</li>
<li>DHT uses a key-space namespace and node-space namespace to create unique signatures for content and IP addresses</li>
<li>The API for manipulating the DHT data structure includes putkey and getkey functions</li>
</ul>
<p>Summary of Section 4: DHT Details<br>
<img src="https://i.imgur.com/1BGPiiw.png" alt=""></p>
<ul>
<li>DHT deals with the key-space namespace and node-space namespace</li>
<li>The key-space namespace is managed by generating a unique key for content using an algorithm like SHA-1</li>
<li>The node-space namespace is managed by creating an SHA-1 hash of IP addresses of nodes that want to share content</li>
<li>The objective of DHT is to store a key in a node ID that is very close to the key</li>
<li>The API for manipulating DHT includes putkey and getkey functions</li>
</ul>
<p>Certainly! Here are some additional details on the topics covered in each section:</p>
<ol start="3">
<li>DHT:</li>
</ol>
<ul>
<li>Content distribution networks (CDNs) are used to distribute user-generated content to others on the internet.</li>
<li>To distribute content, a content hash is created to ensure uniqueness of the content and is paired with the node ID of the computer where the content is stored.</li>
<li>The key-value pair is then stored in a distributed hash table (DHT), which is a distributed solution for storing key-value pairs so that they can be discovered by others.</li>
<li>The DHT works by storing key-value pairs on nodes whose IDs are close enough to the key itself.</li>
<li>The API for manipulating the DHT data structure includes putkey (to store a key-value pair) and getkey (to retrieve a value associated with a key-value pair).</li>
</ul>
<ol start="4">
<li>DHT Details:</li>
</ol>
<ul>
<li>The DHT manages two namespaces: the key-space namespace and the node-space namespace.</li>
<li>Content is disseminated using a unique key generated by an algorithm like SHA-1, which creates a 160-bit key.</li>
<li>Node IDs are created using the same algorithm, such as encoding IP addresses into a 160-bit node ID.</li>
<li>The objective is to store a key in a node ID such that the key is very close to the node ID, and the API for manipulating the DHT includes putkey and getkey.</li>
</ul>
<ol start="5">
<li>CDN (An Overlay Network):<br>
<img src="https://i.imgur.com/7HqjOFt.png" alt=""></li>
</ol>
<ul>
<li>A CDN is an example of an overlay network, which is a virtual network on top of the physical network.</li>
<li>Overlay networks are used at the user level to map virtual addresses to IP addresses for sending messages.</li>
<li>User-level routing tables are constructed by exchanging mapping information between friends to discover one another and send messages.</li>
<li>Overlay networks allow content to be shared and distributed among a set of users who have exchanged information with one another.</li>
</ul>
<ol start="6">
<li>Overlay Networks in General:</li>
</ol>
<ul>
<li>Overlay networks are a general principle and exist at the operating system level, such as the IP network overlaying the local area network.</li>
<li>IP addresses translate to MAC addresses to traverse the local area network to reach the destination.</li>
<li>CDN is an overlay on top of TCP/IP, and a node ID maps to an IP address for sending messages.</li>
</ul>
<ol start="7">
<li>DHT and CDNs:<br>
<img src="https://i.imgur.com/dOBg47Z.png" alt=""></li>
</ol>
<ul>
<li>DHT is an implementation vehicle for CDNs to populate the routing table at the user level.</li>
<li>Put operation is used for placement of key-value pairs, and get operation is used for retrieval of a value associated with a key-value pair.</li>
<li>The construction of the routing table involves storing key-value pairs on nodes and using the put and get operations to retrieve them.</li>
</ul>
<p>Summary of section 8:<br>
<img src="https://i.imgur.com/WApbZ7g.png" alt=""></p>
<ul>
<li>The traditional approach for constructing a distributed hash table involves a greedy algorithm where a key value is placed in a node that is very close to the key, and when retrieving a key, the algorithm looks for the node closest to the key.</li>
<li>Routing tables at each node in the system only list the nodes that can be communicated with directly.</li>
<li>If a node is not in the routing table, the algorithm goes to a node that is close enough to the desired node, hoping that it will know how to communicate with the desired node.</li>
<li>The goal of the greedy approach is to get to the desired destination as quickly as possible with the minimum number of hops.</li>
</ul>
<p>Summary of section 9:<br>
<img src="https://i.imgur.com/L6CacoJ.png" alt=""></p>
<ul>
<li>The greedy approach leads to a metadata server overload problem where a node that is closest to a key value pair becomes congested with traffic from puts and gets.</li>
<li>This creates a tree saturation problem where nodes in close proximity to the congested node also become congested.</li>
<li>If content becomes popular, there is also an origin server overload problem where the server hosting the content becomes inundated with download requests.</li>
<li>A content distribution network can solve the origin server overload problem by automatically mirroring content at geo-local sites and dynamically routing requests to the closest mirror.</li>
<li>However, this solution is expensive and not accessible to individual content providers.</li>
<li>The Coral System is a solution that democratizes content distribution and addresses both the metadata server overload and origin server overload problems.</li>
</ul>
<p>Summary of section 10:<br>
<img src="https://i.imgur.com/dvhuL4n.png" alt=""></p>
<ul>
<li>The first solution to the origin server overload problem is to use a web proxy, but it is not suitable for dynamic content.</li>
<li>Content distribution networks solve the origin server overload problem but are expensive and not accessible to individual content providers.</li>
<li>The Coral System is a solution that democratizes content distribution and addresses both the metadata server overload and origin server overload problems.</li>
</ul>
<ol start="11">
<li>Greedy Approach Leads to Tree Saturation:<br>
<img src="https://i.imgur.com/sP84B76.png" alt=""></li>
</ol>
<ul>
<li>The greedy approach of constructing a DHT leads to tree saturation which happens at the node that maps to a lot of clustered keys.</li>
<li>The coral approach avoids tree saturation by not being greedy and not necessarily storing the key K in the node N with an ID equal to K.</li>
<li>The Coral DHT implements a sloppy DHT that spreads metadata overload so that no single node in the democratic process of helping one another is saturated.</li>
<li>The distance between the source and the destination in Coral key-based routing is computed by XORing the bit patterns of the node IDs for the source and the destination.</li>
</ul>
<ol start="12">
<li>Key-Based Routing:<br>
<img src="https://i.imgur.com/aVjeEY8.png" alt=""></li>
</ol>
<ul>
<li>The greedy approach is to get as close to the desired destination in the node ID namespace and ask a nearby node if it has a way of getting to the desired destination.</li>
<li>The objective in the greedy approach is reaching the destination with the fewest number of hops.</li>
<li>The Coral key-based routing slowly progresses towards the desired destination by going to some node that is half the distance to the destination in the node ID namespace in each hop.</li>
</ul>
<ol start="13">
<li>Coral Key-Based Routing:<br>
<img src="https://i.imgur.com/IBcnqUd.png" alt=""></li>
</ol>
<ul>
<li>The Coral key-based routing reduces the distance by approximately half in each hop and avoids being greedy to avoid congestion and tree saturation.</li>
<li>The distance between the source and the destination is computed by XORing the bit patterns of the node IDs for the source and the destination.</li>
<li>In each hop, the Coral key-based routing goes to some node that is half the distance to the destination in the node ID namespace.</li>
</ul>
<ol start="14">
<li>Key-Based Routing in Coral:<br>
<img src="https://i.imgur.com/zKcI0Q5.png" alt=""></li>
</ol>
<ul>
<li>In Coral key-based routing, the table is populated with the XOR distance of each node that is directly reachable from the source to the desired destination.</li>
<li>In each hop, the distance to the destination is reduced by half by going to a node that is approximately half the distance to the destination in the node ID namespace.</li>
<li>In each hop, the node may not have a direct way to reach the desired node, and a nearby node is contacted to obtain information on nodes that are close enough to the desired destination.</li>
</ul>
<p>Section 15: Coral Sloppy DHT - Put Operation<br>
<img src="https://i.imgur.com/x6q3TsB.png" alt=""></p>
<ul>
<li>The primitives available in Coral for manipulating the sloppy DHT are put and get operations.</li>
<li>The put operation takes two parameters: Key and value.</li>
<li>Key is the content hash, and value is the node ID of the proxy with the content for that key.</li>
<li>Put can be initiated by the origin server or a node that wants to serve as a proxy.</li>
<li>The result of doing the put operation is to store this key value in some metadata server.</li>
<li>We need to place this key value in an appropriate node based on space and time metrics.</li>
<li>Full state means a particular node is already storing l values for a key.</li>
<li>Loaded is stating how many requests per unit time a node is willing to entertain for a particular key.</li>
<li>The Coral key-based routing algorithm reduces the distance by half to find the appropriate node to place the key.</li>
<li>We ask each node along the way if it is loaded or full, and if it is, we retract our steps and choose an appropriate node.</li>
<li>The Coral put operation chooses an appropriate node that is neither full nor loaded to entertain requests for retrieving the particular key value pair.</li>
</ul>
<p>Section 16: Coral Sloppy DHT (cont) - Get Operation<br>
<img src="https://i.imgur.com/5UOslTS.png" alt=""></p>
<ul>
<li>The get operation works similarly to the put operation.</li>
<li>We go to a node that is half the distance to the key we are looking for.</li>
<li>If the content is popular, then multiple proxies may have gotten the key value pair.</li>
<li>They may have put their own node IDs as a potential node for the content.</li>
<li>Our metadata server may not necessarily have to be the destination which exactly matches that key.</li>
</ul>
<p>Section 17: Coral in Action</p>
<ul>
<li>Coral allows for user-generated content to be distributed in a democratic fashion.</li>
<li>The load for serving as a metadata server and content server can get naturally distributed.</li>
<li>Naomi creates a unique signature for her content and uses Coral to put it out on the internet.</li>
<li>Jacques uses Coral to get the content by following the key-based routing algorithm and ends up at David’s computer.</li>
<li>Jacques serves as a proxy for the content and puts a new key value pair in the system.</li>
<li>Kamal also uses Coral to get the content and ends up getting it from Jacques, who serves as a proxy.</li>
<li>The metadata server load is distributed and the origin server is not stressed.</li>
</ul>
<p>Section 18: Content Delivery Networks Conclusion</p>
<ul>
<li>Coral offers a participatory approach to democratize content generation, storage, and distribution.</li>
<li>Commercial CDNs like Akamai contractually mirror content for customers and deploy their own mirrors to deal with increases in requests.</li>
<li>Lesson module on Internet Scale Computing is complete.</li>
</ul>

</div>


  <div class="book-comments">
    




  </div>



<script src="/js/book-post.js"></script>


<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js" integrity="sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

        </div>
      </div>
      <div class="column col-2 hide-lg">
        <div class="book-post-info">
  
    <div class="book-post-meta">
  
  <div class="divider"></div>
</div>

  

  <div class="book-tocbot">
</div>
<div class="book-tocbot-menu">
  <a class="book-toc-expand" onclick="expand_toc()">Expand all</a>
  <a onclick="go_top()">Back to top</a>
  <a onclick="go_bottom()">Go to bottom</a>
</div>


<script src="/js/book-toc.js"></script>

</div>

      </div>
    </div>
  </div>
  
  <a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>

</body>
</html>


<script src="/js/book.js"></script>
