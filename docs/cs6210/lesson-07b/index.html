<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">

    
      <link rel="icon" href="/favicon.png" />
    

    <title>
        
          Eric&#39;s CS Notes
        
    </title>

    <!-- Spectre.css framework -->
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-exp.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-icons.min.css">

    <!-- theme css & js -->
    
<link rel="stylesheet" href="/css/book.css">

    
<script src="/js/book.js"></script>


    <!-- tocbot -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.18.2/tocbot.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.18.2/tocbot.css">
    
    <!-- katex css -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css" integrity="sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/" crossorigin="anonymous">

    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/zooming/2.1.1/zooming.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    const zooming = new Zooming()
    zooming.listen('.book-content img')
})
</script>

<meta name="generator" content="Hexo 6.3.0"></head>


<body>

<div class="book-container">
  <div class="book-sidebar">
    <div class="book-brand">
  <a href="/">
    <img src="/favicon.png">
    <span>ERIC&#39;S CS NOTES</span>
  </a>
</div>

    <div class="book-menu">
  <!--
## Introduction to Probability

* [Unit 1: Probability models and axioms](/introduction-to-probability/unit-1/index.html)
* [Unit 2: Conditioning and independence](/introduction-to-probability/unit-2/index.html)
* [Unit 3: Counting](/introduction-to-probability/unit-3/index.html)
* [Unit 4: Discrete random variables](/introduction-to-probability/unit-4/index.html)
* [Unit 5: Continuous random variables](/introduction-to-probability/unit-5/index.html)
* [Unit 6: Further topics on random variables](/introduction-to-probability/unit-6/index.html)
* [Unit 7: Bayesian inference](/introduction-to-probability/unit-7/index.html)
* [Unit 8: Limit theorems and classical statistics](/introduction-to-probability/unit-8/index.html)
* [Unit 9: Bernoulli and Poisson processes](/introduction-to-probability/unit-9/index.html)
* [Unit 10: Markov chains](/introduction-to-probability/unit-10/index.html)
-->
<!--
## Multivariable Calculus

* [Unit 1: Thinking about multivariable functions](/multivariable-calculus/unit-1/index.html)
* [Unit 2: Derivatives of multivariable functions](/multivariable-calculus/unit-2/index.html)

* [Unit 3: Applications of multivariable derivatives](/multivariable-calculus/unit-3/index.html)
* [Unit 4: Integrating multivariable functions](/multivariable-calculus/unit-4/index.html)
* [Unit 5: Green's, Stokes', and the divergence theorems](/multivariable-calculus/unit-5/index.html)
-->
<h2 id="cs6210-advanced-operating-systems"><a class="markdownIt-Anchor" href="#cs6210-advanced-operating-systems"></a> CS6210: Advanced Operating Systems</h2>
<ul>
<li><a href="/cs6210/lesson-02/index.html">Lesson 2: OS Structure</a></li>
<li><a href="/cs6210/lesson-03/index.html">Lesson 3: Virtualization</a></li>
<li><a href="/cs6210/lesson-04a/index.html">Lesson 4: Parallel Systems - Part 1</a></li>
<li><a href="/cs6210/lesson-04b/index.html">Lesson 4: Parallel Systems - Part 2</a></li>
<li><a href="/cs6210/lesson-05/index.html">Lesson 5: Distributed Systems</a></li>
<li><a href="/cs6210/lesson-06/index.html">Lesson 6: Distributed Objects and Middleware</a></li>
<li><a href="/cs6210/lesson-07a/index.html">Lesson 7: Distributed Subsystems - GMS</a></li>
<li><a href="/cs6210/lesson-07b/index.html">Lesson 7: Distributed Subsystems - DSM</a></li>
<li><a href="/cs6210/lesson-07c/index.html">Lesson 7: Distributed Subsystems - DFS</a></li>
<li><a href="/cs6210/lesson-09/index.html">Lesson 9: Internet Computing</a></li>
<li><a href="/cs6210/lesson-10/index.html">Lesson 10: RT and Multimedia</a></li>
</ul>
<!--
* [Lesson 8: Failures and Recovery](/cs6210/lesson-08/index.html)
* [Lesson 11: Security](/cs6210/lesson-11/index.html)
-->
<h2 id="cs6250-computer-networks"><a class="markdownIt-Anchor" href="#cs6250-computer-networks"></a> CS6250 Computer Networks</h2>
<ul>
<li><a href="/cs6250/week-1-internet-architecture/index.html">Week 1 - Internet Architecture</a></li>
<li><a href="/cs6250/week-2-transport-and-application-layers/index.html">Week 2 - Transport and Application Layers</a></li>
<li><a href="/cs6250/week-3-intradomain-routing/index.html">Week 3 - Intradomain Routing</a></li>
<li><a href="/cs6250/week-4-as-relationships-and-interdomain-routing/index.html">Week 4 - AS Relationships and Interdomain Routing</a></li>
<li><a href="/cs6250/week-5-router-design-and-algorithems-part-1/index.html">Week 5 - Router Design and Algorithms (Part 1)</a></li>
<li><a href="/cs6250/week-6-router-design-and-algorithems-part-2/index.html">Week 6 - Router Design and Algorithms (Part 2)</a></li>
<li><a href="/cs6250/week-7-software-defined-networking-part-1/index.html">Week 7 - Software Defined Networking (Part 1)</a></li>
<li><a href="/cs6250/week-8-software-defined-networking-part-2/index.html">Week 8 - Software Defined Networking (Part 2)</a></li>
<li><a href="/cs6250/week-9-internet-security/index.html">Week 9 - Internet Security</a></li>
<li><a href="/cs6250/week-10-internet-surveillance-and-censorship/index.html">Week 10 - Internet Surveillance and Censorship</a></li>
<li><a href="/cs6250/week-11-applications-video/index.html">Week 11 - Applications Videos</a></li>
<li><a href="/cs6250/week-12-applications-cdns-and-overlay-networks/index.html">Week 12 - Applications CDNs and Overlay Networks</a></li>
</ul>
<h2 id="cs6200-graduate-introduction-to-operating-systems"><a class="markdownIt-Anchor" href="#cs6200-graduate-introduction-to-operating-systems"></a> CS6200 Graduate Introduction to Operating Systems</h2>
<ul>
<li><a href="/cs6200/p1-preparation/index.html">P0 - Preparation</a></li>
<li><a href="/cs6200/p1l2-introduction/index.html">P1L2 - Introduction</a></li>
<li><a href="/cs6200/p2l1-processes-and-process-management/index.html">P2L1 - Processes and Process Management</a></li>
<li><a href="/cs6200/p2l2-threads-and-concurrency/index.html">P2L2 - Threads and Concurrency</a></li>
<li><a href="/cs6200/p2l3-pthread/index.html">P2L3 - PThread</a></li>
<li><a href="/cs6200/p2l4-thread-design-consideration/index.html">P2L4 - Thread Design Considerations</a></li>
<li><a href="/cs6200/p2l5-thread-performance-consideration/index.html">P2L5 - Thread Performance Considerations</a></li>
<li><a href="/cs6200/p3l1-scheduling/index.html">P3L1 - Scheduling</a></li>
<li><a href="/cs6200/p3l2-memory-management/index.html">P3L2 - Memory Management</a></li>
<li><a href="/cs6200/p3l3-inter-process-communication/index.html">P3L3 - Inter-Process Communication</a></li>
<li><a href="/cs6200/p3l4-synchronization-constructs/index.html">P3L4 - Synchronization Constructs</a></li>
<li><a href="/cs6200/p3l5-io-management/index.html">P3L5 - I/O Management</a></li>
<li><a href="/cs6200/p3l6-virtualization/index.html">P3L6 - Virtualization</a></li>
</ul>
<!--
* [P4L1 - Remote Procedure Calls](/cs6200/p4l1-remote-procedure-calls/index.html) 
* [P4L2 - Distributed File Systems](/cs6200/p4l2-distributed-file-systems/index.html) 
* [P4L3 - Distributed Shared Memory](/cs6200/p4l3-distributed-shared-memory/index.html) 
* [P4L4 - Datacenter Technologies](/cs6200/p4l4-datacenter-technologies/index.html) 
-->
<h2 id="algorithms-part-ii"><a class="markdownIt-Anchor" href="#algorithms-part-ii"></a> Algorithms: Part II</h2>
<ul>
<li><a href="/algorithms-2/week-1/index.html">Week 1 - Undirected Graph &amp; Directed Graph</a></li>
<li><a href="/algorithms-2/week-2/index.html">Week 2 - Minimum Spanning Trees &amp; Shortest Path</a></li>
<li><a href="/algorithms-2/week-3/index.html">Week 3 - Maximum Flow and Minimum Cut &amp; String Sort</a></li>
<li><a href="/algorithms-2/week-4/index.html">Week 4 - Tries &amp; Substring Search</a></li>
<li><a href="/algorithms-2/week-5/index.html">Week 5 - Regular Expressions</a></li>
<li><a href="/algorithms-2/week-6/index.html">Week 6 - Reductions</a></li>
</ul>
<h2 id="algorithms-part-i"><a class="markdownIt-Anchor" href="#algorithms-part-i"></a> Algorithms: Part I</h2>
<ul>
<li><a href="/algorithms-1/week-1/index.html">Week 1 - Union-Find &amp; Analysis of Algorithms</a></li>
<li><a href="/algorithms-1/week-2/index.html">Week 2 - Stacks and Queues &amp; Elementary Sorts</a></li>
<li><a href="/algorithms-1/week-3/index.html">Week 3 - Mergesort &amp; Quicksort</a></li>
<li><a href="/algorithms-1/week-4/index.html">Week 4 - Priority Queues &amp; Elementary Symbols</a></li>
<li><a href="/algorithms-1/week-5/index.html">Week 5 - Balanced Search Trees</a></li>
<li><a href="/algorithms-1/week-6/index.html">Week 6 - Hash Tables</a></li>
</ul>
<h2 id="introduction-to-software-design-and-architecture"><a class="markdownIt-Anchor" href="#introduction-to-software-design-and-architecture"></a> Introduction to Software Design and Architecture</h2>
<ul>
<li><a href="/introduction-to-software-design-and-architecture/design-pattern/index.html">Design Pattern</a></li>
</ul>
<h2 id="calculus-two-sequences-and-series"><a class="markdownIt-Anchor" href="#calculus-two-sequences-and-series"></a> Calculus Two: Sequences and Series</h2>
<ul>
<li><a href="/calculus-two/week-1/index.html">Week 1 - Sequences</a></li>
<li><a href="/calculus-two/week-2/index.html">Week 2 - Series</a></li>
<li><a href="/calculus-two/week-3/index.html">Week 3 - Convergence Tests</a></li>
<li><a href="/calculus-two/week-4/index.html">Week 4 - Alternating Series</a></li>
<li><a href="/calculus-two/week-5/index.html">Week 5 - Power Series</a></li>
<li><a href="/calculus-two/week-6/index.html">Week 6 - Taylor Series</a></li>
</ul>
<h2 id="laff-linear-algebra"><a class="markdownIt-Anchor" href="#laff-linear-algebra"></a> LAFF Linear Algebra</h2>
<ul>
<li><a href="/laff-linear-algebra/week-1/index.html">Week 1 - Vectors in Linear Algebra</a></li>
<li><a href="/laff-linear-algebra/week-2/index.html">Week 2 - Linear Transformations and Matrices</a></li>
<li><a href="/laff-linear-algebra/week-3/index.html">Week 3 - Matrix-Vector Operations</a></li>
<li><a href="/laff-linear-algebra/week-4/index.html">Week 4 - Matrix-Vector to Matrix-Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-5/index.html">Week 5 - Matrix- Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-6/index.html">Week 6 - Gaussian Elimination</a></li>
<li><a href="/laff-linear-algebra/week-7/index.html">Week 7 - More Gaussian Elimination and Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-8/index.html">Week 8 - More on Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-9/index.html">Week 9 - Vector Spaces</a></li>
<li><a href="/laff-linear-algebra/week-10/index.html">Week 10 - Vector Spaces, Orthogonality, and Linear Least-Squares</a></li>
<li><a href="/laff-linear-algebra/week-11/index.html">Week 11 - Orthogonal Projection, Low Rank Approximation, and Orthogonal Bases</a></li>
<li><a href="/laff-linear-algebra/week-12/index.html">Week 12 - Eigenvalues and Eigenvectors</a></li>
</ul>
<h2 id="stanford-machine-learning"><a class="markdownIt-Anchor" href="#stanford-machine-learning"></a> Stanford Machine Learning</h2>
<ul>
<li><a href="/stanford-machine-learning/week-1/index.html">Week 1 - Introduction</a></li>
<li><a href="/stanford-machine-learning/week-2/index.html">Week 2 - Linear Regression with Multiple Variables</a></li>
<li><a href="/stanford-machine-learning/week-3/index.html">Week 3 - Logistic Regression &amp; Regularization</a></li>
<li><a href="/stanford-machine-learning/week-4/index.html">Week 4 - Neural Networks: Representation</a></li>
<li><a href="/stanford-machine-learning/week-5/index.html">Week 5 - Neural Networks: Learning</a></li>
<li><a href="/stanford-machine-learning/week-6a/index.html">Week 6a - Advice for Applying Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-6b/index.html">Week 6b - Machine Learning System Design</a></li>
<li><a href="/stanford-machine-learning/week-7/index.html">Week 7 - Support Vector Machines</a></li>
<li><a href="/stanford-machine-learning/week-8/index.html">Week 8 - Unsupervised Learning &amp; Dimensionality Reduction</a></li>
<li><a href="/stanford-machine-learning/week-9a/index.html">Week 9a - Anomaly Detection</a></li>
<li><a href="/stanford-machine-learning/week-9b/index.html">Week 9b - Recommender Systems</a></li>
<li><a href="/stanford-machine-learning/week-10/index.html">Week 10 - Large Scale Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-11/index.html">Week 11 - Application Example: Photo OCR</a></li>
</ul>
<h2 id="calculus-one"><a class="markdownIt-Anchor" href="#calculus-one"></a> Calculus One</h2>
<ul>
<li><a href="/calculus-one/week-2-3/index.html">Week 2-3 - Functions &amp; Limits</a></li>
<li><a href="/calculus-one/week-4/index.html">Week 4 - The Beginning of Derivatives</a></li>
<li><a href="/calculus-one/week-5/index.html">Week 5 - Techniques of Differentiation</a></li>
<li><a href="/calculus-one/week-6/index.html">Week 6 - Chain Rule</a></li>
<li><a href="/calculus-one/week-7/index.html">Week 7 - Derivatives of Trigonometric Functions</a></li>
<li><a href="/calculus-one/week-8/index.html">Week 8 - Derivatives in the Real World</a></li>
<li><a href="/calculus-one/week-9/index.html">Week 9 - Optimization</a></li>
<li><a href="/calculus-one/week-10/index.html">Week 10 - Linear Approximation</a></li>
<li><a href="/calculus-one/week-11-12/index.html">Week 11-12 - Antidifferentiation &amp; Integration</a></li>
<li><a href="/calculus-one/week-13/index.html">Week 13 - Fundamental Theorem of Calculus</a></li>
<li><a href="/calculus-one/week-14/index.html">Week 14 - Substitution Rule</a></li>
<li><a href="/calculus-one/week-15/index.html">Week 15 - Techniques of Integration</a></li>
<li><a href="/calculus-one/week-16/index.html">Week 16 - Applications of Integration</a></li>
</ul>
<h2 id="computational-thinking"><a class="markdownIt-Anchor" href="#computational-thinking"></a> Computational Thinking</h2>
<ul>
<li><a href="/computational-thinking/lecture-1/index.html">Lecture 1 - Optimization and Knapsack Problem</a></li>
<li><a href="/computational-thinking/lecture-2/index.html">Lecture 2 - Decision Trees and Dynamic Programming</a>
<ul>
<li><a href="/computational-thinking/lecture-2-powerset/index.html">Exercise: Power Set Function</a></li>
</ul>
</li>
<li><a href="/computational-thinking/lecture-3/index.html">Lecture 3 - Graphs</a></li>
<li><a href="/computational-thinking/lecture-4-5/index.html">Lecture 4-5 - Plotting</a></li>
<li><a href="/computational-thinking/lecture-6-7/index.html">Lecture 6-7 - Stochastic Programs &amp; Inferential Statistics</a></li>
<li><a href="/computational-thinking/lecture-8/index.html">Lecture 8 - Monte Carlo Simulation</a></li>
<li><a href="/computational-thinking/lecture-9/index.html">Lecture 9 - Sampling and Standard Error</a></li>
<li><a href="/computational-thinking/lecture-10-11/index.html">Lecture 10-11 - Experimental Data</a></li>
<li><a href="/computational-thinking/lecture-12/index.html">Lecture 12 - Machine Learning</a></li>
<li><a href="/computational-thinking/lecture-13/index.html">Lecture 13 - Statistical Abuses</a></li>
</ul>
<h2 id="effective-thinking-through-mathematics"><a class="markdownIt-Anchor" href="#effective-thinking-through-mathematics"></a> Effective Thinking Through Mathematics</h2>
<ul>
<li><a href="/effective-thinking-through-mathematics/note/index.html">Note</a></li>
<li><a href="/effective-thinking-through-mathematics/week-4-telling-the-story-of-infinity/index.html">Week 4 (/Telling the Story of Infinity)</a></li>
<li><a href="/effective-thinking-through-mathematics/week-5-telling-the-story-of-the-euler-circuit-theorem/index.html">Week 5 (/Telling the Story of Euler Circuit Theorem)</a></li>
</ul>
<h2 id="cs50-introduction-to-computer-science"><a class="markdownIt-Anchor" href="#cs50-introduction-to-computer-science"></a> CS50 Introduction to Computer Science</h2>
<ul>
<li><a href="/cs50/week-1/index.html">Week 1 - C</a></li>
<li><a href="/cs50/week-2/index.html">Week 2 - Arrays</a></li>
<li><a href="/cs50/week-3/index.html">Week 3 - Algorithms</a></li>
<li><a href="/cs50/week-4/index.html">Week 4 - Memory</a></li>
<li><a href="/cs50/week-5/index.html">Week 5 - Data Structures</a></li>
<li><a href="/cs50/week-6/index.html">Week 6 - HTTP</a></li>
<li><a href="/cs50/week-7-10/index.html">Week 7-10 - Machine Learning/Python/SQL/Javascript</a></li>
</ul>
<h2 id="others"><a class="markdownIt-Anchor" href="#others"></a> Others</h2>
<ul>
<li><a href="/symbols/index.html">Symbols of Mathematics</a></li>
<li><a href="/glossary/index.html">Glossary</a></li>
</ul>
<h2 id="about-me"><a class="markdownIt-Anchor" href="#about-me"></a> <a target="_blank" rel="noopener" href="https://ericy.me/about/">About Me</a></h2>

</div>


<script src="/js/book-menu.js"></script>

  </div>

  <div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseover="add_inner()" onmouseleave="remove_inner()">
  <div class="sidebar-toggle-inner"></div>
</div>

<script>
function add_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.add('show')  
}

function remove_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.remove('show')
}

function sidebar_toggle() {
    let sidebar_toggle = document.querySelector('.sidebar-toggle')
    let sidebar = document.querySelector('.book-sidebar')
    let content = document.querySelector('.off-canvas-content')
    if (sidebar_toggle.classList.contains('extend')) { // show
        sidebar_toggle.classList.remove('extend')
        sidebar.classList.remove('hide')
        content.classList.remove('extend')
    }
    else { // hide
        sidebar_toggle.classList.add('extend')
        sidebar.classList.add('hide')
        content.classList.add('extend')
    }
}
</script>

  <div class="off-canvas-content">
    <div class="columns">
      <div class="column col-10 col-lg-12">
        <div class="book-navbar">
          <!-- For Responsive Layout -->

<header class="navbar">
  <section class="navbar-section">
    <a onclick="open_sidebar()">
      <i class="icon icon-menu"></i>
    </a>
  </section>
</header>

        </div>
        <div class="book-content">
          <div class="book-post">
  <h1 id="lesson-7-distributed-subsystems-distributed-shared-memory"><a class="markdownIt-Anchor" href="#lesson-7-distributed-subsystems-distributed-shared-memory"></a> Lesson 7: Distributed Subsystems - Distributed Shared Memory</h1>
<p>Can we make the cluster appear like a shared memory machine?</p>
<h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2>
<ul>
<li>Distributed Shared Memory (DSM) is a <strong>software implementation</strong> that provides the illusion of shared memory to applications in a distributed system.</li>
<li>DSM allows exploiting remote memories in a cluster to make it appear as a shared memory machine, simplifying application development.</li>
<li>DSM works by creating an operating system abstraction that provides the illusion of shared memory to applications, even though nodes in the local area network do not physically share memory.</li>
</ul>
<h2 id="cluster-as-a-parallel-machine"><a class="markdownIt-Anchor" href="#cluster-as-a-parallel-machine"></a> Cluster as a Parallel Machine</h2>
<h3 id="sequential-program"><a class="markdownIt-Anchor" href="#sequential-program"></a> Sequential Program</h3>
<img src="https://i.imgur.com/2o38mhX.png" style="width: 800px" />
<ul>
<li>To exploit a cluster starting from a sequential program, one possibility is automatic parallelization, which means writing a sequential program and letting a tool identify opportunities for parallelism and map it to the cluster.</li>
<li>Automatic parallelization is an example of implicitly parallel programming, where the program is not written explicitly in parallel but the tool identifies opportunities for parallelism and maps it to the cluster.</li>
<li>High Performance Fortran is an example of a programming language that does automatic parallelization, where the user uses directives for distribution of data and computation.</li>
<li>Automatic parallelization works well for certain classes of programs called data parallel programs, where the data accesses are fairly static and determinable at compile time.</li>
</ul>
<h3 id="message-passing"><a class="markdownIt-Anchor" href="#message-passing"></a> Message Passing</h3>
<img src="https://i.imgur.com/tPeuuQC.png" style="width: 800px" />
<ul>
<li>The other way to exploit a cluster is to write the program as a truly parallel program, or explicitly parallel program, where the application programmer thinks about the application and writes the program explicitly in parallel.</li>
<li>There are two styles of explicitly parallel programs: <strong>message passing</strong> style and <strong>shared memory</strong> style.</li>
<li>The message passing style is true to the physical nature of the cluster, where every processor has its private memory and cannot directly access the memory of another processor. It uses a message passing library that provides primitives for application threads to do sends and receives to peers executing on other nodes of the cluster.</li>
<li>Examples of message passing libraries include MPI, PVM, and CLF from Digital Equipment Corporation, and many scientific applications running on large scale clusters use this style of programming using MPI as the message passing fabric.</li>
<li>The downside to the message-passing style of programming is that it is difficult to program, as it requires the programmer to think in terms of coordinating the activities on different processes by explicitly sending and receiving messages from their peers, which is a radical change of thinking in terms of how to structure a program.</li>
</ul>
<h3 id="dsm"><a class="markdownIt-Anchor" href="#dsm"></a> DSM</h3>
<img src="https://i.imgur.com/MZj30m5.png" style="width: 800px" />
<ul>
<li>The DSM abstraction is a way of giving the illusion to the application programmer that all of the memory in the entire cluster is shared, even though it is not physically shared.</li>
<li>The DSM library provides a shared memory semantic to the application program, allowing for an easier transition path from a sequential program or program written on an SMP to a program that runs on the cluster.</li>
<li>The DSM abstraction eliminates the need for marshaling and unmarshaling arguments being passed from one processor to another, as all of that is handled by the fact that there is shared memory.</li>
<li>The DSM abstraction gives the same level of comfort to a programmer who is used to programming on a true shared memory machine when they move to a cluster, as they can use the same set of primitives for synchronization and thread creation.</li>
<li>The advantage of DSM style of writing an explicitly parallel program is that it allows for thinking in terms of shared memory, sharing pointers across the entire cluster, and using locks and barriers for synchronization.</li>
</ul>
<h3 id="history-of-shared-memory-systems"><a class="markdownIt-Anchor" href="#history-of-shared-memory-systems"></a> History of Shared Memory Systems</h3>
<ul>
<li>The history of shared memory systems over the last 20+ years shows the space occupied by the efforts to build shared memory systems in hardware and software.</li>
<li>Software DSM was first thought of in the mid-80s, with systems like Ivy, Clouds Operating System, and similar systems built at UPenn.</li>
<li>In the early '90s, systems like Munin and TreadMarks were built, followed by Blizzard, Shasta, Cashmere, and Beehive in the later half of the '90s.</li>
<li>Structured DSM provided a higher-level abstraction than just memory to computations that needed to be built on a cluster. Systems like Linda, Orca, Stampede, and PTS were built in the early to mid-'90s.</li>
<li>Early hardware shared memory systems like BBN Butterfly and Sequent Symmetry appeared in the market in the mid-'80s, followed by KSR-1, Alewife, and DASH in the early to mid-'90s.</li>
<li>Commercial versions of distributed shared memory machines like SGI Origin 2000 and SGI Altix were built, with thousands of processors in the latter.</li>
<li>IBM Bluegene is another example of a shared memory system, and clusters of SMPs have become the workhorses in data centers.</li>
<li>It is important to reflect on the progress made in shared memory systems and learn about the details of machines built in the past, either in hardware or software.</li>
</ul>
<h2 id="shared-memory-programming"><a class="markdownIt-Anchor" href="#shared-memory-programming"></a> Shared Memory Programming</h2>
<img src="https://i.imgur.com/pcKtHIu.png" style="width: 800px" />
<ul>
<li>Shared memory programming uses synchronization primitives like locks and barriers to protect data structures so that one thread can exclusively modify the data.</li>
<li>Mutual exclusion locks and barrier synchronization are popular synchronization primitives used in shared memory programming.</li>
<li>There are two types of memory accesses that happen in a shared memory program:
<ol>
<li>Normal reads and writes to shared data,</li>
<li>Accesses to synchronization variables used in implementing locks and barriers.</li>
</ol>
</li>
<li>The operating system or user-level threads library provides mutual exclusion locks or barrier primitives, and in implementing those synchronization primitives, algorithms use reads and writes to shared memory.</li>
</ul>
<h3 id="memory-consistency-and-cache-coherence"><a class="markdownIt-Anchor" href="#memory-consistency-and-cache-coherence"></a> Memory Consistency and Cache Coherence</h3>
<ul>
<li><strong>Memory consistency</strong> model is a contract between the application programmer and the system that answers when <strong>the changes made to a shared memory location by one processor</strong> will be <strong>visible to other processes</strong> that share the same memory location in their respective private caches. It deals with the ordering and visibility of memory operations (like read, write, and atomic operations) across different processors or cores in a multiprocessor system.</li>
<li><strong>Cache coherence</strong> addresses the problem that arises when multi-processors have their own <strong>private caches</strong>. When a shared memory location that is cached by other processors, it can lead to inconsistent data across the caches. Cache coherence ensures all caches have a consistent view of the shared memory.</li>
<li>In parallel programming, coherence mechanism must fulfill the guarantee made by the <strong>memory consistency model to the application programmer</strong>.</li>
</ul>
<h3 id="sequential-consistency"><a class="markdownIt-Anchor" href="#sequential-consistency"></a> Sequential Consistency</h3>
<img src="https://i.imgur.com/7l4Gg7n.png" style="width: 800px" />
<ul>
<li>In sequential consistency, memory accesses are expected to happen in textual order on individual processors but the interleaving of memory accesses from different processors is arbitrary.</li>
<li>The memory model preserves atomicity for individual read-write operations and honors the program order.</li>
<li>SC memory model doesn’t distinguish between data accesses and synchronization accesses.</li>
</ul>
<h4 id="sc-memory-model"><a class="markdownIt-Anchor" href="#sc-memory-model"></a> SC Memory Model</h4>
<img src="https://i.imgur.com/MjZGeN3.png" style="width: 800px" />
<ul>
<li>SC memory model <em>doesn’t know the association between locks and data structures</em>.</li>
<li><em>Coherence action is taken on every read-write access</em>, which leads to more overhead and poorer scalability.</li>
</ul>
<h3 id="typical-parallel-program"><a class="markdownIt-Anchor" href="#typical-parallel-program"></a> Typical Parallel Program</h3>
<img src="https://i.imgur.com/J1llQoD.png" style="width: 800px" />
<ul>
<li>A typical parallel program involves getting a lock for accessing data structures and releasing it after the critical section.</li>
<li>Coherence action is taken on every access even if it’s not warranted until the lock is released.</li>
</ul>
<h3 id="release-consistencyrc"><a class="markdownIt-Anchor" href="#release-consistencyrc"></a> Release Consistency(RC)</h3>
<img src="https://i.imgur.com/1CQmKvJ.png" style="width: 800px" />
<ul>
<li>Release consistency is a memory consistency model that distinguishes between synchronization and data accesses.</li>
<li>Every critical section consists of acquire, data accesses governed by the lock, and release.</li>
<li>Coherence actions prior to the release operation by P1 have to be complete before P2 acquires the same lock.</li>
<li>Barrier synchronization can also be mapped to acquire and release.</li>
</ul>
<h4 id="rc-memory-model"><a class="markdownIt-Anchor" href="#rc-memory-model"></a> RC Memory Model</h4>
<img src="https://i.imgur.com/ym1Xrc4.png" style="width: 800px" />
<ul>
<li>It initiates coherence actions corresponding to normal data accesses but doesn’t block the processors.</li>
<li>It only takes coherence action when a release synchronization operation is encountered.</li>
</ul>
<h3 id="distributed-shared-memory-example"><a class="markdownIt-Anchor" href="#distributed-shared-memory-example"></a> Distributed Shared Memory Example</h3>
<img src="https://i.imgur.com/SVq87tU.png" style="width: 800px" />
<ul>
<li>RC memory model allows for parallel modifications to shared data structures</li>
<li>Program example: one thread modifies structure A while another waits and then uses it</li>
<li>P2 waits on a condition variable and releases a lock until P1 signals that the modification is done</li>
<li>Coherence actions are only completed before releasing the lock</li>
</ul>
<h3 id="advantage-of-rc-over-sc"><a class="markdownIt-Anchor" href="#advantage-of-rc-over-sc"></a> Advantage of RC over SC</h3>
<ul>
<li>RC memory model allows for overlapping of computation with communication</li>
<li>Better performance in shared memory machines compared to SC memory model</li>
</ul>
<h3 id="lazy-rc"><a class="markdownIt-Anchor" href="#lazy-rc"></a> Lazy RC</h3>
<img src="https://i.imgur.com/LXs0SJT.png" style="width: 800px" />
<ul>
<li>Lazy RC defers coherence actions to the point of lock acquisition rather than lock release</li>
<li>Overlapping of computation with communication is still possible in the window of time between lock release and lock acquisition</li>
</ul>
<h3 id="eager-vs-lazy-rc"><a class="markdownIt-Anchor" href="#eager-vs-lazy-rc"></a> Eager vs Lazy RC</h3>
<img src="https://i.imgur.com/lctRnu6.png" style="width: 800px" />
<ul>
<li>Vanilla RC is the eager release consistent memory model while LRC is the lazy release consistent memory model</li>
<li>Eager RC broadcasts changes to all processors while Lazy RC only communicates with the necessary processors through point-to-point communication</li>
</ul>
<h3 id="pros-and-cons-of-lazy-and-eager-release-consistency-model"><a class="markdownIt-Anchor" href="#pros-and-cons-of-lazy-and-eager-release-consistency-model"></a> Pros and Cons of Lazy and Eager Release Consistency Model</h3>
<ul>
<li>A system is release consistent if specific operations are performed before ordinary access or release is allowed to perform with respect to any other processor</li>
<li>Eager release consistency: A processor delays propagating its modification to shared data until it comes to a release.</li>
<li>Lazy release consistency: The propagation of modifications is postponed until the time of the acquire.</li>
<li>Lazy release consistency <strong>reduces the number of messages and data transferred</strong> between processors, which is especially <strong>significant for</strong> programs that <em>exhibit false sharing and make extensive use of locks</em>.</li>
</ul>
<h3 id="software-dsm"><a class="markdownIt-Anchor" href="#software-dsm"></a> Software DSM</h3>
<img src="https://i.imgur.com/vBEaWiw.png" style="width: 800px" />
<p>Software DSM is a way to implement the illusion of a global shared memory in a computational cluster where each node has its own private physical memory. The software has to implement the consistency model to the programmer as there is no physically shared memory. In a tightly coupled multiprocessor, coherence is maintained at individual memory access level by the hardware. However, in a cluster, this <strong>fine-grain</strong> of maintaining coherence at individual memory access level leads to too much <strong>overhead</strong>.</p>
<p>To implement software DSM, <strong>the granularity of coherence maintenance is at the level of a page</strong>. The global virtual memory abstraction is provided to the application program running on the cluster, which views the entire cluster as a globally shared virtual memory. Under the cover, the DSM software partition the global address space into chunks that are managed individually on the nodes of the different processors of the cluster.</p>
<p>The DSM software maintains coherence by having <strong>distributed ownership for the different virtual pages</strong> that constitute this global virtual address space. The ownership responsibility is split into individual processors, which are responsible for keeping complete coherence information for that particular page and taking the coherence actions commensurate with that page.</p>
<p>The DSM software implements the global virtual memory abstraction and knows exactly who to contact as the owner of the page to get the current copy of the page. When there is a page fault, the DSM software communicates with the operating system to handle it, contacts the owner of the page, and asks for the current copy of the page. The owner sends the page to the node that is requesting it, and the page is put into the physical memory, and the VM manager updates the page table for the thread to resume its execution.</p>
<p>An early example of systems that built software DSM includes Ivy, Clouds, Mirage, and Munin. They used coherence maintenance at the granularity of an individual page and a single-writer multiple-reader protocol. However, the single-writer multiple-reader protocol has the potential for false sharing, which is the problem of data appearing to be shared even though programmatically they are not. The page-based coherence maintenance and the single-writer multiple-reader protocol do not live happily together and can lead to false sharing and ping-ponging of the pages due to the false sharing among the threads of the application across the entire network.</p>
<h3 id="lrc-with-multi-writer-coherence-protocol"><a class="markdownIt-Anchor" href="#lrc-with-multi-writer-coherence-protocol"></a> LRC with Multi Writer Coherence Protocol</h3>
<img src="https://i.imgur.com/9ipNyv3.png" style="width: 800px" />
<ul>
<li>The goal of the <strong>multiple writer coherence protocol</strong> is to maintain coherence information at the granularity of pages, so the DSM can be integrated with the operating system.</li>
<li>Multiple writer coherence allows multiple writers to write to the same page, recognizing that an application programmer may have packed lots of different data structures within the same page.</li>
<li>The coherence protocol used in this system is LRC, which defers consistency traffic until the point of access.</li>
<li>When a processor acquires a lock and makes modifications, the DSM software creates a diff of the changes made to the pages within the critical section.</li>
<li>When another processor requests the same lock, the DSM software invalidates the pages modified by the previous lock holder, based on the diffs stored by the DSM software.</li>
<li>If a processor tries to access an invalidated page, the DSM software retrieves the original page and diffs from the previous lock holder and applies them to create the current version of the page for the new lock holder.</li>
<li>If multiple processors modify the same page, the DSM software applies all the diffs in order to create the current version of the page.</li>
<li>LRC allows the DSM software to bring in only the data that the new lock holder needs, deferring the retrieval of diffs until the point of access.</li>
<li>The DSM software can extend this protocol to any number of processors that may have made modifications to the same pages under the provision of the lock.</li>
</ul>
<h4 id="implementation"><a class="markdownIt-Anchor" href="#implementation"></a> Implementation</h4>
<img src="https://i.imgur.com/acchRwm.png" style="width: 800px" />
<ul>
<li>When a process or thread attempts to write to a page X, the operating system creates a twin for that page and makes the original page writeable by the process.</li>
<li>At the release point, the DSM software calculates the diff between the original and modified versions of the page, storing it in a diff data structure.</li>
<li>When a different processor acquires the same lock governing the released page, all pages touched in the previous critical section, including X, are invalidated. If there is a page fault for X, the DSM software retrieves the necessary diff from another node to update the page for the current lock acquirer.</li>
<li>After the thread in the critical section completes its release operation, the original page is write-protected, and the twin is deleted to free up the physical memory that was allocated for it.</li>
<li>In case multiple writers are modifying the same page under different locks, it represents an application problem and a data race.</li>
<li>The LRC multiple writer coherence protocol was implemented on a Unix system by TreadMarks.</li>
<li>When a thread accesses a shared page, the DSM software catches the SIGSEGV exception to take appropriate action.</li>
<li>There is a <strong>space overhead</strong> for creating a twin at the point of write and creating a diff data structure at release.</li>
<li><strong>Garbage collection is used to reduce the space overhead</strong> by periodically applying diffs to the original copy of the page and getting rid of them.</li>
</ul>
<h3 id="non-page-based-dsm"><a class="markdownIt-Anchor" href="#non-page-based-dsm"></a> Non Page Based DSM</h3>
<img src="https://i.imgur.com/tq2pE8U.png" style="width: 800px" />
<ul>
<li>The library-based approach uses a programming framework/library to annotate shared variables, causing a trap at the point of access to contact the DSM software, which can take coherence actions. This approach eliminates false sharing possible in page-based systems and single write or cache coherence protocols. Systems that use this approach include Shasta and Beehive.</li>
<li>The structured DSM approach provides abstractions at the level of meaningful application structures, using API calls to execute semantics and take coherence actions. Examples of systems that use this approach include Linda, Orca, Stampede, Stampede RT, and PTS.</li>
</ul>
<h3 id="scalability"><a class="markdownIt-Anchor" href="#scalability"></a> Scalability</h3>
<img src="https://i.imgur.com/EfXq8fF.png" style="width: 800px" />
<ul>
<li>DSM provides a programming model that looks and feels like a shared memory threads package, but performance does not necessarily scale up as we increase the number of processors in the cluster.</li>
<li>Overhead increases with the number of processors, and this buildup of overhead happens in true memory multi-processors.</li>
</ul>
<h3 id="dsm-and-speedup"><a class="markdownIt-Anchor" href="#dsm-and-speedup"></a> DSM and Speedup</h3>
<img src="https://i.imgur.com/qxvVk16.png" style="width: 800px" />
<ul>
<li>The computation to communication ratio must be high for any hope of speed up with DSM.</li>
<li>If sharing is too fine-grained, there is no hope of speed up, especially with DSM systems that are only an illusion of shared memory via software.</li>
<li>Pointer codes may result in increasing overhead for coherence maintenance in DSM in a local area network.</li>
</ul>
<h3 id="distributed-shared-memory-conclusion"><a class="markdownIt-Anchor" href="#distributed-shared-memory-conclusion"></a> Distributed Shared Memory Conclusion</h3>
<ul>
<li>DSM as originally envisioned, that is a threads package for a cluster, is dead.</li>
<li>Structured DSM, which provides higher-level data abstractions for sharing among threads, is attractive to reduce programming pain for developers of distributed applications on a cluster.</li>
</ul>

</div>


  <div class="book-comments">
    




  </div>



<script src="/js/book-post.js"></script>


<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js" integrity="sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

        </div>
      </div>
      <div class="column col-2 hide-lg">
        <div class="book-post-info">
  
    <div class="book-post-meta">
  
  <div class="divider"></div>
</div>

  

  <div class="book-tocbot">
</div>
<div class="book-tocbot-menu">
  <a class="book-toc-expand" onclick="expand_toc()">Expand all</a>
  <a onclick="go_top()">Back to top</a>
  <a onclick="go_bottom()">Go to bottom</a>
</div>


<script src="/js/book-toc.js"></script>

</div>

      </div>
    </div>
  </div>
  
  <a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>

</body>
</html>


<script src="/js/book.js"></script>
