<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">

    
      <link rel="icon" href="/favicon.png" />
    

    <title>
        
          calculus-two/week-5 - Eric&#39;s CS Notes
        
    </title>

    <!-- Spectre.css framework -->
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-exp.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-icons.min.css">

    <!-- theme css & js -->
    
<link rel="stylesheet" href="/css/book.css">

    
<script src="/js/book.js"></script>


    <!-- tocbot -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css">
    
    <!-- katex -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">

    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/zooming/2.1.1/zooming.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    const zooming = new Zooming()
    zooming.listen('.book-content img')
})
</script>

<meta name="generator" content="Hexo 4.2.0"></head>

<body>

<div class="book-container">
  <div class="book-sidebar">
    <div class="book-brand">
  <a href="/">
    <img src="/favicon.png">
    <span>ERIC&#39;S CS NOTES</span>
  </a>
</div>
    <div class="book-menu">
  <h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2>
<ul>
<li><a href="/symbols/index.html">Symbols of Mathematics</a></li>
<li><a href="/glossary/index.html">Glossary</a></li>
</ul>
<!--
## Stanford Statistical Learning

* [Chapter 2 - Overview of Statistical Learning](/statistical-learning/chapter-2/index.html)
* Chapter 3 - Linear Regression
* Chapter 4 - Classification
* Chapter 5 - Resampling Methods
* Chapter 6 - Linear Model Selection and Regularization
* Chapter 7 - Moving Beyond Linearity
* Chapter 8 - Tree-Based Methods
* Chapter 9 - Support Vector Machines
* Chapter 10 - Unsupervised Learning
-->
<h2 id="introduction-to-probability"><a class="markdownIt-Anchor" href="#introduction-to-probability"></a> Introduction to Probability</h2>
<ul>
<li><a href="/introduction-to-probability/unit-1/index.html">Unit 1: Probability models and axioms</a></li>
<li><a href="/introduction-to-probability/unit-2/index.html">Unit 2: Conditioning and independence</a></li>
<li><a href="/introduction-to-probability/unit-3/index.html">Unit 3: Counting</a></li>
</ul>
<!--
* [Unit 4: Discrete random variables](/introduction-to-probability/unit-4/index.html)
* [Unit 5: Continuous random variables](/introduction-to-probability/unit-5/index.html)
* [Unit 6: Further topics on random variables](/introduction-to-probability/unit-6/index.html)
* [Unit 7: Bayesian inference](/introduction-to-probability/unit-7/index.html)
* [Unit 8: Limit theorems and classical statistics](/introduction-to-probability/unit-8/index.html)
* [Unit 9: Bernoulli and Poisson processes](/introduction-to-probability/unit-9/index.html)
* [Unit 10: Markov chains](/introduction-to-probability/unit-10/index.html)
-->
<h2 id="multivariable-calculus"><a class="markdownIt-Anchor" href="#multivariable-calculus"></a> Multivariable Calculus</h2>
<ul>
<li><a href="/multivariable-calculus/unit-1/index.html">Unit 1: Thinking about multivariable functions</a></li>
<li><a href="/multivariable-calculus/unit-2/index.html">Unit 2: Derivatives of multivariable functions</a></li>
</ul>
<!--
* [Unit 3: Applications of multivariable derivatives](/multivariable-calculus/unit-3/index.html)
* [Unit 4: Integrating multivariable functions](/multivariable-calculus/unit-4/index.html)
* [Unit 5: Green's, Stokes', and the divergence theorems](/multivariable-calculus/unit-5/index.html)
-->
<h2 id="algorithms-part-ii"><a class="markdownIt-Anchor" href="#algorithms-part-ii"></a> Algorithms: Part II</h2>
<ul>
<li><a href="/algorithms-2/week-1/index.html">Week 1 - Undirected Graph &amp; Directed Graph</a></li>
<li><a href="/algorithms-2/week-3/index.html">Week 3 - Maximum Flow and Minimum Cut &amp; Radix Sort</a></li>
<li><a href="/algorithms-2/week-4/index.html">Week 4 - Tries &amp; Substring Search</a></li>
</ul>
<h2 id="algorithms-part-i"><a class="markdownIt-Anchor" href="#algorithms-part-i"></a> Algorithms: Part I</h2>
<ul>
<li><a href="/algorithms-1/week-1/index.html">Week 1 - Union-Find &amp; Analysis of Algorithms</a></li>
<li><a href="/algorithms-1/week-2/index.html">Week 2 - Stacks and Queues &amp; Elementary Sorts</a></li>
<li><a href="/algorithms-1/week-3/index.html">Week 3 - Mergesort &amp; Quicksort</a></li>
<li><a href="/algorithms-1/week-4/index.html">Week 4 - Priority Queues &amp; Elementary Symbols</a></li>
<li><a href="/algorithms-1/week-5/index.html">Week 5 - Balanced Search Trees</a></li>
<li><a href="/algorithms-1/week-6/index.html">Week 6 - Hash Tables</a></li>
</ul>
<h2 id="introduction-to-software-design-and-architecture"><a class="markdownIt-Anchor" href="#introduction-to-software-design-and-architecture"></a> Introduction to Software Design and Architecture</h2>
<ul>
<li><a href="/introduction-to-software-design-and-architecture/design-pattern/index.html">Design Pattern</a></li>
</ul>
<h2 id="calculus-two-sequences-and-series"><a class="markdownIt-Anchor" href="#calculus-two-sequences-and-series"></a> Calculus Two: Sequences and Series</h2>
<ul>
<li><a href="/calculus-two/week-1/index.html">Week 1 - Sequences</a></li>
<li><a href="/calculus-two/week-2/index.html">Week 2 - Series</a></li>
<li><a href="/calculus-two/week-3/index.html">Week 3 - Convergence Tests</a></li>
<li><a href="/calculus-two/week-4/index.html">Week 4 - Alternating Series</a></li>
<li><a href="/calculus-two/week-5/index.html">Week 5 - Power Series</a></li>
<li><a href="/calculus-two/week-6/index.html">Week 6 - Taylor Series</a></li>
</ul>
<h2 id="laff-linear-algebra"><a class="markdownIt-Anchor" href="#laff-linear-algebra"></a> LAFF Linear Algebra</h2>
<ul>
<li><a href="/laff-linear-algebra/week-1/index.html">Week 1 - Vectors in Linear Algebra</a></li>
<li><a href="/laff-linear-algebra/week-2/index.html">Week 2 - Linear Transformations and Matrices</a></li>
<li><a href="/laff-linear-algebra/week-3/index.html">Week 3 - Matrix-Vector Operations</a></li>
<li><a href="/laff-linear-algebra/week-4/index.html">Week 4 - Matrix-Vector to Matrix-Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-5/index.html">Week 5 - Matrix- Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-6/index.html">Week 6 - Gaussian Elimination</a></li>
<li><a href="/laff-linear-algebra/week-7/index.html">Week 7 - More Gaussian Elimination and Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-8/index.html">Week 8 - More on Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-9/index.html">Week 9 - Vector Spaces</a></li>
<li><a href="/laff-linear-algebra/week-10/index.html">Week 10 - Vector Spaces, Orthogonality, and Linear Least-Squares</a></li>
<li><a href="/laff-linear-algebra/week-11/index.html">Week 11 - Orthogonal Projection, Low Rank Approximation, and Orthogonal Bases</a></li>
<li><a href="/laff-linear-algebra/week-12/index.html">Week 12 - Eigenvalues and Eigenvectors</a></li>
</ul>
<h2 id="stanford-machine-learning"><a class="markdownIt-Anchor" href="#stanford-machine-learning"></a> Stanford Machine Learning</h2>
<ul>
<li><a href="/stanford-machine-learning/week-1/index.html">Week 1 - Introduction</a></li>
<li><a href="/stanford-machine-learning/week-2/index.html">Week 2 - Linear Regression with Multiple Variables</a></li>
<li><a href="/stanford-machine-learning/week-3/index.html">Week 3 - Logistic Regression &amp; Regularization</a></li>
<li><a href="/stanford-machine-learning/week-4/index.html">Week 4 - Neural Networks: Representation</a></li>
<li><a href="/stanford-machine-learning/week-5/index.html">Week 5 - Neural Networks: Learning</a></li>
<li><a href="/stanford-machine-learning/week-6a/index.html">Week 6a - Advice for Applying Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-6b/index.html">Week 6b - Machine Learning System Design</a></li>
<li><a href="/stanford-machine-learning/week-7/index.html">Week 7 - Support Vector Machines</a></li>
<li><a href="/stanford-machine-learning/week-8/index.html">Week 8 - Unsupervised Learning &amp; Dimensionality Reduction</a></li>
<li><a href="/stanford-machine-learning/week-9a/index.html">Week 9a - Anomaly Detection</a></li>
<li><a href="/stanford-machine-learning/week-9b/index.html">Week 9b - Recommender Systems</a></li>
<li><a href="/stanford-machine-learning/week-10/index.html">Week 10 - Large Scale Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-11/index.html">Week 11 - Application Example: Photo OCR</a></li>
</ul>
<h2 id="calculus-one"><a class="markdownIt-Anchor" href="#calculus-one"></a> Calculus One</h2>
<ul>
<li><a href="/calculus-one/week-2-3/index.html">Week 2-3 - Functions &amp; Limits</a></li>
<li><a href="/calculus-one/week-4/index.html">Week 4 - The Beginning of Derivatives</a></li>
<li><a href="/calculus-one/week-5/index.html">Week 5 - Techniques of Differentiation</a></li>
<li><a href="/calculus-one/week-6/index.html">Week 6 - Chain Rule</a></li>
<li><a href="/calculus-one/week-7/index.html">Week 7 - Derivatives of Trigonometric Functions</a></li>
<li><a href="/calculus-one/week-8/index.html">Week 8 - Derivatives in the Real World</a></li>
<li><a href="/calculus-one/week-9/index.html">Week 9 - Optimization</a></li>
<li><a href="/calculus-one/week-10/index.html">Week 10 - Linear Approximation</a></li>
<li><a href="/calculus-one/week-11-12/index.html">Week 11-12 - Antidifferentiation &amp; Integration</a></li>
<li><a href="/calculus-one/week-13/index.html">Week 13 - Fundamental Theorem of Calculus</a></li>
<li><a href="/calculus-one/week-14/index.html">Week 14 - Substitution Rule</a></li>
<li><a href="/calculus-one/week-15/index.html">Week 15 - Techniques of Integration</a></li>
<li><a href="/calculus-one/week-16/index.html">Week 16 - Applications of Integration</a></li>
</ul>
<h2 id="computational-thinking"><a class="markdownIt-Anchor" href="#computational-thinking"></a> Computational Thinking</h2>
<ul>
<li><a href="/computational-thinking/lecture-1/index.html">Lecture 1 - Optimization and Knapsack Problem</a></li>
<li><a href="/computational-thinking/lecture-2/index.html">Lecture 2 - Decision Trees and Dynamic Programming</a>
<ul>
<li><a href="/computational-thinking/lecture-2-powerset/index.html">Exercise: Power Set Function</a></li>
</ul>
</li>
<li><a href="/computational-thinking/lecture-3/index.html">Lecture 3 - Graphs</a></li>
<li><a href="/computational-thinking/lecture-4-5/index.html">Lecture 4-5 - Plotting</a></li>
<li><a href="/computational-thinking/lecture-6-7/index.html">Lecture 6-7 - Stochastic Programs &amp; Inferential Statistics</a></li>
<li><a href="/computational-thinking/lecture-8/index.html">Lecture 8 - Monte Carlo Simulation</a></li>
<li><a href="/computational-thinking/lecture-9/index.html">Lecture 9 - Sampling and Standard Error</a></li>
<li><a href="/computational-thinking/lecture-10-11/index.html">Lecture 10-11 - Experimental Data</a></li>
<li><a href="/computational-thinking/lecture-12/index.html">Lecture 12 - Machine Learning</a></li>
<li><a href="/computational-thinking/lecture-13/index.html">Lecture 13 - Statistical Abuses</a></li>
</ul>
<h2 id="effective-thinking-through-mathematics"><a class="markdownIt-Anchor" href="#effective-thinking-through-mathematics"></a> Effective Thinking Through Mathematics</h2>
<ul>
<li><a href="/effective-thinking-through-mathematics/note/index.html">Note</a></li>
<li><a href="/effective-thinking-through-mathematics/week-4-telling-the-story-of-infinity/index.html">Week 4 (/Telling the Story of Infinity)</a></li>
<li><a href="/effective-thinking-through-mathematics/week-5-telling-the-story-of-the-euler-circuit-theorem/index.html">Week 5 (/Telling the Story of Euler Circuit Theorem)</a></li>
</ul>
<h2 id="cs50-introduction-to-computer-science"><a class="markdownIt-Anchor" href="#cs50-introduction-to-computer-science"></a> CS50 Introduction to Computer Science</h2>
<ul>
<li><a href="/cs50/week-1/index.html">Week 1 - C</a></li>
<li><a href="/cs50/week-2/index.html">Week 2 - Arrays</a></li>
<li><a href="/cs50/week-3/index.html">Week 3 - Algorithms</a></li>
<li><a href="/cs50/week-4/index.html">Week 4 - Memory</a></li>
<li><a href="/cs50/week-5/index.html">Week 5 - Data Structures</a></li>
<li><a href="/cs50/week-6/index.html">Week 6 - HTTP</a></li>
<li><a href="/cs50/week-7-10/index.html">Week 7-10 - Machine Learning/Python/SQL/Javascript</a></li>
</ul>

</div>


<script src="/js/book-menu.js"></script>

  </div>

  <div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseover="add_inner()" onmouseleave="remove_inner()">
  <div class="sidebar-toggle-inner"></div>
</div>

<script>
function add_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.add('show')  
}

function remove_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.remove('show')
}

function sidebar_toggle() {
    let sidebar_toggle = document.querySelector('.sidebar-toggle')
    let sidebar = document.querySelector('.book-sidebar')
    let content = document.querySelector('.off-canvas-content')
    if (sidebar_toggle.classList.contains('extend')) { // show
        sidebar_toggle.classList.remove('extend')
        sidebar.classList.remove('hide')
        content.classList.remove('extend')
    }
    else { // hide
        sidebar_toggle.classList.add('extend')
        sidebar.classList.add('hide')
        content.classList.add('extend')
    }
}
</script>

  <div class="off-canvas-content">
    <div class="columns">
      <div class="column col-10 col-lg-12">
        <div class="book-navbar">
          <!-- For Responsive Layout -->

<header class="navbar">
  <section class="navbar-section">
    <a onclick="open_sidebar()">
      <i class="icon icon-menu"></i>
    </a>
  </section>
</header>

        </div>
        <div class="book-content">
          <div class="book-post">
  <h1 id="week-5-power-series"><a class="markdownIt-Anchor" href="#week-5-power-series"></a> Week 5 - Power Series</h1>
<h2 id="definition"><a class="markdownIt-Anchor" href="#definition"></a> Definition</h2>
<ul>
<li>A <strong>power series</strong> is a series of the form \[\sum_{n=0}^{\infty}c_n x^n = c_0 + c_1 x + c_2 x^2 + c_3 x^3 + \cdots\] where <strong>x</strong> is a variable and the \(c_n\)’s are constants called the <strong>coefficients</strong> of the series.</li>
<li>The sum of the series is a function \[f(x) = c_0 + c_1 x + c_2 x^2 + \cdots + c_n x^n + \cdots \] whose domain is the set of all x for which the series converges. Notice that \(f\) resembles a polynomial.</li>
</ul>
<h2 id="convergence-of-power-series"><a class="markdownIt-Anchor" href="#convergence-of-power-series"></a> Convergence of Power Series</h2>
<h3 id="interval-of-convergence"><a class="markdownIt-Anchor" href="#interval-of-convergence"></a> Interval of convergence</h3>
<ul>
<li>\[C = { x \in \mathbb{R} | \sum_{n = 0}^{\infty} a_n x^n\ \text{converge}}\]</li>
<li><strong>C</strong> is called the <strong>interval of convergence</strong>.</li>
</ul>
<h3 id="theorem"><a class="markdownIt-Anchor" href="#theorem"></a> Theorem</h3>
<ul>
<li>
<p>Suppose \(\displaystyle \sum_{n = 0}^{\infty} a_n x^n\) converge when \(x = x_0\), then the series converge absolutely x between \(-x_0\) and \(x_0\).</p>
</li>
<li>
<p>Prove:</p>
<ul>
<li>Suppose \(\displaystyle \sum_{n = 0}^{\infty} a_n x^n\) converge when \(x = x_0\)</li>
<li>So, \(\displaystyle \lim_{n \to \infty} a_n x_0^n = 0\),</li>
<li>There is an <strong>M</strong>, so that for all <strong>n</strong>, \(|a_n x_0^n| \le M\),</li>
<li>Pick \(x \in (-|x_0|, |x_0|)\),</li>
<li>\(|a_n x^n| = |a_n x_0^n| \cdot |\frac{x<sup>n}{x_0</sup>n}| \le M \cdot |\frac{x<sup>n}{x_0</sup>n}|\)</li>
<li>With ratio test, \(r = |\frac{x<sup>n}{x_0</sup>n}| &lt; 1\), we can get \(\sum_{n = 0}^{\infty} M \cdot r^n\) converge,</li>
<li>So \(\sum_{n=0}^{\infty} |a_n x^n|\) converge.</li>
</ul>
</li>
<li>
<p>Corollary:</p>
<ul>
<li>Consider \(\sum_{n=0}^{\infty} a_n x^n\).</li>
<li>There is an <strong>R</strong>, so that the series,
<ul>
<li>converges absolutely for \(x \in (-R, R)\),</li>
<li>diverges for \(x &gt; R \text{ or } x &lt; -R\)</li>
</ul>
</li>
<li>The number <strong>R</strong> is called the <strong>radius of convergence</strong> of the power series.</li>
</ul>
</li>
</ul>
<h3 id="example"><a class="markdownIt-Anchor" href="#example"></a> Example</h3>
<ul>
<li>What’s the interval of convergence of the series \(\displaystyle \sum_{n=1}<sup>{\infty}\frac{x</sup>n}{n}\)</li>
<li>First, we ask the easier question. What’s its radius of convergence?
<ul>
<li>Think this series converge absolutely, and use ratio test:
<ul>
<li>\(\displaystyle\lim_{n \to \infty}|\frac{x^{n+1}/{n+1} }{x^n/n}| = |x|\)</li>
<li>\(|x|&lt;1\), the series converges,</li>
<li>\(|x|&gt;1\), the series diverges.</li>
</ul>
</li>
<li>So the radius of convergence is <strong>1</strong>.</li>
</ul>
</li>
<li>What about the endpoints <strong>1</strong> and <strong>-1</strong>?
<ul>
<li>if x = 1, then the series = \(\sum_{n=1}^{\infty} \frac{1}{n}\), which is harmonic series, also diverges.</li>
<li>if x = -1, then the series = \(\sum_{n=1}^{\infty} \frac{(-1)^n}{n}\), that’s the alternating harmonic series, and converges.</li>
</ul>
</li>
<li>To summarize this:
<ul>
<li>The interval of convergence of series \(\displaystyle \sum_{n=1}<sup>{\infty}\frac{x</sup>n}{n}\) is \([-1, 1)\)</li>
</ul>
</li>
</ul>
<h2 id="differentiation-and-integration-of-power-series"><a class="markdownIt-Anchor" href="#differentiation-and-integration-of-power-series"></a> Differentiation and integration of power Series</h2>
<h3 id="power-series-centered-around-a"><a class="markdownIt-Anchor" href="#power-series-centered-around-a"></a> Power Series Centered Around a</h3>
<ul>
<li>\(\displaystyle\sum_{n=0}^{\infty}c_n (x - a)^n\)</li>
<li>Use ratio test:
<ul>
<li>\(\displaystyle \lim_{n \to \infty} |\frac{c_{n+1} \cdot (x - a)^{n+1} }{c_n \cdot (x - a)^n}| = \lim_{n \to \infty} |\frac{c_{n+1} }{c_n}| \cdot |x - a|\)</li>
<li>To get the interval of convergence, let’s set \(|\frac{c_{n+1} }{c_n}| = \frac{1}{R}\)
<ul>
<li>\(\frac{1}{R} \cdot |x - a| &lt; 1\)</li>
<li>\(a - R &lt; x &lt; a + R\)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="differentiate-a-power-series"><a class="markdownIt-Anchor" href="#differentiate-a-power-series"></a> Differentiate a Power Series</h3>
<ul>
<li>Theorem:
<ul>
<li>\(\displaystyle f(x) = \sum_{n=0}^{\infty} a_n x^n\), <strong>R</strong> = radius of convergence.</li>
<li>Then \(\displaystyle f’(x) = \sum_{n=1}^{\infty} n \cdot a_n \cdot x^{n-1} \text{ for } x \in (-R, R)\)
<ul>
<li>Notice the index of <strong>n</strong> start from 1, because when \(n = 0, f’(x) = 0\)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="integrate-a-power-series"><a class="markdownIt-Anchor" href="#integrate-a-power-series"></a> Integrate a Power Series</h3>
<ul>
<li>Theorem:
<ul>
<li>\(\displaystyle f(x) = \sum_{n=0}^{\infty} a_n x^n\), <strong>R</strong> = radius of convergence.</li>
<li>Then \(\displaystyle \int_0^t f(x) dx = \sum_{n=1}^{\infty} \frac{a_n \cdot t^{n+1} }{n+1} \text{ for } x \in (-R, R)\)</li>
</ul>
</li>
<li>Example: \(\displaystyle \int_{x=0}^{t} \sum_{n=0}^{\infty} x^n dx\)
<ul>
<li>First to prove: \(\displaystyle \sum_{n=0}^{\infty} x^n = \frac{1}{1-x}\), \(|x| &lt; 1\) =&gt; \(R = 1\)
<ul>
<li>Use Geometric Series theorem, we know \(\displaystyle \sum_{n=1}^{\infty} x^n = \sum_{n=1}^{\infty} x \cdot x^{n-1} = \frac{x}{1-x}\)</li>
<li>So \(\displaystyle \sum_{n=0}^{\infty} x^n = \frac{x}{1-x} + x^0 = \frac{1}{1-x}\)</li>
</ul>
</li>
<li>\(\displaystyle \int_{x = 0}^t \frac{1}{1-x} dx\), use substitution rule, set \(u = 1 - x, du = -dx\)</li>
<li>\(\displaystyle \int_{x = 0}^t \frac{1}{1-x} dx = - \int_{x = 0}^t \frac{du}{u} \)</li>
<li>\(= -\log|u| \rbrack_{x=0}^t = - \log|1-x|\rbrack_{x=0}^t = - \log |1 - t|\)</li>
<li>In another way, we can backwards the derivation:
<ul>
<li>\(\displaystyle - \log |1 - t| = \int_{x=0}^t \frac{1}{1-x} dx\)</li>
<li>\(\displaystyle = \int_{x=0}^{t} \sum_{n=0}^{\infty} x^n dx = \sum_{n=0}^{\infty} \int_{x=0}^{t} x^n dx\)</li>
<li>\(\displaystyle = \sum_{n=0}^{\infty} \frac{t^{n+1} }{n+1}, |t| &lt; 1\)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="ex"><a class="markdownIt-Anchor" href="#ex"></a> e^x</h3>
<ul>
<li>To prove \(\displaystyle \sum_{n=0}^{\infty} \frac{x^n}{n!} = e^x\)
<ul>
<li>It’s the sum, n goes from 0 to infinity of x to the n over n factorial</li>
</ul>
</li>
<li>\(\displaystyle f(x) = \sum_{n=0}^{\infty} \frac{x^n}{n!}\)</li>
<li>prove \(f(0) = e^0 = 0\):
<ul>
<li>\(\displaystyle f(0) = \sum_{n=0}^{\infty} \frac{0^n}{n!} = 1\)
<ul>
<li>\(0^0 = 1, 0^1 = 0, \ldots, 0^n = 0\)</li>
<li>\(0! = 1\). because: \((n+1)! = (n+1) \cdot n!\)</li>
</ul>
</li>
<li>\(e^0 = 1\)</li>
</ul>
</li>
<li>after differentiation, both functions still themselves:
<ul>
<li>\(\displaystyle \frac{d}{dx} \sum_{n=0}^{\infty} \frac{x^n}{n!} = \sum_{n=0}^{\infty} \frac{x^{n-1} }{(n-1)!} = \sum_{n=0}^{\infty} \frac{x^n}{n!} \)</li>
<li>\(\frac{d}{dx}e^x = e^x\)</li>
</ul>
</li>
<li>These two star-crossed functions agree at a single point, and they’re changing in the same way, and consequently, they must be the same function.</li>
</ul>
<h3 id="multiply-two-power-series"><a class="markdownIt-Anchor" href="#multiply-two-power-series"></a> Multiply Two Power Series</h3>
<ul>
<li>\(\displaystyle (\sum_{n=0}^{\infty} a_n x^n) \cdot (\sum_{n=0}^{\infty} b_n x^n) \)</li>
<li>\(=(a_0 + a_1 x + a_2 x^2 + \cdots) \cdot (b_0 + b_1 x + b_2 x^2 + \cdots)\)</li>
<li>\(= a_0 b_0 + (a_1 + b_1) x + (a_0 b_2 + a_1 b_1 + a_2 b_0) x^2 + \cdots\)</li>
<li>\(\displaystyle (\sum_{n=0}^{\infty} a_n x^n) \cdot (\sum_{n=0}^{\infty} b_n x^n) = \sum_{n=0}^{\infty} (\sum_{i=0}^{n} a_i b_{n-i}) x^n\)</li>
</ul>
<h4 id="theorem-2"><a class="markdownIt-Anchor" href="#theorem-2"></a> Theorem</h4>
<ul>
<li>
<p>\(f(x) = \displaystyle (\sum_{n=0}^{\infty} a_n x^n)\), \(g(x) = \displaystyle (\sum_{n=0}^{\infty} b_n x^n)\), and their radius of convergence \(\ge \mathbb{R}\)</p>
</li>
<li>
<p>Then we can get: \(\displaystyle f(x) g(x) = \sum_{n=0}^{\infty} (\sum_{i=0}^{n} a_i b_{n-i}) x^n\), for \(x \in (-R, R)\)</p>
</li>
<li>
<p>Example:</p>
<ul>
<li>\(\displaystyle e^x = \sum_{n=0}^{\infty} \frac{x^n}{n!} = 1 + x + \frac{x^2}{2} + \frac{x^3}{6} + \cdots \)</li>
<li>\((e<sup>x)</sup>2 = e^{(2x)}\)</li>
<li>\(e^{(2x)} = 1 + 2x + 2x^2 + \frac{8x^3}{6} + \cdots \)</li>
<li>\((e<sup>x)</sup>2 = (1 + x + \frac{x^2}{2} + \frac{x^3}{6} + \cdots)^2 \)
<ul>
<li>\(= 1 + 2x + (\frac{1}{2} + 1 + \frac{1}{2}) x^2 + (\frac{1}{6} + \frac{1}{2} + \frac{1}{2} + \frac{1}{6}) x^3 + \cdots\)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="fibonacci-numbers"><a class="markdownIt-Anchor" href="#fibonacci-numbers"></a> Fibonacci Numbers</h2>
<h3 id="transform-11-x"><a class="markdownIt-Anchor" href="#transform-11-x"></a> Transform 1/(1-x)</h3>
<ul>
<li>\(\displaystyle \sum_{n=0}^{\infty} x^n = \frac{1}{1-x}\)</li>
<li>Two ways to transfer \(\frac{1}{(1-x)^2}\)
<ul>
<li>First:
<ul>
<li>\(\displaystyle \frac{1}{(1-x)^2} = \frac{d}{dx} (\frac{1}{1-x}) = \frac{d}{dx} \sum_{n=0}^{\infty} x^n = \sum_{n=0}^{\infty} \frac{d}{dx}(x^n) = \sum_{n=1}^{\infty} n \cdot x^{n-1}\)</li>
<li>\(= 1 \cdot x^0 + 2 \cdot x^1 + 3 \cdot x^2 + \cdots \)</li>
<li>\(\displaystyle = \sum_{n=0}^{\infty} (n+1) \cdot x^n\)</li>
</ul>
</li>
<li>Second:
<ul>
<li>\(\displaystyle (\sum_{n=0}^{\infty} x<sup>n)</sup>2 = (1+x+x<sup>2+x</sup>3+\cdots)(1+x+x<sup>2+x</sup>3+\cdots) \)</li>
<li>\(= 1 + 2x + 3x^2 + 4x^3 + \cdots\)</li>
<li>\(\displaystyle = \sum_{n=0}^{\infty} (n+1) \cdot x^n\)</li>
<li>Or we can use the theorem of <strong>Multiply Two Power Series</strong>
<ul>
<li>\(\displaystyle \sum_{n=0}^{\infty} (\sum_{i=0}^{n} a_i b_{n-i}) x^n\)</li>
<li>In this case, \(a_i = b_{n-i} = 1\),</li>
<li>So  \(\displaystyle = \sum_{n=0}^{\infty} (\sum_{i=0}^{n} 1) x^n = \sum_{n=0}^{\infty} (n+1) \cdot x^n\)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="a-formula-for-the-fibonacci-numbers"><a class="markdownIt-Anchor" href="#a-formula-for-the-fibonacci-numbers"></a> A Formula for the Fibonacci Numbers</h3>
<ul>
<li>Let’s say, \({a_n}\) is Fibonacci Sequence, and \(f(x) = \displaystyle \sum_{n=0}^{\infty} a_n x^n\)
<ul>
<li>\( = x + x^2 + 2x^3 + 3x^4 + 5x^5 + \cdots\)</li>
</ul>
</li>
<li>Then:
<ul>
<li>\( x \cdot f(x) = x^2 + x^3 + 2x^4 + 3x^5 + 5x^6 + \cdots\)</li>
<li>\( x^2 \cdot f(x) = x^3 + x^4 + 2x^5 + 3x^6 + 5x^7 + \cdots\)</li>
</ul>
</li>
<li>Conclude above equations, we get:
<ul>
<li>\(f(x) - x \cdot f(x) - x^2 \cdot f(x) = x\)</li>
</ul>
</li>
<li>So \(\displaystyle f(x) = \frac{x}{1-x-x^2}\)</li>
<li>set \(\displaystyle \phi = \frac{1+\sqrt{5} }{2}\), after some calculation we get:</li>
<li>\[\begin{aligned}<br />
f(x) &amp;= \frac{1/{\sqrt{5} } }{1-(x \cdot \phi)} + \frac{-1/{\sqrt{5} } }{1-(x \cdot (1-\phi))} \<br />
&amp;= \frac{1}{\sqrt{5} } \cdot \frac{1}{1-(x \cdot \phi)} + \frac{-1}{\sqrt{5} } \cdot \frac{1}{1-(x \cdot (1-\phi))} \<br />
&amp;= \frac{1}{\sqrt{5} } \sum_{n=0}^{\infty} (x \cdot \phi)^n + \frac{-1}{\sqrt{5} } \sum_{n=0}^{\infty} (x \cdot (1 - \phi))^n \<br />
&amp;= \sum_{n=0}^{\infty} (\frac{1}{\sqrt{ 5 } } \cdot \phi^{n} - \frac{1}{\sqrt{ 5 } } \cdot (1 - \phi)^{n}) x^n \<br />
&amp;= \sum_{n=0}^{\infty} \frac{\phi^n - (1 - \phi)^n}{\sqrt{5} } x^n = \sum_{n=0}^{\infty} a_n x^n<br />
\end{aligned}\]</li>
<li>So, \(\displaystyle a_n = \frac{\phi^n - (1 - \phi)^n}{\sqrt{5} } \)</li>
</ul>

</div>


  <div class="book-comments">
    




  </div>



<script src="/js/book-post.js"></script>

        </div>
      </div>
      <div class="column col-2 hide-lg">
        <div class="book-post-info">
  
    <div class="book-post-meta">

  <div class="author">

    <!-- Author image -->
    <div class="author-img">
      
        <figure
          class="avatar avatar-lg"
          data-initial="E"
          style="background-color: #3b4351;">
        </figure>
      
    </div>

    <!-- Author title -->
    <div class="author-title">
      <div>Eric Yang</div>
      <div>2020-04-13</div>
    </div>
  </div>

  

  <div class="divider"></div>
</div>
  

  <div class="book-tocbot">
</div>
<div class="book-tocbot-menu">
  <a class="book-toc-expand" onclick="expand_toc()">Expand all</a>
  <a onclick="go_top()">Back to top</a>
  <a onclick="go_bottom()">Go to bottom</a>
</div>


<script src="/js/book-toc.js"></script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "\\(", right: "\\)", display: false},
                {left: "\\[", right: "\\]", display: true}
            ]
        });
    });
</script>

</div>

      </div>
    </div>
  </div>
  
  <a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>

</body>
</html>


<script src="/js/book.js"></script>
