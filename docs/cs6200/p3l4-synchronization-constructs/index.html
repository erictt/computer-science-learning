<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">

    
      <link rel="icon" href="/favicon.png" />
    

    <title>
        
          Eric&#39;s CS Notes
        
    </title>

    <!-- Spectre.css framework -->
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-exp.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-icons.min.css">

    <!-- theme css & js -->
    
<link rel="stylesheet" href="/css/book.css">

    
<script src="/js/book.js"></script>


    <!-- tocbot -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css">
    
    <!-- katex -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/zooming/2.1.1/zooming.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    const zooming = new Zooming()
    zooming.listen('.book-content img')
})
</script>

<meta name="generator" content="Hexo 6.3.0"></head>


<body>

<div class="book-container">
  <div class="book-sidebar">
    <div class="book-brand">
  <a href="/">
    <img src="/favicon.png">
    <span>ERIC&#39;S CS NOTES</span>
  </a>
</div>

    <div class="book-menu">
  <!--
## Introduction to Probability

* [Unit 1: Probability models and axioms](/introduction-to-probability/unit-1/index.html)
* [Unit 2: Conditioning and independence](/introduction-to-probability/unit-2/index.html)
* [Unit 3: Counting](/introduction-to-probability/unit-3/index.html)
* [Unit 4: Discrete random variables](/introduction-to-probability/unit-4/index.html)
* [Unit 5: Continuous random variables](/introduction-to-probability/unit-5/index.html)
* [Unit 6: Further topics on random variables](/introduction-to-probability/unit-6/index.html)
* [Unit 7: Bayesian inference](/introduction-to-probability/unit-7/index.html)
* [Unit 8: Limit theorems and classical statistics](/introduction-to-probability/unit-8/index.html)
* [Unit 9: Bernoulli and Poisson processes](/introduction-to-probability/unit-9/index.html)
* [Unit 10: Markov chains](/introduction-to-probability/unit-10/index.html)
-->
<!--
## Multivariable Calculus

* [Unit 1: Thinking about multivariable functions](/multivariable-calculus/unit-1/index.html)
* [Unit 2: Derivatives of multivariable functions](/multivariable-calculus/unit-2/index.html)

* [Unit 3: Applications of multivariable derivatives](/multivariable-calculus/unit-3/index.html)
* [Unit 4: Integrating multivariable functions](/multivariable-calculus/unit-4/index.html)
* [Unit 5: Green's, Stokes', and the divergence theorems](/multivariable-calculus/unit-5/index.html)
-->
<h2 id="cs6250-computer-networks"><a class="markdownIt-Anchor" href="#cs6250-computer-networks"></a> CS6250 Computer Networks</h2>
<ul>
<li><a href="/cs6250/week-1-internet-architecture/index.html">Week 1 - Internet Architecture</a></li>
<li><a href="/cs6250/week-2-transport-and-application-layers/index.html">Week 2 - Transport and Application Layers</a></li>
<li><a href="/cs6250/week-3-intradomain-routing/index.html">Week 3 - Intradomain Routing</a></li>
<li><a href="/cs6250/week-4-as-relationships-and-interdomain-routing/index.html">Week 4 - AS Relationships and Interdomain Routing</a></li>
<li><a href="/cs6250/week-5-router-design-and-algorithems-part-1/index.html">Week 5 - Router Design and Algorithms (Part 1)</a></li>
<li><a href="/cs6250/week-6-router-design-and-algorithems-part-2/index.html">Week 6 - Router Design and Algorithms (Part 2)</a></li>
<li><a href="/cs6250/week-7-software-defined-networking-part-1/index.html">Week 7 - Software Defined Networking (Part 1)</a></li>
</ul>
<!--
* [Week 8 - Software Defined Networking (Part 2)](/cs6250/week-8-software-defined-networking-part-2/index.html)
* [Week 9 - Internet Security](/cs6250/week-9-internet-security/index.html)
* [Week 10 - Internet Surveillance and Censorship](/cs6250/week-10-internet-surveillance-and-censorship/index.html)
* [Week 11 - Applications Videos](/cs6250/week-11-applications-video/index.html)
* [Week 12 - Applications CDNs and Overlay Networks](/cs6250/week-12-applications-cdns-and-overlay-networks/index.html)
-->
<h2 id="cs6200-graduate-introduction-to-operating-systems"><a class="markdownIt-Anchor" href="#cs6200-graduate-introduction-to-operating-systems"></a> CS6200 Graduate Introduction to Operating Systems</h2>
<ul>
<li><a href="/cs6200/p1-preparation/index.html">P0 - Preparation</a></li>
<li><a href="/cs6200/p1l2-introduction/index.html">P1L2 - Introduction</a></li>
<li><a href="/cs6200/p2l1-processes-and-process-management/index.html">P2L1 - Processes and Process Management</a></li>
<li><a href="/cs6200/p2l2-threads-and-concurrency/index.html">P2L2 - Threads and Concurrency</a></li>
<li><a href="/cs6200/p2l3-pthread/index.html">P2L3 - PThread</a></li>
<li><a href="/cs6200/p2l4-thread-design-consideration/index.html">P2L4 - Thread Design Considerations</a></li>
<li><a href="/cs6200/p2l5-thread-performance-consideration/index.html">P2L5 - Thread Performance Considerations</a></li>
<li><a href="/cs6200/p3l1-scheduling/index.html">P3L1 - Scheduling</a></li>
<li><a href="/cs6200/p3l2-memory-management/index.html">P3L2 - Memory Management</a></li>
<li><a href="/cs6200/p3l3-inter-process-communication/index.html">P3L3 - Inter-Process Communication</a></li>
<li><a href="/cs6200/p3l4-synchronization-constructs/index.html">P3L4 - Synchronization Constructs</a></li>
</ul>
<!--
* [P3L5 - I/O Management](/cs6200/p3l5-io-management/index.html) 
* [P3L6 - Virtualization](/cs6200/p3l6-virtualization/index.html) 
* [P4L1 - Remote Procedure Calls](/cs6200/p4l1-remote-procedure-calls/index.html) 
* [P4L2 - Distributed File Systems](/cs6200/p4l2-distributed-file-systems/index.html) 
* [P4L3 - Distributed Shared Memory](/cs6200/p4l3-distributed-shared-memory/index.html) 
* [P4L4 - Datacenter Technologies](/cs6200/p4l4-datacenter-technologies/index.html) 
* [P5L1 - Memory Management](/cs6200/p5l1-memory-management-t/index.html)
-->
<h2 id="algorithms-part-ii"><a class="markdownIt-Anchor" href="#algorithms-part-ii"></a> Algorithms: Part II</h2>
<ul>
<li><a href="/algorithms-2/week-1/index.html">Week 1 - Undirected Graph &amp; Directed Graph</a></li>
<li><a href="/algorithms-2/week-2/index.html">Week 2 - Minimum Spanning Trees &amp; Shortest Path</a></li>
<li><a href="/algorithms-2/week-3/index.html">Week 3 - Maximum Flow and Minimum Cut &amp; String Sort</a></li>
<li><a href="/algorithms-2/week-4/index.html">Week 4 - Tries &amp; Substring Search</a></li>
<li><a href="/algorithms-2/week-5/index.html">Week 5 - Regular Expressions</a></li>
<li><a href="/algorithms-2/week-6/index.html">Week 6 - Reductions</a></li>
</ul>
<h2 id="algorithms-part-i"><a class="markdownIt-Anchor" href="#algorithms-part-i"></a> Algorithms: Part I</h2>
<ul>
<li><a href="/algorithms-1/week-1/index.html">Week 1 - Union-Find &amp; Analysis of Algorithms</a></li>
<li><a href="/algorithms-1/week-2/index.html">Week 2 - Stacks and Queues &amp; Elementary Sorts</a></li>
<li><a href="/algorithms-1/week-3/index.html">Week 3 - Mergesort &amp; Quicksort</a></li>
<li><a href="/algorithms-1/week-4/index.html">Week 4 - Priority Queues &amp; Elementary Symbols</a></li>
<li><a href="/algorithms-1/week-5/index.html">Week 5 - Balanced Search Trees</a></li>
<li><a href="/algorithms-1/week-6/index.html">Week 6 - Hash Tables</a></li>
</ul>
<h2 id="introduction-to-software-design-and-architecture"><a class="markdownIt-Anchor" href="#introduction-to-software-design-and-architecture"></a> Introduction to Software Design and Architecture</h2>
<ul>
<li><a href="/introduction-to-software-design-and-architecture/design-pattern/index.html">Design Pattern</a></li>
</ul>
<h2 id="calculus-two-sequences-and-series"><a class="markdownIt-Anchor" href="#calculus-two-sequences-and-series"></a> Calculus Two: Sequences and Series</h2>
<ul>
<li><a href="/calculus-two/week-1/index.html">Week 1 - Sequences</a></li>
<li><a href="/calculus-two/week-2/index.html">Week 2 - Series</a></li>
<li><a href="/calculus-two/week-3/index.html">Week 3 - Convergence Tests</a></li>
<li><a href="/calculus-two/week-4/index.html">Week 4 - Alternating Series</a></li>
<li><a href="/calculus-two/week-5/index.html">Week 5 - Power Series</a></li>
<li><a href="/calculus-two/week-6/index.html">Week 6 - Taylor Series</a></li>
</ul>
<h2 id="laff-linear-algebra"><a class="markdownIt-Anchor" href="#laff-linear-algebra"></a> LAFF Linear Algebra</h2>
<ul>
<li><a href="/laff-linear-algebra/week-1/index.html">Week 1 - Vectors in Linear Algebra</a></li>
<li><a href="/laff-linear-algebra/week-2/index.html">Week 2 - Linear Transformations and Matrices</a></li>
<li><a href="/laff-linear-algebra/week-3/index.html">Week 3 - Matrix-Vector Operations</a></li>
<li><a href="/laff-linear-algebra/week-4/index.html">Week 4 - Matrix-Vector to Matrix-Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-5/index.html">Week 5 - Matrix- Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-6/index.html">Week 6 - Gaussian Elimination</a></li>
<li><a href="/laff-linear-algebra/week-7/index.html">Week 7 - More Gaussian Elimination and Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-8/index.html">Week 8 - More on Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-9/index.html">Week 9 - Vector Spaces</a></li>
<li><a href="/laff-linear-algebra/week-10/index.html">Week 10 - Vector Spaces, Orthogonality, and Linear Least-Squares</a></li>
<li><a href="/laff-linear-algebra/week-11/index.html">Week 11 - Orthogonal Projection, Low Rank Approximation, and Orthogonal Bases</a></li>
<li><a href="/laff-linear-algebra/week-12/index.html">Week 12 - Eigenvalues and Eigenvectors</a></li>
</ul>
<h2 id="stanford-machine-learning"><a class="markdownIt-Anchor" href="#stanford-machine-learning"></a> Stanford Machine Learning</h2>
<ul>
<li><a href="/stanford-machine-learning/week-1/index.html">Week 1 - Introduction</a></li>
<li><a href="/stanford-machine-learning/week-2/index.html">Week 2 - Linear Regression with Multiple Variables</a></li>
<li><a href="/stanford-machine-learning/week-3/index.html">Week 3 - Logistic Regression &amp; Regularization</a></li>
<li><a href="/stanford-machine-learning/week-4/index.html">Week 4 - Neural Networks: Representation</a></li>
<li><a href="/stanford-machine-learning/week-5/index.html">Week 5 - Neural Networks: Learning</a></li>
<li><a href="/stanford-machine-learning/week-6a/index.html">Week 6a - Advice for Applying Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-6b/index.html">Week 6b - Machine Learning System Design</a></li>
<li><a href="/stanford-machine-learning/week-7/index.html">Week 7 - Support Vector Machines</a></li>
<li><a href="/stanford-machine-learning/week-8/index.html">Week 8 - Unsupervised Learning &amp; Dimensionality Reduction</a></li>
<li><a href="/stanford-machine-learning/week-9a/index.html">Week 9a - Anomaly Detection</a></li>
<li><a href="/stanford-machine-learning/week-9b/index.html">Week 9b - Recommender Systems</a></li>
<li><a href="/stanford-machine-learning/week-10/index.html">Week 10 - Large Scale Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-11/index.html">Week 11 - Application Example: Photo OCR</a></li>
</ul>
<h2 id="calculus-one"><a class="markdownIt-Anchor" href="#calculus-one"></a> Calculus One</h2>
<ul>
<li><a href="/calculus-one/week-2-3/index.html">Week 2-3 - Functions &amp; Limits</a></li>
<li><a href="/calculus-one/week-4/index.html">Week 4 - The Beginning of Derivatives</a></li>
<li><a href="/calculus-one/week-5/index.html">Week 5 - Techniques of Differentiation</a></li>
<li><a href="/calculus-one/week-6/index.html">Week 6 - Chain Rule</a></li>
<li><a href="/calculus-one/week-7/index.html">Week 7 - Derivatives of Trigonometric Functions</a></li>
<li><a href="/calculus-one/week-8/index.html">Week 8 - Derivatives in the Real World</a></li>
<li><a href="/calculus-one/week-9/index.html">Week 9 - Optimization</a></li>
<li><a href="/calculus-one/week-10/index.html">Week 10 - Linear Approximation</a></li>
<li><a href="/calculus-one/week-11-12/index.html">Week 11-12 - Antidifferentiation &amp; Integration</a></li>
<li><a href="/calculus-one/week-13/index.html">Week 13 - Fundamental Theorem of Calculus</a></li>
<li><a href="/calculus-one/week-14/index.html">Week 14 - Substitution Rule</a></li>
<li><a href="/calculus-one/week-15/index.html">Week 15 - Techniques of Integration</a></li>
<li><a href="/calculus-one/week-16/index.html">Week 16 - Applications of Integration</a></li>
</ul>
<h2 id="computational-thinking"><a class="markdownIt-Anchor" href="#computational-thinking"></a> Computational Thinking</h2>
<ul>
<li><a href="/computational-thinking/lecture-1/index.html">Lecture 1 - Optimization and Knapsack Problem</a></li>
<li><a href="/computational-thinking/lecture-2/index.html">Lecture 2 - Decision Trees and Dynamic Programming</a>
<ul>
<li><a href="/computational-thinking/lecture-2-powerset/index.html">Exercise: Power Set Function</a></li>
</ul>
</li>
<li><a href="/computational-thinking/lecture-3/index.html">Lecture 3 - Graphs</a></li>
<li><a href="/computational-thinking/lecture-4-5/index.html">Lecture 4-5 - Plotting</a></li>
<li><a href="/computational-thinking/lecture-6-7/index.html">Lecture 6-7 - Stochastic Programs &amp; Inferential Statistics</a></li>
<li><a href="/computational-thinking/lecture-8/index.html">Lecture 8 - Monte Carlo Simulation</a></li>
<li><a href="/computational-thinking/lecture-9/index.html">Lecture 9 - Sampling and Standard Error</a></li>
<li><a href="/computational-thinking/lecture-10-11/index.html">Lecture 10-11 - Experimental Data</a></li>
<li><a href="/computational-thinking/lecture-12/index.html">Lecture 12 - Machine Learning</a></li>
<li><a href="/computational-thinking/lecture-13/index.html">Lecture 13 - Statistical Abuses</a></li>
</ul>
<h2 id="effective-thinking-through-mathematics"><a class="markdownIt-Anchor" href="#effective-thinking-through-mathematics"></a> Effective Thinking Through Mathematics</h2>
<ul>
<li><a href="/effective-thinking-through-mathematics/note/index.html">Note</a></li>
<li><a href="/effective-thinking-through-mathematics/week-4-telling-the-story-of-infinity/index.html">Week 4 (/Telling the Story of Infinity)</a></li>
<li><a href="/effective-thinking-through-mathematics/week-5-telling-the-story-of-the-euler-circuit-theorem/index.html">Week 5 (/Telling the Story of Euler Circuit Theorem)</a></li>
</ul>
<h2 id="cs50-introduction-to-computer-science"><a class="markdownIt-Anchor" href="#cs50-introduction-to-computer-science"></a> CS50 Introduction to Computer Science</h2>
<ul>
<li><a href="/cs50/week-1/index.html">Week 1 - C</a></li>
<li><a href="/cs50/week-2/index.html">Week 2 - Arrays</a></li>
<li><a href="/cs50/week-3/index.html">Week 3 - Algorithms</a></li>
<li><a href="/cs50/week-4/index.html">Week 4 - Memory</a></li>
<li><a href="/cs50/week-5/index.html">Week 5 - Data Structures</a></li>
<li><a href="/cs50/week-6/index.html">Week 6 - HTTP</a></li>
<li><a href="/cs50/week-7-10/index.html">Week 7-10 - Machine Learning/Python/SQL/Javascript</a></li>
</ul>
<h2 id="others"><a class="markdownIt-Anchor" href="#others"></a> Others</h2>
<ul>
<li><a href="/symbols/index.html">Symbols of Mathematics</a></li>
<li><a href="/glossary/index.html">Glossary</a></li>
</ul>
<h2 id="about-me"><a class="markdownIt-Anchor" href="#about-me"></a> <a target="_blank" rel="noopener" href="https://ericyy.me/about/">About Me</a></h2>

</div>


<script src="/js/book-menu.js"></script>

  </div>

  <div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseover="add_inner()" onmouseleave="remove_inner()">
  <div class="sidebar-toggle-inner"></div>
</div>

<script>
function add_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.add('show')  
}

function remove_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.remove('show')
}

function sidebar_toggle() {
    let sidebar_toggle = document.querySelector('.sidebar-toggle')
    let sidebar = document.querySelector('.book-sidebar')
    let content = document.querySelector('.off-canvas-content')
    if (sidebar_toggle.classList.contains('extend')) { // show
        sidebar_toggle.classList.remove('extend')
        sidebar.classList.remove('hide')
        content.classList.remove('extend')
    }
    else { // hide
        sidebar_toggle.classList.add('extend')
        sidebar.classList.add('hide')
        content.classList.add('extend')
    }
}
</script>

  <div class="off-canvas-content">
    <div class="columns">
      <div class="column col-10 col-lg-12">
        <div class="book-navbar">
          <!-- For Responsive Layout -->

<header class="navbar">
  <section class="navbar-section">
    <a onclick="open_sidebar()">
      <i class="icon icon-menu"></i>
    </a>
  </section>
</header>

        </div>
        <div class="book-content">
          <div class="book-post">
  <h1 id="p3l4-synchronization-constructs"><a class="markdownIt-Anchor" href="#p3l4-synchronization-constructs"></a> P3L4: Synchronization Constructs</h1>
<!-- toc -->
<ul>
<li><a href="#Common-Synchronization-Contructs">Common Synchronization Contructs</a>
<ul>
<li><a href="#Spinlocks">Spinlocks</a></li>
<li><a href="#Semaphores">Semaphores</a></li>
<li><a href="#Readerx2FWriter-Locks">Reader/Writer Locks</a></li>
<li><a href="#Monitors">Monitors</a></li>
<li><a href="#More-Synchronization-Constructs">More Synchronization Constructs</a></li>
</ul>
</li>
<li><a href="#Atomic-Instructions">Atomic Instructions</a></li>
<li><a href="#Shared-Memory-Multiprocessors">Shared Memory Multiprocessors</a></li>
<li><a href="#Cache-Coherence">Cache Coherence</a>
<ul>
<li><a href="#Cache-Coherence-and-Atomics">Cache Coherence and Atomics</a></li>
</ul>
</li>
<li><a href="#Spinlock">Spinlock</a>
<ul>
<li><a href="#Performance-Metrics">Performance Metrics</a></li>
<li><a href="#Test-and-Set-Spinlock">Test and Set Spinlock</a></li>
<li><a href="#Test-and-Test-and-Set-Spinlock">Test and Test and Set Spinlock</a></li>
<li><a href="#Spinlock-%2522Delay%2522-Alternatives">Spinlock “Delay” Alternatives</a>
<ul>
<li><a href="#Picking-a-Delay">Picking a Delay</a></li>
</ul>
</li>
<li><a href="#Queueing-Lock">Queueing Lock</a>
<ul>
<li><a href="#Queueing-Lock-Implementation">Queueing Lock Implementation</a></li>
</ul>
</li>
<li><a href="#Spinlock-Performance-Comparisons">Spinlock Performance Comparisons</a></li>
</ul>
</li>
<li><a href="#Terms">Terms</a></li>
</ul>
<!-- tocstop -->
<hr />
<p>Mutexes and condition variables and any other software synchronization construct requires a lower level support from the hardware in order to guarantee the correctness. Hardware provides this type of low level support via <strong>atomic instructions</strong>.</p>
<h2 id="common-synchronization-contructs"><a class="markdownIt-Anchor" href="#common-synchronization-contructs"></a> Common Synchronization Contructs</h2>
<h3 id="spinlocks"><a class="markdownIt-Anchor" href="#spinlocks"></a> Spinlocks</h3>
<p>It’s similar to mutex lock/unlock:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spinlock_lock(s);</span><br><span class="line">    <span class="comment">// critical sesion</span></span><br><span class="line">spinlock_unlock(s);</span><br></pre></td></tr></table></figure>
<p>The difference is, when lock == busy, the thread that is suspended it’s execution at the spinlock_lock() isn’t blocked like mutex. Instead, it’s spinning. The thread will continue to burn CPU cycles until the lock is free or gets preempted for some reasons.</p>
<h3 id="semaphores"><a class="markdownIt-Anchor" href="#semaphores"></a> Semaphores</h3>
<p>It’s common sync construct in OS kernels, act like a traffic light to allow the traffic to STOP or GO.</p>
<ul>
<li>Semaphore is represented via a positive integer number initially, which is the max running threads. If threads arrive at semaphore with
<ul>
<li>a non-zero value, the semaphore will decrease the value and the thread will process,</li>
<li>a zero value, the thread will wait</li>
</ul>
</li>
<li>when the semaphore is initialized with value: 1, it will be a binary semaphore, act like mutex</li>
</ul>
<p>POSIX Semaphores(<a target="_blank" rel="noopener" href="http://linux.die.net/man/3/sem_init">link</a>):</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;semaphore.h&gt;</span></span></span><br><span class="line">    </span><br><span class="line"><span class="type">sem_t</span> sem;</span><br><span class="line"><span class="comment">// pshared: flag of whether this semaphore is in a single process or across processes.</span></span><br><span class="line">sem_init(<span class="type">sem_t</span> *sem, <span class="type">int</span> pshared, <span class="type">int</span> count);  </span><br><span class="line">sem_wait(<span class="type">sem_t</span> *sem);</span><br><span class="line">sem_post(<span class="type">sem_t</span> *sem);</span><br></pre></td></tr></table></figure>
<h3 id="readerwriter-locks"><a class="markdownIt-Anchor" href="#readerwriter-locks"></a> Reader/Writer Locks</h3>
<p>When specifying synchronization requirements, it is sometimes useful to distinguish among different types of resource access.</p>
<p>For instance,</p>
<ul>
<li>“read” accesses - “never modify” <strong>shared access</strong> lock</li>
<li>“write” accesses - “always modify” <strong>exclusive access</strong> lock.<br />
To cope with this situation, we have <strong>rwlock</strong>s which is similar to mutex, but developers can specify the type of access, and then the lock can behave accordingly.</li>
</ul>
<p>The primary API(<a target="_blank" rel="noopener" href="https://elixir.bootlin.com/linux/v6.0.3/source/include/linux/rwlock.h">link</a>)</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;linux/spinlock.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">rwlock_t</span> m;</span><br><span class="line">read_lock(m);</span><br><span class="line">    <span class="comment">// critical section</span></span><br><span class="line">read_unlock(m);</span><br><span class="line"></span><br><span class="line">write_lock(m);</span><br><span class="line">    <span class="comment">// critical section</span></span><br><span class="line">write_unlock(m);</span><br></pre></td></tr></table></figure>
<ul>
<li>read/write == share/exclusive</li>
</ul>
<p>Semantic differences:</p>
<ul>
<li>Recursive read lock
<ul>
<li>May differ on how recursively-obtained read locks are unlocked. When unlocking a recursive read lock, some implementations unlock all the recursive locks, while others only unlock the most recently acquired lock.</li>
</ul>
</li>
<li>Upgrade/downgrade <strong>priority</strong>
<ul>
<li>Allow readers to convert their lock into a writer lock mid-execution, while other implementations do not allow this upgrade.</li>
</ul>
</li>
<li>Interaction with scheduling policy
<ul>
<li>Block an incoming reader if there is a writer with higher priority waiting</li>
</ul>
</li>
</ul>
<h3 id="monitors"><a class="markdownIt-Anchor" href="#monitors"></a> Monitors</h3>
<p><strong>Monitors</strong> are a <u>higher level synchronization construct</u> that allow us to avoid manually invoking these synchronization operations.</p>
<p>Monitors specify:</p>
<ul>
<li>shared resource</li>
<li>entry procedure for accessing that resource</li>
<li>possible condition variable used to wake up different types of waiting threads.</li>
</ul>
<p>When invoking the entry procedure, all of the necessary locking and checking will take place by the monitor behind the scenes. When a thread is done with a shared resource and is exiting the procedure, all of the unlocking and potential signaling is again performed by the monitor behind the scenes.</p>
<h3 id="more-synchronization-constructs"><a class="markdownIt-Anchor" href="#more-synchronization-constructs"></a> More Synchronization Constructs</h3>
<p><strong>Serializers</strong> make it easier to define priorities and also hide the need for explicit signaling and explicit use of condition variables from the programmer.</p>
<p><strong>Path Expressions</strong> require that a programmer specify a regular expression that captures the correct synchronization behavior. As opposed to using locks, the programmer would specify something like “Many Reads, Single Write”, and the runtime will make sure that the way the operations are interleaved satisfies the regular expression.</p>
<p><strong>Barriers</strong> are like reverse semaphores. While a semaphore allows n threads to proceed before it blocks, a barrier blocks until n threads arrive at the barrier point. Similarly, <strong>Rendezvous Points</strong> also wait for multiple threads to arrive at a particular point in execution.</p>
<p>To further boost scalability and efficiency metrics, there are efforts to achieve concurrency without explicitly locking and waiting. These <strong>wait-free synchronization constructs</strong> are optimistic in the sense that they bet on the fact that there won’t be any concurrent writes and that it is safe to allow reads to proceed concurrently. An example of this is <strong>read-copy-update (RCU)</strong> lock that is part of the Linux kernel.</p>
<p>All of these methods require some support from the underlying hardware to atomically make updates to a memory location.</p>
<p>The following lecture will discuss different implementations of spinlocks base on the paper <a target="_blank" rel="noopener" href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud923/references/ud923-anderson-paper.pdf">“The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors”</a> by Anderson, Thomas E.</p>
<p>Quiz: Does this spinlock implementation correctly guarantee mutual exclusion? is it efficient?</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">spinlock_init(lock):</span><br><span class="line">    lock = <span class="built_in">free</span>; <span class="comment">// 0 = free, 1 = busy</span></span><br><span class="line"></span><br><span class="line">spinlock_lock(lock):</span><br><span class="line">    spin:</span><br><span class="line">        <span class="keyword">if</span> (lock == <span class="built_in">free</span>) &#123;lock = busy;&#125;</span><br><span class="line">        <span class="keyword">else</span> &#123; <span class="keyword">goto</span> spin; &#125;</span><br><span class="line"></span><br><span class="line">spinlock_unlock(lock):</span><br><span class="line">    lock = <span class="built_in">free</span>;</span><br></pre></td></tr></table></figure>
<p>What if changing the lock part to</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spinlock_lock(lock):</span><br><span class="line">    <span class="keyword">while</span> (lock = busy);</span><br><span class="line">    lock = busy;</span><br></pre></td></tr></table></figure>
<p>The answer of both implementations are neither correct, nor efficient. Because we have continuous loop that is spinning as long as the lock is busy which is inefficient. And potentially, when lock is free, all of the waiting threads will set lock to busy at the same time.</p>
<p>So to check the lock and set the lock value atomically, we need hardware support.</p>
<h2 id="atomic-instructions"><a class="markdownIt-Anchor" href="#atomic-instructions"></a> Atomic Instructions</h2>
<p>Each type of hardware will support a number of atomic instructions. Some examples include</p>
<ul>
<li><code>test_and_set</code></li>
<li><code>read_and_increment</code></li>
<li><code>compare_and_swap</code><br />
Each of these operations performs <strong>multi cycle operation</strong>.</li>
</ul>
<p>Because they are atomic instructions, the hardware guarantees:</p>
<ul>
<li>operations will happen atomically</li>
<li>mutual exclusion - threads on multiple cores cannot perform the atomic operations in parallel</li>
<li>concurrent attempts to execute an instruction will be queued and performed serially.</li>
</ul>
<p>To fix the quiz:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spinlock_lock(lock) &#123;</span><br><span class="line">    <span class="keyword">while</span>(test_and_set(lock) == busy);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>When the first thread arrives, <code>test_and_set</code> looks at the value that lock points to. This value will initially be zero. <code>test_and_set</code> atomically sets the value to one, and returns zero(<em>busy</em> is equal to one). Thus, the first thread can proceed. When subsequent threads arrive, <code>test_and_set</code> will look at the value - which is busy - and just return it. Then these threads will spin.</p>
<p>Different platform has different hardware. There may be differences in the efficiencies with which different atomic operations execute on different architectures. We need to check the implementation uses one of the atomic instructions available on the target platform. And also make sure that the software is optimized so that it uses the most efficient atomics on a target platform and uses them in an efficient way.</p>
<h2 id="shared-memory-multiprocessors"><a class="markdownIt-Anchor" href="#shared-memory-multiprocessors"></a> Shared Memory Multiprocessors</h2>
<p>A multiprocessor system consists of more than one CPU and some memory unit that is accessible to all of these CPUs. That is, the memory component is shared by the CPUs.</p>
<img src="https://i.imgur.com/VDfBtIR.jpg" style="width: 500px" />
<p>In a <strong>bus-based</strong> configuration, the shared bus can only support one memory module at a time. However, it can apply to the multiple memory modules as well.</p>
<p>In the <strong>interconnect-based</strong> configuration, multiple memory references can be in flight at a given moment when each CPU want to access different memory addresses. In bus-based configuration, only one memory access is allow since the CPUs share the same bus.</p>
<p>Shared memory multiprocessors are also referred to as <strong>symmetric multiprocessors(SMPs)</strong>. Each CPU in the SMP platform can have caches to hide latencies for two reasons:</p>
<ol>
<li>The cache is much closer to the CPU</li>
<li>Due to the contention amongst the CPUs for accessing shared memory, cache makes it much faster by avoid it.</li>
</ol>
<img src="https://i.imgur.com/YYwzE91.jpg" style="width: 500px" />
<p>There are different strategy associated with write action:</p>
<ol>
<li>no-write. When CPU need to write, it will bypass cache, and go straight to the memory. And any copy of that memory will be invalidate.</li>
<li>write-through.  CPU applies write action to both cache and memory.</li>
<li>write-back. CPU writes to cache, and perform the write to memory at some later point in time, perhaps when the particular cache line is evicted.</li>
</ol>
<h2 id="cache-coherence"><a class="markdownIt-Anchor" href="#cache-coherence"></a> Cache Coherence</h2>
<p>To cope with cache coherence, there are two architectures: <strong>non-cache-coherent(NCC) architecture</strong> and <strong>cache-coherent(CC) architecture</strong>. NCC refers to software-based solution, CC refers to hardware-based solution.</p>
<p>In CC, there are two strategies: <strong>write-invalidate(WI)</strong> and <strong>write-update(WU)</strong>.</p>
<p>In WI, the hardware will invalidate all cache entries once one CPU updates its copy. Future references to the invalidated cache entries will have to go through the memory before being re-cached.</p>
<p>In WU, the hardware will ensure all cache entries are updated once one CPU updated its copy.</p>
<p>In comparison:</p>
<ol>
<li>WI require lower bandwidth because we don’t send data to each cache, but only the addresses of the cache for invalidation.
<ul>
<li>If other CPUs don’t need the data anytime soon, it’s possible to amortize the cost of the coherence traffic over multiple changes. Because the data can be changed multiple times on one CPU before it is needed on another CPU.</li>
</ul>
</li>
<li>The advantage of WU is, we have immediate access the updated cache entries. The downside is, the hardware needs to update all caches when any CPU updated the cache.</li>
</ol>
<h3 id="cache-coherence-and-atomics"><a class="markdownIt-Anchor" href="#cache-coherence-and-atomics"></a> Cache Coherence and Atomics</h3>
<p>The question is, how to deal with atomic instruction in SMP?</p>
<p>When an atomic instruction is performed against the cached value on one CPU, it is really challenging to know whether or not an atomic instruction will be attempted on the cached value on another CPU.</p>
<p>One solution is, always bypass caches, and the atomic operations are issued directly to the memory. This way, we enforce a central entry point where all of the references can be ordered and synchronized in a unique manner.</p>
<p>But the problem is, atomic instructions are gonna take much longer than other instructions since they don’t use cache. Not only will they always have to access main memory, but they will also have to contend(竞争) on that memory. Moreover, to guarantee the atomic behavior, we have to generate the coherence traffic to either update or invalidate all cached references regardless of the changes. This is necessary in order to stay on the side of safety and be able to guarantee correctness of the atomic operations.</p>
<p>In summary, atomic instructions on SMP systems are more expensive than on single CPU system because of bus or I/C contention. In addition, atomics in general are more expensive because they bypass the cache and always trigger coherence traffic.</p>
<h2 id="spinlock"><a class="markdownIt-Anchor" href="#spinlock"></a> Spinlock</h2>
<h3 id="performance-metrics"><a class="markdownIt-Anchor" href="#performance-metrics"></a> Performance Metrics</h3>
<ol>
<li>latency - time to acquire a free lock. ideally immediately execute atomically</li>
<li>waiting time(delay) - time to stop spinning and acquire a lock that has been freed. ideally immediately</li>
<li>contention - bus/network inter-connection(I/C) traffic. ideally zero
<ul>
<li>By contention, we mean the contention due to the actual atomic memory references as well as the subsequent coherence traffic.</li>
</ul>
</li>
</ol>
<p>1 and 2 are conflicted with each other. Because 1 means we want to try to execute the atomic operation asap. As a result, the locking operation will immediately incur an atomic operation which can potentially create some additional contention on the network.</p>
<p>2 and 3 are conflicted as well. To reduce the waiting time, we need to continuously spin on the lock as long as it’s not available. So we can acquire the lock asap.</p>
<h3 id="test-and-set-spinlock"><a class="markdownIt-Anchor" href="#test-and-set-spinlock"></a> Test and Set Spinlock</h3>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spinlock_init(lock):</span><br><span class="line">    lock = <span class="built_in">free</span>; <span class="comment">// 0 = free, 1 = busy</span></span><br><span class="line"></span><br><span class="line">spinlock_lock(lock) : <span class="comment">// spin</span></span><br><span class="line">    <span class="keyword">while</span>(test_and_set(lock) == busy);</span><br><span class="line"></span><br><span class="line">spinlock_unlock(lock):</span><br><span class="line">    lock = <span class="built_in">free</span>;</span><br></pre></td></tr></table></figure>
<p>The <code>test_and_set</code> instruction is a very common atomic that most hardware platforms support.</p>
<p><strong>Latency</strong>: minimal (just atomic)<br />
<strong>Delay</strong>: potentially minimal (spinning continuously on the atomic operation)<br />
<strong>Contention</strong>: not so well because processors go to memory on each spin.</p>
<p>With this implementation, even if we have coherent caches, they will be bypassed because we’re using an atomic instruction.</p>
<h3 id="test-and-test-and-set-spinlock"><a class="markdownIt-Anchor" href="#test-and-test-and-set-spinlock"></a> Test and Test and Set Spinlock</h3>
<p>The problem with the previous implementation is that all of the CPUs are spinning on the atomic operation. Let’s try to separate the test part from the atomic. The intuition is that CPUs can potentially test their cached copy of the lock and only execute the atomic if it detects that its cached copy has changed.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// test (cache), test_and_set (Mm)</span></span><br><span class="line"><span class="comment">// spins on cache (lock == busy)</span></span><br><span class="line"><span class="comment">// atomic if freed (test_and_set)</span></span><br><span class="line"></span><br><span class="line">spinlock_lock(lock):</span><br><span class="line">    <span class="keyword">while</span>((lock == busy) OR (test_and_set(lock) == busy))</span><br></pre></td></tr></table></figure>
<p>This spinlock is referred to as the <strong>test-and-test-and-set spinlock</strong>. It is also called a spin-on-read or spin-on-cached-value spinlock.</p>
<p><strong>Latency</strong>: slightly worse than the test-and-set lock due to the extra step for checking lock in memory<br />
<strong>Delay</strong>: same as latency due to the extra step.<br />
<strong>Contention</strong>: Depends on our <strong>cache coherence</strong> strategy</p>
<ul>
<li>NCC: no different from test-and-set</li>
<li>CC-WU: improves. only problem is when all caches are updated to free, they will all try to acquire the lock</li>
<li>CC-WI: horrible. Every single attempt to acquire the lock will generate contention for the memory module and will also create invalidation traffic.
<ul>
<li>One outcome of executing an atomic instruction is that we will trigger the cache coherence strategy regardless of whether or not the value protected by the atomic changes.</li>
<li>write-invalidate will invalidate the cached copy. Even if the value hasn’t changed, the invalidation will force the CPU to go to main memory to execute the atomic. What this means is that any time another CPU executes an atomic, all of the other CPUs will be invalidated and will have to go to memory.</li>
</ul>
</li>
</ul>
<p>In an SMP system with N processors, what is the complexity of the memory contention(accesses), relative to N, that will result from releasing a <strong>test_and_test_and_set</strong> spinlock?</p>
<ul>
<li>CC-WU: O(N) When lock is released, the <code>test_and_lock</code> function will be executed N times because each of the processors in the spin lock will  pass the (lock == busy) and run the next condition.</li>
<li>CC-WI: O(N^2) When lock is released, the cache on each processor will be invalidated, and they will all fetch the lock status from memory, update their own cache status and invalidate the others’ cache. So it will be N^2.</li>
</ul>
<h3 id="spinlock-delay-alternatives"><a class="markdownIt-Anchor" href="#spinlock-delay-alternatives"></a> Spinlock “Delay” Alternatives</h3>
<p>Delay after lock release</p>
<ul>
<li>Everyone sees lock is free at the same time, but not everyone attempts to acquire it.</li>
<li>The rationale behind this is to prevent every thread from executing the atomic instruction at the same time.</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spinlock_lock(lock):</span><br><span class="line">    <span class="keyword">while</span>((lock == busy) OR (test_and_set(lock) == busy)) &#123;</span><br><span class="line">        <span class="comment">// failed to get lock</span></span><br><span class="line">        <span class="keyword">while</span>(lock == busy);</span><br><span class="line">        delay();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>You can see that all processors waits at the inner while. Once lock == free, they are all gonna delay for a certain time, then go to the outer while. In here, one of the processors will call <code>test_and_set()</code> and set the lock to busy, and the others will continue back to the inner while.</p>
<p><strong>Latency</strong>: ok. We still have to perform a memory reference to bring the lock into the cache, and then another to perform the atomic. However, this is not much different from what we have seen in the past.<br />
<strong>Delay</strong>: Much worse because we add more delay to it.<br />
<strong>Contention</strong>: significantly improved.</p>
<ul>
<li>When the delay expires, the delayed threads will try to re-check the value of the lock, and it’s possible that another thread executed the atomic and the delayed threads will see that the lock is busy. If the lock is free, the delayed thread will execute the atomic.</li>
<li>There will be fewer cases in which threads see a busy lock as free and try to execute an atomic that will not be successful.</li>
</ul>
<p>An alternative delay-based lock introduces a delay after each memory reference.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spinlock_lock(lock):</span><br><span class="line">    <span class="keyword">while</span>((lock == busy) OR (test_and_set(lock) == busy)) &#123;</span><br><span class="line">        delay();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>The main benefit of this is that it works on NCC architectures. Since a thread has to go to main memory on every reference on NCC architectures, introducing an artificial delay great decreases the number of reference the thread has to perform while spinning.</p>
<p>The problem is, when lock is busy, every spin will have some delay, not just when the lock is free. So it adds more delay to the processors.</p>
<h4 id="picking-a-delay"><a class="markdownIt-Anchor" href="#picking-a-delay"></a> Picking a Delay</h4>
<p>Two strategies for picking the delay: <strong>static delay</strong> and <strong>dynamic delay</strong>.</p>
<p><strong>Static delay</strong>: base on fixed info, like CPU ID where the process is running.</p>
<ul>
<li>it’s simple.</li>
<li>under high load, it will likely spread out all of the atomic references such that there is little or no contention.</li>
<li>problem is, create unnecessary load under low contention.
<ul>
<li>e.g. one process on CPU 1, the other process on CPU 32, the static delay = ID * 100ms. the second thread has to wait an inordinate amount of time before executing, even though there is no contention.</li>
</ul>
</li>
</ul>
<p><strong>Dynamic delay</strong>: each thread take a random delay value from a range of possible delays that increases with the “perceived” contention in the system.</p>
<ul>
<li>Under high load, both dynamic and static delays will be sufficient enough to reduce contention within the system.</li>
</ul>
<p>How to evaluate the number of contentions in the system? Track the number of failed <code>test_and_set</code> operations. More fails, more likely a higher degree of contentions.</p>
<p>If we delay after each lock reference, however, our delay grows not only as a function of contention but also as a function of the the length of the critical section. If a thread is executing a large critical section, all spinning threads will be increasing their delays even though the contention in the system hasn’t actually increased.</p>
<h3 id="queueing-lock"><a class="markdownIt-Anchor" href="#queueing-lock"></a> Queueing Lock</h3>
<p>Alternative way to prevent every thread from seeing that the lock has been freed at the same time, aka prevent all threads rushing to acquire the lock simultaneously. The new strategy is called <strong>queueing lock</strong>.</p>
<p>It uses an array of flags with up to <code>n</code> elements(n = no. of threads in the system). Each element can have either <code>has_lock</code> or <code>must_wait</code> flag. In addition, one pointer points to the current lock holder(who has the <code>has_lock</code> flag), and another pointer points to the last element of the queue. (remember how to use a fixed-length queue to implement stack?)</p>
<p>Since multiple threads may enter the lock at the same time, it’s important to <strong>increment the <code>queuelast</code> pointer atomically</strong>. This requires some support for a <strong>read_and_incremement</strong> atomic.</p>
<p>For each thread arriving at the lock, the assigned element of the flags array at the ticket index acts like a private lock. As long as this value is <code>must_wait</code>, the thread will have to spin. When the value of the element is becomes <code>has_lock</code>, this will signify to the threads that the lock is free and they can attempt to enter their critical section.</p>
<p>When a thread completes a critical section and needs to release the lock, it needs to signal the next thread. Thus <code>queue[ticket + 1] = has_lock</code>.</p>
<p>This strategy has two drawbacks:</p>
<ul>
<li>First, it requires support for the <strong>read_and_increment</strong> atomic, which is less common that <strong>test_and_set</strong>.</li>
<li>In addition, this lock requires much more space than other locks. All other locks required a single memory location to track the value of the lock. This lock requires <code>n</code> such locations, one for each thread.</li>
</ul>
<h4 id="queueing-lock-implementation"><a class="markdownIt-Anchor" href="#queueing-lock-implementation"></a> Queueing Lock Implementation</h4>
<pre class="highlight"><code class="c">init:
    flag[<span class="number">0</span>] = has-lock;
    flags[<span class="number">1.</span>.p<span class="number">-1</span>] - must-wait;
    queuelast = <span class="number">0</span>; <span class="comment">// global variable</span>

lock:
    myplace = r&amp;inc(queuelast); <span class="comment">// get ticket</span>
    <span class="comment">// spin</span>
    <span class="keyword">while</span>(flags[my place % p] == must-wait)
    <span class="comment">// now in C.S</span>
    flags[myplace % p] = must-wait;
    
unlock:
    flags[myplace+<span class="number">1</span> % p] = has-lock;
</code></pre>
<p><strong>Latency</strong>: not very efficient. It performs a more complex atomic operation, <code>read_and_increment</code> takes more cycles than <code>test_and_set</code>.<br />
<strong>Delay</strong>: good. Each lock holder directly signals the next element in the queue that the lock has been freed.<br />
<strong>Contention</strong>: much better than any locks we discussed. The atomic operation is only performed once up front and is not part of the spinning code. The atomic operation and the spinning done with different variables, so cache invalidation on the atomic variable doesn’t impact spinning.</p>
<p>In order to realize these contention gains, we must have a cache coherent architecture. Otherwise, spinning must happen on potentially remote memory references. In addition, we have to make sure that every element is on a separated cache line. Otherwise, when we change the value of one element in the array, we will invalidate the entire <a target="_blank" rel="noopener" href="https://open-cas.github.io/cache_line.html">cache line</a>, so the processors spinning on other elements will have their caches invalidated.</p>
<h3 id="spinlock-performance-comparisons"><a class="markdownIt-Anchor" href="#spinlock-performance-comparisons"></a> Spinlock Performance Comparisons</h3>
<img src="https://i.imgur.com/z0SXeYY.jpg" style="width: 600px" />
<p>This figures shows measurements that were gathered from executing a program that had multiple processes. Each process executed a critical section in a loop, one million times. The number of processes in the system was varied such that there was <em>only one process per processor</em>.</p>
<p><strong>Overhead</strong>: how long it would take to execute the number of critical sections.</p>
<p>Under <em>high</em> load, the <strong>queueing lock</strong> performs the best. It is the most scalable; as we add more processors, the overhead does not increase.</p>
<p>The <strong>test_and_test_and_set lock</strong> performs the worst under load. The platform is cache coherent with write-invalidate, and we discussed earlier how this strategy forces O(N^2) memory references to maintain cache coherence.</p>
<p>The spinlock with <strong>static delay</strong> are a little better than the one with <strong>dynamic delay</strong>, since dynamic delay have some measure of randomness and will have more <em>collisions</em> than static locks. Also, delaying after every reference is slightly better than delaying after every lock release.</p>
<p>Under <em>smaller</em> loads, <strong>test_and_test_and_set</strong> performs pretty well: it has low latency. We can also see that the <strong>dynamic delay</strong> alternatives perform better than the <strong>static delay</strong>. Static delay locks can potentially enforce unnecessarily large delays under small loads, while dynamic delay adjust based on contention.</p>
<p>Under <em>light</em> loads, the <strong>queueing lock</strong> performs the worst. This is because of the higher latency inherent to the queueing lock, in the form of the more complex atomic, <strong>read_and_increment</strong>, as well as some additional computation that is required at lock time.</p>
<h2 id="terms"><a class="markdownIt-Anchor" href="#terms"></a> Terms</h2>
<ul>
<li><a target="_blank" rel="noopener" href="https://open-cas.github.io/cache_line.html">cache line</a>: A <strong>cache line</strong> is the smallest portion of data that can be mapped into a cache. Every mapped cache line is associated with a <strong>core line</strong>, which is a corresponding region on a <strong>backend storage</strong>. Both the <strong>cache storage</strong> and the backend storage are split into blocks of the size of a cache line, and all the cache mappings are aligned to these blocks.</li>
</ul>

</div>


  <div class="book-comments">
    




  </div>



<script src="/js/book-post.js"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

        </div>
      </div>
      <div class="column col-2 hide-lg">
        <div class="book-post-info">
  
    <div class="book-post-meta">
  
  <div class="divider"></div>
</div>

  

  <div class="book-tocbot">
</div>
<div class="book-tocbot-menu">
  <a class="book-toc-expand" onclick="expand_toc()">Expand all</a>
  <a onclick="go_top()">Back to top</a>
  <a onclick="go_bottom()">Go to bottom</a>
</div>


<script src="/js/book-toc.js"></script>

</div>

      </div>
    </div>
  </div>
  
  <a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>

</body>
</html>


<script src="/js/book.js"></script>
