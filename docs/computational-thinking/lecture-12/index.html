<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">

    
      <link rel="icon" href="/favicon.png" />
    

    <title>
        
          Eric&#39;s CS Notes
        
    </title>

    <!-- Spectre.css framework -->
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-exp.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-icons.min.css">

    <!-- theme css & js -->
    
<link rel="stylesheet" href="/css/book.css">

    
<script src="/js/book.js"></script>


    <!-- tocbot -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css">
    
    <!-- katex -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/zooming/2.1.1/zooming.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    const zooming = new Zooming()
    zooming.listen('.book-content img')
})
</script>

<meta name="generator" content="Hexo 6.0.0"></head>


<body>

<div class="book-container">
  <div class="book-sidebar">
    <div class="book-brand">
  <a href="/">
    <img src="/favicon.png">
    <span>ERIC&#39;S CS NOTES</span>
  </a>
</div>

    <div class="book-menu">
  
</div>


<script src="/js/book-menu.js"></script>

  </div>

  <div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseover="add_inner()" onmouseleave="remove_inner()">
  <div class="sidebar-toggle-inner"></div>
</div>

<script>
function add_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.add('show')  
}

function remove_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.remove('show')
}

function sidebar_toggle() {
    let sidebar_toggle = document.querySelector('.sidebar-toggle')
    let sidebar = document.querySelector('.book-sidebar')
    let content = document.querySelector('.off-canvas-content')
    if (sidebar_toggle.classList.contains('extend')) { // show
        sidebar_toggle.classList.remove('extend')
        sidebar.classList.remove('hide')
        content.classList.remove('extend')
    }
    else { // hide
        sidebar_toggle.classList.add('extend')
        sidebar.classList.add('hide')
        content.classList.add('extend')
    }
}
</script>

  <div class="off-canvas-content">
    <div class="columns">
      <div class="column col-10 col-lg-12">
        <div class="book-navbar">
          <!-- For Responsive Layout -->

<header class="navbar">
  <section class="navbar-section">
    <a onclick="open_sidebar()">
      <i class="icon icon-menu"></i>
    </a>
  </section>
</header>

        </div>
        <div class="book-content">
          <div class="book-post">
  <h1 id="lecture-12"><a class="markdownIt-Anchor" href="#lecture-12"></a> Lecture 12</h1>
<h2 id="machine-learning"><a class="markdownIt-Anchor" href="#machine-learning"></a> Machine Learning</h2>
<ul>
<li>Definition
<ul>
<li>
<img src="https://i.imgur.com/KABd2dx.jpg" style="width:200px" />
</li>
</ul>
</li>
<li>Basic Paradigm
<ul>
<li>Observe set of examples: <strong>training data</strong></li>
<li>Infer something about process that generated that data</li>
<li>Use inference to make predictions about previously unseen data: <strong>test data</strong></li>
</ul>
</li>
<li>Procedures
<ul>
<li>Representation of the features
<ul>
<li>separate people with features(man/woman, educated/not, etc.)</li>
</ul>
</li>
<li>Distance metric for feature vectors
<ul>
<li>make feature vectors can be calculated in a same range.</li>
</ul>
</li>
<li>Objective function and constraints</li>
<li>Optimization method for learning the model</li>
<li>Evaluation method</li>
</ul>
</li>
</ul>
<h3 id="supervised-learning"><a class="markdownIt-Anchor" href="#supervised-learning"></a> Supervised Learning</h3>
<ul>
<li>Start with set of feature vector/value pairs</li>
<li>Goal: find a model that predicts a value for a previously unseen feature vector</li>
<li><strong>Regression models</strong> predict a real
<ul>
<li>As with linear regression</li>
</ul>
</li>
<li><strong>Classification models</strong> predict a label (chosen from a finite set of labels)</li>
</ul>
<h3 id="unsupervised-learning"><a class="markdownIt-Anchor" href="#unsupervised-learning"></a> Unsupervised Learning</h3>
<ul>
<li>Start with a set of feature vectors</li>
<li>Goal: uncover some latent structure in the set of feature vectors</li>
<li><strong>Clustering</strong> the most common technique
<ul>
<li>Define some metric that captures how similar one feature vector is to another</li>
<li>Group examples based on this metric</li>
</ul>
</li>
</ul>
<h3 id="difference-between-supervised-and-unsupervised"><a class="markdownIt-Anchor" href="#difference-between-supervised-and-unsupervised"></a> Difference between Supervised and Unsupervised</h3>
<ul>
<li>
<img src="https://i.imgur.com/cSdabJu.jpg" style="width:300px" />
</li>
<li>with label, we can classify the data to two clusters by wight or height, or four clusters by wight and height, which is Supervised Learning</li>
<li>without label, to figure out how to clustering the data, is Unsupervised Learning.</li>
</ul>
<h3 id="choose-feature-vectors"><a class="markdownIt-Anchor" href="#choose-feature-vectors"></a> Choose Feature Vectors</h3>
<ul>
<li>Why should careful?
<ul>
<li>Irrelevant features can lead to a bad model.</li>
<li>Irrelevant features can greatly slow the learning process.</li>
</ul>
</li>
<li>How?
<ul>
<li><strong>signal-to-noise ratio (SNR)</strong>
<ul>
<li>Think of it as the ratio of useful input to irrelevant input.</li>
</ul>
</li>
<li>The purpose of feature extraction is to separate those features in the available data that contribute to the signal from those that are merely noise.</li>
</ul>
</li>
</ul>
<h3 id="distance-between-vectors"><a class="markdownIt-Anchor" href="#distance-between-vectors"></a> Distance Between Vectors</h3>
<h4 id="minkowski-metric"><a class="markdownIt-Anchor" href="#minkowski-metric"></a> Minkowski Metric</h4>
<ul>
<li>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mi>X</mi><mn>1</mn><mo separator="true">,</mo><mi>X</mi><mn>2</mn><mo separator="true">,</mo><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mstyle scriptlevel="0" displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow><mrow><mi>l</mi><mi>e</mi><mi>n</mi></mrow></munderover><mi>a</mi><mi>b</mi><mi>s</mi><mo stretchy="false">(</mo><mi>X</mi><msub><mn>1</mn><mi>k</mi></msub><mo>−</mo><mi>X</mi><msub><mn>2</mn><mi>k</mi></msub><msup><mo stretchy="false">)</mo><mi>p</mi></msup><msup><mo stretchy="false">)</mo><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mi>p</mi></mrow></msup></mstyle></mrow><annotation encoding="application/x-tex">dist(X1, X2, p)=(\displaystyle\sum_{k-1}^{len}abs(X1_{k}-X2_{k})^p)^{1/p}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">d</span><span class="mord mathdefault">i</span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.1965570000000003em;vertical-align:-1.360444em;"></span><span class="mopen">(</span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.836113em;"><span style="top:-1.8478869999999998em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.300005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.360444em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">a</span><span class="mord mathdefault">b</span><span class="mord mathdefault">s</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord">1</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.714392em;"><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">/</span><span class="mord mathdefault mtight">p</span></span></span></span></span></span></span></span></span></span></span></span></p>
</li>
<li>
<p>p = 1: Manhattan Distance</p>
</li>
<li>
<p>P = 2: Euclidean Distance</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">minkowskiDist</span>(<span class="params">v1, v2, p</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Assumes v1 and v2 are equal-length arrays of numbers </span></span><br><span class="line"><span class="string">       Returns Minkowski distance of order p between v1 and v2&quot;&quot;&quot;</span> </span><br><span class="line">    dist = <span class="number">0.0</span> </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(v1)):</span><br><span class="line">        dist += <span class="built_in">abs</span>(v1[i] - v2[i])**p </span><br><span class="line">    <span class="keyword">return</span> dist**(<span class="number">1.0</span>/p)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>For example:</p>
<ul>
<li>
<img src="https://i.imgur.com/6htFCaX.jpg" style="width:150px" />
</li>
<li>To compare the distance between star and circle and the distance between cross and circle</li>
<li>Use Manhattan Distance, they should be 3 and 4</li>
<li>Use Euclidean Distance, they should be 3 and 2.8 = <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msqrt><mrow><msup><mn>2</mn><mn>2</mn></msup><mo>+</mo><msup><mn>2</mn><mn>2</mn></msup></mrow></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{2^2+2^2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.12661100000000003em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.913389em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.740108em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.740108em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span><span style="top:-2.873389em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.12661100000000003em;"><span></span></span></span></span></span></span></span></span></li>
</ul>
</li>
</ul>
<h5 id="using-distance-matrix-for-classification"><a class="markdownIt-Anchor" href="#using-distance-matrix-for-classification"></a> Using Distance Matrix for Classification</h5>
<ul>
<li>
<p>Procedures</p>
<ul>
<li>Simplest approach is probably nearest neighbor</li>
<li>Remember training data</li>
<li>When predicting the label of a new example
<ul>
<li>Find the nearest example in the training data</li>
<li>Predict the label associated with that example</li>
</ul>
</li>
</ul>
</li>
<li>
<p>To predict the color of <code>X</code></p>
<ul>
<li>
<img src="https://i.imgur.com/ytCzwnn.jpg" style="width:300px" />
</li>
<li>The closest one is pink, so X should be pink</li>
</ul>
</li>
<li>
<p>K-nearest Neighbors</p>
<ul>
<li>Find <code>K</code> nearest neighbors, and choose the label associated with the majority of those neighbors.</li>
<li>Usually, we use odd number. This sample, we use <code>k = 3</code></li>
<li>
<img src="https://i.imgur.com/Pf5xuga.jpg" style="width:300px" />
</li>
</ul>
</li>
<li>
<p>Advantages and Disadvantages of KNN</p>
<ul>
<li>Advantages
<ul>
<li>Learning fast, no explicit training</li>
<li>No theory required</li>
<li>Easy to explain method and results</li>
</ul>
</li>
<li>Disadvantages
<ul>
<li>Memory intensive and predictions can take a long time</li>
<li>Are better algorithms than brute force</li>
<li>No model to shed light on process that generated data</li>
</ul>
</li>
</ul>
</li>
<li>
<p>For Example</p>
<ul>
<li>
<img src="https://i.imgur.com/20zasHX.jpg" style="width:400px" />
</li>
<li>To predict whether zebra, python and alligator are reptile or not.</li>
<li>Calculate the distances, we got:
<ul>
<li>
<img src="https://i.imgur.com/dIPjgHs.jpg" style="width:400px" />
</li>
<li>The closest three animals to alligator are boa constrictor, chicken and dark frog, and two of them are not reptile, so alligator is not reptile.</li>
<li>But we know alligator is reptile. So what’s wrong?</li>
<li>We notice, all of the features are 0 or 1, except number of legs, which gets disproportionate weight.
<ul>
<li>So, Instead of number of legs, we say “has legs.” And then this becomes a one.</li>
</ul>
</li>
</ul>
</li>
<li>
<img src="https://i.imgur.com/9cR0I6e.jpg" style="width:400px" />
  * The closest three animals to alligator are boa constrictor, chicken and cobra, and two of them are reptile, so alligator is reptile.
</li>
</ul>
</li>
<li>
<p>A More General Approach: Scaling</p>
<ul>
<li>Z-scaling
<ul>
<li>Each feature has a mean of 0 &amp; a standard deviation of 1</li>
</ul>
</li>
<li>Interpolation
<ul>
<li>Map minimum value to 0, maximum value to 1, and linearly interpolate</li>
</ul>
</li>
</ul>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">zScaleFeatures</span>(<span class="params">vals</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Assumes vals is a sequence of floats&quot;&quot;&quot;</span></span><br><span class="line">    result = pylab.array(vals)</span><br><span class="line">    mean = <span class="built_in">float</span>(<span class="built_in">sum</span>(result))/<span class="built_in">len</span>(result)</span><br><span class="line">    result = result - mean</span><br><span class="line">    <span class="keyword">return</span> result/stdDev(result)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">iScaleFeatures</span>(<span class="params">vals</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Assumes vals is a sequence of floats&quot;&quot;&quot;</span></span><br><span class="line">    minVal, maxVal = <span class="built_in">min</span>(vals), <span class="built_in">max</span>(vals)</span><br><span class="line">    fit = pylab.polyfit([minVal, maxVal], [<span class="number">0</span>, <span class="number">1</span>], <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> pylab.polyval(fit, vals)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="clustering"><a class="markdownIt-Anchor" href="#clustering"></a> Clustering</h3>
<ul>
<li>Partition examples into groups (clusters) such that examples in a group are more similar to each other than to examples in other groups</li>
<li>Unlike classification, there is not typically a “right answer”
<ul>
<li>Answer dictated by feature vector and distance metric, not by a ground truth label</li>
</ul>
</li>
</ul>
<h4 id="optimization-problem"><a class="markdownIt-Anchor" href="#optimization-problem"></a> Optimization Problem</h4>
<ul>
<li>Clustering is an optimization problem. The goal is to find a set of clusters that optimizes an objective function, subject to some set of constraints.</li>
<li>Given a distance metric that can be used to decide how close two examples are to each other, we need to define an <strong>objective function</strong> that
<ul>
<li>Minimizes the distance between examples in the same clusters, i.e., minimizes the dissimilarity of the examples within a cluster.</li>
</ul>
</li>
<li>To compute the variability of the examples within a cluster
<ul>
<li>First compute the mean(<code>sum(V)/float(len(V))</code>, more precisely the Euclidean mean) of the feature vectors of all the examples in the cluster. , <code>V</code> is a list of feature vectors.</li>
<li>Compute the distance between feature vectors
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext>variability</mtext><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle scriptlevel="0" displaystyle="true"><munder><mo>∑</mo><mrow><mi>e</mi><mo>∈</mo><mi>c</mi></mrow></munder><mtext>distance</mtext><mo stretchy="false">(</mo><mtext>mean</mtext><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>e</mi><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mstyle></mrow><annotation encoding="application/x-tex">\text{variability}(c)=\displaystyle\sum_{e \in c}\text{distance}(\text{mean}(c), e)^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">variability</span></span><span class="mopen">(</span><span class="mord mathdefault">c</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.3273800000000002em;vertical-align:-1.2773750000000001em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8999949999999997em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="mrel mtight">∈</span><span class="mord mathdefault mtight">c</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2773750000000001em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">distance</span></span><span class="mopen">(</span><span class="mord text"><span class="mord">mean</span></span><span class="mopen">(</span><span class="mord mathdefault">c</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">e</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></li>
</ul>
</li>
</ul>
</li>
<li>The definition of variability within a single cluster, <code>c</code>, can be extended to define a dissimilarity metric for a set of clusters, <code>C</code>:
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext>dissimilarity</mtext><mo stretchy="false">(</mo><mi>C</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle scriptlevel="0" displaystyle="true"><munder><mo>∑</mo><mrow><mi>e</mi><mo>∈</mo><mi>c</mi></mrow></munder><mtext>variability(c)</mtext></mstyle></mrow><annotation encoding="application/x-tex">\text{dissimilarity}(C)=\displaystyle\sum_{e \in c}\text{variability(c)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">dissimilarity</span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.3273800000000002em;vertical-align:-1.2773750000000001em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8999949999999997em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="mrel mtight">∈</span><span class="mord mathdefault mtight">c</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2773750000000001em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">variability(c)</span></span></span></span></span></li>
</ul>
</li>
<li>It’s NOT the optimization problem to find a set of clusters, C, such that <code>dissimilarity(C)</code> is minimized. Because it can easily be minimized by putting each example in its own cluster.</li>
<li>We could put a constraint on the distance between clusters or require that the maximum number of clusters is <code>k</code>. Then to find the minimum between clusters.</li>
</ul>
<h5 id="k-means-clustering"><a class="markdownIt-Anchor" href="#k-means-clustering"></a> K-means Clustering</h5>
<ul>
<li>
<p>Constraint: exactly <code>k</code> non-empty clusters</p>
</li>
<li>
<p>Use a greedy algorithm to find an approximation to minimizing objective function</p>
</li>
<li>
<p>Algorithm</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">randomly chose k examples as initial centroids</span><br><span class="line">while true:</span><br><span class="line">    create k clusters by assigning each</span><br><span class="line">        example to closest centroid</span><br><span class="line">    compute k new centroids by averaging</span><br><span class="line">        examples in each cluster</span><br><span class="line">    if centroids don’t change:</span><br><span class="line">        break</span><br></pre></td></tr></table></figure>
<ul>
<li>Sample: <a href="./unit-4/lecture12-3.py">lecture12-4.py</a>
<ul>
<li><code>k=4</code>, Initial Centroids:
<ul>
<li>
<img src="https://i.imgur.com/V8dCSjw.jpg" style="width:200px" />
</li>
</ul>
</li>
<li>Result:
<ul>
<li>
<img src="https://i.imgur.com/AlE5EKX.jpg" style="width:200px" />
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Unlucky Initial Centroids</p>
<ul>
<li>
<p><code>k=4</code>, Initial Centroids:</p>
<ul>
<li>
<img src="https://i.imgur.com/wp4iegG.jpg" style="width:200px" />
</li>
</ul>
</li>
<li>
<p>Result:</p>
<ul>
<li>
<img src="https://i.imgur.com/AH4D3uZ.jpg" style="width:200px" />
</li>
</ul>
</li>
<li>
<p>Mitigating Dependence on Initial Centroids</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">best = kMeans(points)</span><br><span class="line">for t in range(numTrials):</span><br><span class="line">    C = kMeans(points)</span><br><span class="line">    if dissimilarity(C) &lt; dissimilarity(best):</span><br><span class="line">    best = C</span><br><span class="line">return best</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h3 id="wrapping-up-machine-learning"><a class="markdownIt-Anchor" href="#wrapping-up-machine-learning"></a> Wrapping Up Machine Learning</h3>
<ul>
<li>Use data to build statistical models that can be used to
<ul>
<li>Shed light on system that produced data</li>
<li>Make predictions about unseen data</li>
</ul>
</li>
<li>Supervised learning</li>
<li>Unsupervised learning</li>
<li>Feature engineering</li>
<li>Goal was to expose you to some important ideas
<ul>
<li>Not to get you to the point where you could apply them</li>
<li>Much more detail, including implementations, in text</li>
</ul>
</li>
</ul>

</div>


  <div class="book-comments">
    




  </div>



<script src="/js/book-post.js"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

        </div>
      </div>
      <div class="column col-2 hide-lg">
        <div class="book-post-info">
  
    <div class="book-post-meta">
  
  <div class="divider"></div>
</div>

  

  <div class="book-tocbot">
</div>
<div class="book-tocbot-menu">
  <a class="book-toc-expand" onclick="expand_toc()">Expand all</a>
  <a onclick="go_top()">Back to top</a>
  <a onclick="go_bottom()">Go to bottom</a>
</div>


<script src="/js/book-toc.js"></script>

</div>

      </div>
    </div>
  </div>
  
  <a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>

</body>
</html>


<script src="/js/book.js"></script>
