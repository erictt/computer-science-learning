<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">

    
      <link rel="icon" href="/favicon.png" />
    

    <title>
        
          computational-thinking/lecture-10-11 - Eric&#39;s CS Notes
        
    </title>

    <!-- Spectre.css framework -->
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-exp.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-icons.min.css">

    <!-- theme css & js -->
    
<link rel="stylesheet" href="/css/book.css">

    
<script src="/js/book.js"></script>


    <!-- tocbot -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css">
    
    <!-- katex -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/zooming/2.1.1/zooming.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    const zooming = new Zooming()
    zooming.listen('.book-content img')
})
</script>

<meta name="generator" content="Hexo 4.2.0"></head>


<body>

<div class="book-container">
  <div class="book-sidebar">
    <div class="book-brand">
  <a href="/">
    <img src="/favicon.png">
    <span>ERIC&#39;S CS NOTES</span>
  </a>
</div>
    <div class="book-menu">
  <h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2>
<ul>
<li><a href="/symbols/index.html">Symbols of Mathematics</a></li>
<li><a href="/glossary/index.html">Glossary</a></li>
</ul>
<!--
## Stanford Statistical Learning

* [Chapter 2 - Overview of Statistical Learning](/statistical-learning/chapter-2/index.html)
* Chapter 3 - Linear Regression
* Chapter 4 - Classification
* Chapter 5 - Resampling Methods
* Chapter 6 - Linear Model Selection and Regularization
* Chapter 7 - Moving Beyond Linearity
* Chapter 8 - Tree-Based Methods
* Chapter 9 - Support Vector Machines
* Chapter 10 - Unsupervised Learning
-->
<h2 id="introduction-to-probability"><a class="markdownIt-Anchor" href="#introduction-to-probability"></a> Introduction to Probability</h2>
<ul>
<li><a href="/introduction-to-probability/unit-1/index.html">Unit 1: Probability models and axioms</a></li>
<li><a href="/introduction-to-probability/unit-2/index.html">Unit 2: Conditioning and independence</a></li>
<li><a href="/introduction-to-probability/unit-3/index.html">Unit 3: Counting</a></li>
</ul>
<!--
* [Unit 4: Discrete random variables](/introduction-to-probability/unit-4/index.html)
* [Unit 5: Continuous random variables](/introduction-to-probability/unit-5/index.html)
* [Unit 6: Further topics on random variables](/introduction-to-probability/unit-6/index.html)
* [Unit 7: Bayesian inference](/introduction-to-probability/unit-7/index.html)
* [Unit 8: Limit theorems and classical statistics](/introduction-to-probability/unit-8/index.html)
* [Unit 9: Bernoulli and Poisson processes](/introduction-to-probability/unit-9/index.html)
* [Unit 10: Markov chains](/introduction-to-probability/unit-10/index.html)
-->
<h2 id="multivariable-calculus"><a class="markdownIt-Anchor" href="#multivariable-calculus"></a> Multivariable Calculus</h2>
<ul>
<li><a href="/multivariable-calculus/unit-1/index.html">Unit 1: Thinking about multivariable functions</a></li>
<li><a href="/multivariable-calculus/unit-2/index.html">Unit 2: Derivatives of multivariable functions</a></li>
</ul>
<!--
* [Unit 3: Applications of multivariable derivatives](/multivariable-calculus/unit-3/index.html)
* [Unit 4: Integrating multivariable functions](/multivariable-calculus/unit-4/index.html)
* [Unit 5: Green's, Stokes', and the divergence theorems](/multivariable-calculus/unit-5/index.html)
-->
<h2 id="algorithms-part-ii"><a class="markdownIt-Anchor" href="#algorithms-part-ii"></a> Algorithms: Part II</h2>
<ul>
<li><a href="/algorithms-2/week-1/index.html">Week 1 - Undirected Graph &amp; Directed Graph</a></li>
<li><a href="/algorithms-2/week-3/index.html">Week 3 - Maximum Flow and Minimum Cut &amp; Radix Sort</a></li>
<li><a href="/algorithms-2/week-4/index.html">Week 4 - Tries &amp; Substring Search</a></li>
</ul>
<h2 id="algorithms-part-i"><a class="markdownIt-Anchor" href="#algorithms-part-i"></a> Algorithms: Part I</h2>
<ul>
<li><a href="/algorithms-1/week-1/index.html">Week 1 - Union-Find &amp; Analysis of Algorithms</a></li>
<li><a href="/algorithms-1/week-2/index.html">Week 2 - Stacks and Queues &amp; Elementary Sorts</a></li>
<li><a href="/algorithms-1/week-3/index.html">Week 3 - Mergesort &amp; Quicksort</a></li>
<li><a href="/algorithms-1/week-4/index.html">Week 4 - Priority Queues &amp; Elementary Symbols</a></li>
<li><a href="/algorithms-1/week-5/index.html">Week 5 - Balanced Search Trees</a></li>
<li><a href="/algorithms-1/week-6/index.html">Week 6 - Hash Tables</a></li>
</ul>
<h2 id="introduction-to-software-design-and-architecture"><a class="markdownIt-Anchor" href="#introduction-to-software-design-and-architecture"></a> Introduction to Software Design and Architecture</h2>
<ul>
<li><a href="/introduction-to-software-design-and-architecture/design-pattern/index.html">Design Pattern</a></li>
</ul>
<h2 id="calculus-two-sequences-and-series"><a class="markdownIt-Anchor" href="#calculus-two-sequences-and-series"></a> Calculus Two: Sequences and Series</h2>
<ul>
<li><a href="/calculus-two/week-1/index.html">Week 1 - Sequences</a></li>
<li><a href="/calculus-two/week-2/index.html">Week 2 - Series</a></li>
<li><a href="/calculus-two/week-3/index.html">Week 3 - Convergence Tests</a></li>
<li><a href="/calculus-two/week-4/index.html">Week 4 - Alternating Series</a></li>
<li><a href="/calculus-two/week-5/index.html">Week 5 - Power Series</a></li>
<li><a href="/calculus-two/week-6/index.html">Week 6 - Taylor Series</a></li>
</ul>
<h2 id="laff-linear-algebra"><a class="markdownIt-Anchor" href="#laff-linear-algebra"></a> LAFF Linear Algebra</h2>
<ul>
<li><a href="/laff-linear-algebra/week-1/index.html">Week 1 - Vectors in Linear Algebra</a></li>
<li><a href="/laff-linear-algebra/week-2/index.html">Week 2 - Linear Transformations and Matrices</a></li>
<li><a href="/laff-linear-algebra/week-3/index.html">Week 3 - Matrix-Vector Operations</a></li>
<li><a href="/laff-linear-algebra/week-4/index.html">Week 4 - Matrix-Vector to Matrix-Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-5/index.html">Week 5 - Matrix- Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-6/index.html">Week 6 - Gaussian Elimination</a></li>
<li><a href="/laff-linear-algebra/week-7/index.html">Week 7 - More Gaussian Elimination and Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-8/index.html">Week 8 - More on Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-9/index.html">Week 9 - Vector Spaces</a></li>
<li><a href="/laff-linear-algebra/week-10/index.html">Week 10 - Vector Spaces, Orthogonality, and Linear Least-Squares</a></li>
<li><a href="/laff-linear-algebra/week-11/index.html">Week 11 - Orthogonal Projection, Low Rank Approximation, and Orthogonal Bases</a></li>
<li><a href="/laff-linear-algebra/week-12/index.html">Week 12 - Eigenvalues and Eigenvectors</a></li>
</ul>
<h2 id="stanford-machine-learning"><a class="markdownIt-Anchor" href="#stanford-machine-learning"></a> Stanford Machine Learning</h2>
<ul>
<li><a href="/stanford-machine-learning/week-1/index.html">Week 1 - Introduction</a></li>
<li><a href="/stanford-machine-learning/week-2/index.html">Week 2 - Linear Regression with Multiple Variables</a></li>
<li><a href="/stanford-machine-learning/week-3/index.html">Week 3 - Logistic Regression &amp; Regularization</a></li>
<li><a href="/stanford-machine-learning/week-4/index.html">Week 4 - Neural Networks: Representation</a></li>
<li><a href="/stanford-machine-learning/week-5/index.html">Week 5 - Neural Networks: Learning</a></li>
<li><a href="/stanford-machine-learning/week-6a/index.html">Week 6a - Advice for Applying Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-6b/index.html">Week 6b - Machine Learning System Design</a></li>
<li><a href="/stanford-machine-learning/week-7/index.html">Week 7 - Support Vector Machines</a></li>
<li><a href="/stanford-machine-learning/week-8/index.html">Week 8 - Unsupervised Learning &amp; Dimensionality Reduction</a></li>
<li><a href="/stanford-machine-learning/week-9a/index.html">Week 9a - Anomaly Detection</a></li>
<li><a href="/stanford-machine-learning/week-9b/index.html">Week 9b - Recommender Systems</a></li>
<li><a href="/stanford-machine-learning/week-10/index.html">Week 10 - Large Scale Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-11/index.html">Week 11 - Application Example: Photo OCR</a></li>
</ul>
<h2 id="calculus-one"><a class="markdownIt-Anchor" href="#calculus-one"></a> Calculus One</h2>
<ul>
<li><a href="/calculus-one/week-2-3/index.html">Week 2-3 - Functions &amp; Limits</a></li>
<li><a href="/calculus-one/week-4/index.html">Week 4 - The Beginning of Derivatives</a></li>
<li><a href="/calculus-one/week-5/index.html">Week 5 - Techniques of Differentiation</a></li>
<li><a href="/calculus-one/week-6/index.html">Week 6 - Chain Rule</a></li>
<li><a href="/calculus-one/week-7/index.html">Week 7 - Derivatives of Trigonometric Functions</a></li>
<li><a href="/calculus-one/week-8/index.html">Week 8 - Derivatives in the Real World</a></li>
<li><a href="/calculus-one/week-9/index.html">Week 9 - Optimization</a></li>
<li><a href="/calculus-one/week-10/index.html">Week 10 - Linear Approximation</a></li>
<li><a href="/calculus-one/week-11-12/index.html">Week 11-12 - Antidifferentiation &amp; Integration</a></li>
<li><a href="/calculus-one/week-13/index.html">Week 13 - Fundamental Theorem of Calculus</a></li>
<li><a href="/calculus-one/week-14/index.html">Week 14 - Substitution Rule</a></li>
<li><a href="/calculus-one/week-15/index.html">Week 15 - Techniques of Integration</a></li>
<li><a href="/calculus-one/week-16/index.html">Week 16 - Applications of Integration</a></li>
</ul>
<h2 id="computational-thinking"><a class="markdownIt-Anchor" href="#computational-thinking"></a> Computational Thinking</h2>
<ul>
<li><a href="/computational-thinking/lecture-1/index.html">Lecture 1 - Optimization and Knapsack Problem</a></li>
<li><a href="/computational-thinking/lecture-2/index.html">Lecture 2 - Decision Trees and Dynamic Programming</a>
<ul>
<li><a href="/computational-thinking/lecture-2-powerset/index.html">Exercise: Power Set Function</a></li>
</ul>
</li>
<li><a href="/computational-thinking/lecture-3/index.html">Lecture 3 - Graphs</a></li>
<li><a href="/computational-thinking/lecture-4-5/index.html">Lecture 4-5 - Plotting</a></li>
<li><a href="/computational-thinking/lecture-6-7/index.html">Lecture 6-7 - Stochastic Programs &amp; Inferential Statistics</a></li>
<li><a href="/computational-thinking/lecture-8/index.html">Lecture 8 - Monte Carlo Simulation</a></li>
<li><a href="/computational-thinking/lecture-9/index.html">Lecture 9 - Sampling and Standard Error</a></li>
<li><a href="/computational-thinking/lecture-10-11/index.html">Lecture 10-11 - Experimental Data</a></li>
<li><a href="/computational-thinking/lecture-12/index.html">Lecture 12 - Machine Learning</a></li>
<li><a href="/computational-thinking/lecture-13/index.html">Lecture 13 - Statistical Abuses</a></li>
</ul>
<h2 id="effective-thinking-through-mathematics"><a class="markdownIt-Anchor" href="#effective-thinking-through-mathematics"></a> Effective Thinking Through Mathematics</h2>
<ul>
<li><a href="/effective-thinking-through-mathematics/note/index.html">Note</a></li>
<li><a href="/effective-thinking-through-mathematics/week-4-telling-the-story-of-infinity/index.html">Week 4 (/Telling the Story of Infinity)</a></li>
<li><a href="/effective-thinking-through-mathematics/week-5-telling-the-story-of-the-euler-circuit-theorem/index.html">Week 5 (/Telling the Story of Euler Circuit Theorem)</a></li>
</ul>
<h2 id="cs50-introduction-to-computer-science"><a class="markdownIt-Anchor" href="#cs50-introduction-to-computer-science"></a> CS50 Introduction to Computer Science</h2>
<ul>
<li><a href="/cs50/week-1/index.html">Week 1 - C</a></li>
<li><a href="/cs50/week-2/index.html">Week 2 - Arrays</a></li>
<li><a href="/cs50/week-3/index.html">Week 3 - Algorithms</a></li>
<li><a href="/cs50/week-4/index.html">Week 4 - Memory</a></li>
<li><a href="/cs50/week-5/index.html">Week 5 - Data Structures</a></li>
<li><a href="/cs50/week-6/index.html">Week 6 - HTTP</a></li>
<li><a href="/cs50/week-7-10/index.html">Week 7-10 - Machine Learning/Python/SQL/Javascript</a></li>
</ul>

</div>


<script src="/js/book-menu.js"></script>

  </div>

  <div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseover="add_inner()" onmouseleave="remove_inner()">
  <div class="sidebar-toggle-inner"></div>
</div>

<script>
function add_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.add('show')  
}

function remove_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.remove('show')
}

function sidebar_toggle() {
    let sidebar_toggle = document.querySelector('.sidebar-toggle')
    let sidebar = document.querySelector('.book-sidebar')
    let content = document.querySelector('.off-canvas-content')
    if (sidebar_toggle.classList.contains('extend')) { // show
        sidebar_toggle.classList.remove('extend')
        sidebar.classList.remove('hide')
        content.classList.remove('extend')
    }
    else { // hide
        sidebar_toggle.classList.add('extend')
        sidebar.classList.add('hide')
        content.classList.add('extend')
    }
}
</script>

  <div class="off-canvas-content">
    <div class="columns">
      <div class="column col-10 col-lg-12">
        <div class="book-navbar">
          <!-- For Responsive Layout -->

<header class="navbar">
  <section class="navbar-section">
    <a onclick="open_sidebar()">
      <i class="icon icon-menu"></i>
    </a>
  </section>
</header>

        </div>
        <div class="book-content">
          <div class="book-post">
  <h1 id="lecture-10-11"><a class="markdownIt-Anchor" href="#lecture-10-11"></a> Lecture 10 - 11</h1>
<h2 id="experimental-data"><a class="markdownIt-Anchor" href="#experimental-data"></a> Experimental Data</h2>
<ul>
<li>These lectures are talking about the interplay between statistics and  experimental science. And how to get a valid statistical conclusion.</li>
</ul>
<h3 id="the-behavior-of-springs"><a class="markdownIt-Anchor" href="#the-behavior-of-springs"></a> The Behavior of Springs</h3>
<ul>
<li>
<p><strong>Hooke’s law</strong> of elasticity: <code>F = -kx</code></p>
<ul>
<li>In other words, the force, <code>F</code>, stored in a spring is linearly related to the distance, <code>x</code>, the spring has been compressed(or stretched).</li>
<li><code>k</code> is called the spring constant.</li>
<li>All springs have an <strong>elastic limit</strong>, beyond which the law fails.</li>
</ul>
</li>
<li>
<p>For example, How much does a rider have to weigh to compress the spring  on motorbike 1cm? (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>≈</mo><mn>35</mn><mo separator="true">,</mo><mn>000</mn><mi>N</mi><mi mathvariant="normal">/</mi><mi>m</mi></mrow><annotation encoding="application/x-tex">k \approx 35,000 N/m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">3</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mord">/</span><span class="mord mathdefault">m</span></span></span></span>)</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>=</mo><mn>0.01</mn><mi>m</mi><mo>∗</mo><mn>35</mn><mo separator="true">,</mo><mn>000</mn><mi>N</mi><mi mathvariant="normal">/</mi><mi>m</mi></mrow><annotation encoding="application/x-tex">F = 0.01m * 35,000 N/m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mord">1</span><span class="mord mathdefault">m</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">3</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mord">/</span><span class="mord mathdefault">m</span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>=</mo><mn>350</mn><mi>N</mi></mrow><annotation encoding="application/x-tex">F = 350 N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">3</span><span class="mord">5</span><span class="mord">0</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>=</mo><mi>m</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo>∗</mo><mi>a</mi><mi>c</mi><mi>c</mi></mrow><annotation encoding="application/x-tex">F = mass * acc</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.46528em;vertical-align:0em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">s</span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span><span class="mord mathdefault">c</span><span class="mord mathdefault">c</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>=</mo><mi>m</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo>∗</mo><mn>9.81</mn><mi>m</mi><mi mathvariant="normal">/</mi><msup><mi>s</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">F = mass * 9.81m/s^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.46528em;vertical-align:0em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">s</span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord">9</span><span class="mord">.</span><span class="mord">8</span><span class="mord">1</span><span class="mord mathdefault">m</span><span class="mord">/</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo>∗</mo><mn>9.81</mn><mi>m</mi><mi mathvariant="normal">/</mi><msup><mi>s</mi><mn>2</mn></msup><mo>=</mo><mn>350</mn><mi>N</mi></mrow><annotation encoding="application/x-tex">mass * 9.81m/s^2 = 350N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.46528em;vertical-align:0em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">s</span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord">9</span><span class="mord">.</span><span class="mord">8</span><span class="mord">1</span><span class="mord mathdefault">m</span><span class="mord">/</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">3</span><span class="mord">5</span><span class="mord">0</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo>≈</mo><mn>35.68</mn><mi>k</mi><mi>g</mi></mrow><annotation encoding="application/x-tex">mass \approx 35.68kg</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.48312em;vertical-align:0em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">s</span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord">3</span><span class="mord">5</span><span class="mord">.</span><span class="mord">6</span><span class="mord">8</span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span></span></span></span></li>
</ul>
</li>
<li>
<p>Finding <code>k</code></p>
<ul>
<li>
<p>We can’t conduct the experiment perfectly, so, to hang a series of increasingly heavier weights on the spring, measure the stretch of the spring each time, and plot the results will be a better way.</p>
</li>
<li>
<p>First we should know:</p>
<ul>
<li><code>F=-kx</code></li>
<li><code>k=-F/x</code></li>
<li><code>k=9.81*m/x</code></li>
<li>
<img src="https://i.imgur.com/8cGM01y.jpg" style="width:200px" />
</li>
</ul>
</li>
<li>
<p>Then, we got some data</p>
   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">0.0865 0.1</span><br><span class="line">0.1015 0.15</span><br><span class="line">...</span><br><span class="line">0.4263 0.65</span><br><span class="line">0.4562 0.7</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>Plot the data, we got:</p>
<ul>
<li>
<img src="https://i.imgur.com/ff83AbS.jpg" style="width:250px" />
</li>
<li>It has some measurement errors.</li>
</ul>
</li>
<li>
<p>Next step is to fit curves to data.</p>
<ul>
<li>
<p>When we fit a curve to a set of data, we are finding a fit that relates an independent variable (the mass) to an estimated value of a dependent variable (the distance)</p>
</li>
<li>
<p>To fit curves to data, we need to define an <strong>objective function</strong> that provides a quantitative assessment of how well the curve fits the data. In this case, a straight line, that is <strong>linear function</strong>.</p>
</li>
<li>
<p>Then, finding the best fit is an optimization problem. most commonly used <strong>objective function</strong> is called <strong>least squares</strong>:</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>l</mi><mi>e</mi><mi>n</mi><mo stretchy="false">(</mo><mi>o</mi><mi>b</mi><mi>s</mi><mi>e</mi><mi>r</mi><mi>v</mi><mi>e</mi><mi>d</mi><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn></mrow></munderover><mo stretchy="false">(</mo><mi>o</mi><mi>b</mi><mi>s</mi><mi>e</mi><mi>r</mi><mi>v</mi><mi>e</mi><mi>d</mi><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo>−</mo><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi><mi>e</mi><mi>d</mi><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mstyle></mrow><annotation encoding="application/x-tex">\displaystyle\sum_{i=0}^{len(observed)-1}(observed[i] - predicted[i])^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.2386740000000005em;vertical-align:-1.277669em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.9610050000000003em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.386005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">n</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">b</span><span class="mord mathdefault mtight">s</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">d</span><span class="mclose mtight">)</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">o</span><span class="mord mathdefault">b</span><span class="mord mathdefault">s</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mord mathdefault">e</span><span class="mord mathdefault">d</span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord mathdefault">p</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">e</span><span class="mord mathdefault">d</span><span class="mord mathdefault">i</span><span class="mord mathdefault">c</span><span class="mord mathdefault">t</span><span class="mord mathdefault">e</span><span class="mord mathdefault">d</span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mclose">]</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></li>
</ul>
</li>
<li>
<p>Next is to use a successive approximation algorithm to find the best least-squares fit. <strong>PyLab</strong> provides a built-in function, <strong>polyfit</strong>:</p>
   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pylab.polyfit(observedXVals, observedYVals, n)</span><br></pre></td></tr></table></figure>
<ul>
<li>this function finds the coefficients of a polynomial of degree <code>n</code> that provides a best least-squares fit for the set of points defined by the arrays observedXVals and observedYVals.</li>
<li>The algorithm used by polyfit is called <strong>linear regression</strong>.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Visualizing the Fit</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getData</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    dataFile = open(fileName, <span class="string">'r'</span>)</span><br><span class="line">    distances = []</span><br><span class="line">    masses = []</span><br><span class="line">    dataFile.readline() <span class="comment">#discard header</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> dataFile:</span><br><span class="line">        d, m = line.split()</span><br><span class="line">        distances.append(float(d))</span><br><span class="line">        masses.append(float(m))</span><br><span class="line">    dataFile.close()</span><br><span class="line">    <span class="keyword">return</span> (masses, distances)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">labelPlot</span><span class="params">()</span>:</span></span><br><span class="line">    pylab.title(<span class="string">'Measured Displacement of Spring'</span>)</span><br><span class="line">    pylab.xlabel(<span class="string">'|Force| (Newtons)'</span>)</span><br><span class="line">    pylab.ylabel(<span class="string">'Distance (meters)'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotData</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    xVals, yVals = getData(fileName)</span><br><span class="line">    xVals = pylab.array(xVals)</span><br><span class="line">    yVals = pylab.array(yVals)</span><br><span class="line">    xVals = xVals*<span class="number">9.81</span>  <span class="comment">#acc. due to gravity</span></span><br><span class="line">    pylab.plot(xVals, yVals, <span class="string">'bo'</span>,</span><br><span class="line">               label = <span class="string">'Measured displacements'</span>)</span><br><span class="line">    labelPlot()</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fitData</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    xVals, yVals = getData(fileName)</span><br><span class="line">    xVals = pylab.array(xVals)</span><br><span class="line">    yVals = pylab.array(yVals)</span><br><span class="line">    xVals = xVals*<span class="number">9.81</span> <span class="comment">#get force</span></span><br><span class="line">    pylab.plot(xVals, yVals, <span class="string">'bo'</span>,</span><br><span class="line">               label = <span class="string">'Measured points'</span>)</span><br><span class="line">    labelPlot()                 </span><br><span class="line">    a,b = pylab.polyfit(xVals, yVals, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># estYVals = a*xVals + b</span></span><br><span class="line">    estYVals = pylab.polyval(model, xVals)</span><br><span class="line">    print(<span class="string">'a ='</span>, a, <span class="string">'b ='</span>, b)</span><br><span class="line">    pylab.plot(xVals, estYVals, <span class="string">'r'</span>,</span><br><span class="line">               label = <span class="string">'Linear fit, k = '</span></span><br><span class="line">               + str(round(<span class="number">1</span>/a, <span class="number">5</span>)))</span><br><span class="line">    pylab.legend(loc = <span class="string">'best'</span>)</span><br><span class="line">    </span><br><span class="line">fitData(<span class="string">'springData.txt'</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>
<img src="https://i.imgur.com/XGEOx1N.jpg" style="width:250px" />
</li>
<li>
<p><code>k = 21.53686</code></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="another-experiment"><a class="markdownIt-Anchor" href="#another-experiment"></a> Another Experiment</h3>
<ul>
<li>
<p>to fit curves to these mystery data:</p>
<ul>
<li>
<img src="https://i.imgur.com/AdHxTCO.jpg" style="width:250px" />
</li>
<li>
<img src="https://i.imgur.com/q0aeuY3.jpg" style="width:250px" />
</li>
</ul>
</li>
<li>
<p>how good are these fits</p>
<ul>
<li>
<p>we can see that quadratic model is better than linear model, but how bad a fit is the line and how good is the quadratic fit?</p>
</li>
<li>
<p>comparing <strong>mean squared error</strong></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aveMeanSquareError</span><span class="params">(data, predicted)</span>:</span></span><br><span class="line">    error = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data)):</span><br><span class="line">        error += (data[i] - predicted[i])**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> error/len(data)</span><br></pre></td></tr></table></figure>
<ul>
<li>then we got:
<ul>
<li>Ave. mean square error for linear model = 9372.73078965</li>
<li>Ave. mean square error for quadratic model = 1524.02044718</li>
</ul>
</li>
<li>Seems like, quadratic model is better than linear model. But we still have to ask, is the quadratic fit good in an absolute sense?</li>
<li>The mean square error is useful for comparing two different models of the same data, but it’s not actually very useful for getting the absolute goodness of fit.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="coefficient-of-determination-r2"><a class="markdownIt-Anchor" href="#coefficient-of-determination-r2"></a> Coefficient of Determination ( R^2 )</h3>
<ul>
<li>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mrow><msub><mo>∑</mo><mi>i</mi></msub><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>p</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><mrow><msub><mo>∑</mo><mi>i</mi></msub><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">R^{2}=1-\frac{\sum_{i}(y_i-p_i)^2}{\sum_{i}(y_i-\mu_i)^2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.728934em;vertical-align:-0.5700069999999999em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.158927em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1496471428571428em;"><span style="top:-2.1785614285714283em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.32143857142857146em;"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathdefault mtight">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight"><span class="mclose mtight">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463142857142857em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.5350070000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1496471428571428em;"><span style="top:-2.1785614285714283em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.32143857142857146em;"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight"><span class="mclose mtight">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913142857142857em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5700069999999999em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<ul>
<li>By comparing the estimation errors (the numerator) with the variability of the original values (the denominator), R^2 is intended to capture the proportion of variability in a data set that is accounted for by the statistical model provided by the fit</li>
<li>Always between 0 and 1 when fit generated by a linear regression and tested on training data</li>
<li>If R^2 = 1, the model explains all of the variability in the data. If R^2 = 0, there is no relationship between the values predicted by the model and the actual data.</li>
</ul>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rSquared</span><span class="params">(observed, predicted)</span>:</span></span><br><span class="line"> error = ((predicted - observed)**<span class="number">2</span>).sum()</span><br><span class="line"> meanError = error/len(observed)</span><br><span class="line"> <span class="keyword">return</span> <span class="number">1</span> - (meanError/numpy.var(observed))</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>Testing Goodness of Fits</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">genFits</span><span class="params">(xVals, yVals, degrees)</span>:</span></span><br><span class="line"> models = []</span><br><span class="line"> <span class="keyword">for</span> d <span class="keyword">in</span> degrees:</span><br><span class="line">     model = pylab.polyfit(xVals, yVals, d)</span><br><span class="line">     models.append(model)</span><br><span class="line"> <span class="keyword">return</span> models</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testFits</span><span class="params">(models, degrees, xVals, yVals, title)</span>:</span></span><br><span class="line"> pylab.plot(xVals, yVals, <span class="string">'o'</span>, label = <span class="string">'Data'</span>)</span><br><span class="line"> <span class="keyword">for</span> i <span class="keyword">in</span> range(len(models)):</span><br><span class="line">     estYVals = pylab.polyval(models[i], xVals)</span><br><span class="line">     error = rSquared(yVals, estYVals)</span><br><span class="line">     pylab.plot(xVals, estYVals,</span><br><span class="line">                label = <span class="string">'Fit of degree '</span>\</span><br><span class="line">                + str(degrees[i])\</span><br><span class="line">                + <span class="string">', R2 = '</span> + str(round(error, <span class="number">5</span>)))</span><br><span class="line"> pylab.legend(loc = <span class="string">'best'</span>)</span><br><span class="line"> pylab.title(title)</span><br><span class="line">  </span><br><span class="line">xVals, yVals = getData(<span class="string">'mysteryData.txt'</span>)</span><br><span class="line">degrees = (<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">models = genFits(xVals, yVals, degrees)</span><br><span class="line">testFits(models, degrees, xVals, yVals, <span class="string">'Mystery Data'</span>)</span><br></pre></td></tr></table></figure>
  <img src="https://i.imgur.com/4wim8KX.jpg" style="width:250px" />
<ul>
<li>Quadratic model get 84%, and Linear get almost 0%.</li>
<li>Since the degree of polynomial can affect the Goodness of Fits, what if we use some bigger ones? Can we get a tighter Fit?
<ul>
<li>
<img src="https://i.imgur.com/Jr5Pi74.jpg" style="width:250px" />
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="a-tighter-fit-may-not-be-we-need-so-how-to-choose"><a class="markdownIt-Anchor" href="#a-tighter-fit-may-not-be-we-need-so-how-to-choose"></a> A Tighter Fit May Not Be We Need, So How to Choose?</h3>
<ul>
<li>
<p>Why We Build Models</p>
<ul>
<li>Help us understand the process that generated the data</li>
<li>Help us make predictions about out-of-sample data</li>
<li>A good model helps us do these things</li>
</ul>
</li>
<li>
<p>How Mystery Data Was Generated</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def genNoisyParabolicData(a, b, c, xVals, fName):</span><br><span class="line">    yVals &#x3D; []</span><br><span class="line">    for x in xVals:</span><br><span class="line">        theoreticalVal &#x3D; a*x**2 + b*x + c</span><br><span class="line">        yVals.append(theoreticalVal + random.gauss(0, 35))</span><br><span class="line">    f &#x3D; open(fName,&#39;w&#39;)</span><br><span class="line">    f.write(&#39;y        x\n&#39;)</span><br><span class="line">    for i in range(len(yVals)):</span><br><span class="line">        f.write(str(yVals[i]) + &#39; &#39; + str(xVals[i]) + &#39;\n&#39;)</span><br><span class="line">    f.close()</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>Let’s Look at Two Data Sets</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#parameters for generating data</span></span><br><span class="line">xVals = range(<span class="number">-10</span>, <span class="number">11</span>, <span class="number">1</span>)</span><br><span class="line">a, b, c = <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">degrees = (<span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#generate data</span></span><br><span class="line">random.seed(<span class="number">0</span>)</span><br><span class="line">genNoisyParabolicData(a, b, c, xVals, <span class="string">'Dataset 1.txt'</span>)</span><br><span class="line"></span><br><span class="line">genNoisyParabolicData(a, b, c, xVals, <span class="string">'Dataset 2.txt'</span>)</span><br><span class="line"></span><br><span class="line">xVals1, yVals1 = getData(<span class="string">'Dataset 1.txt'</span>)</span><br><span class="line">models1 = genFits(xVals1, yVals1, degrees)</span><br><span class="line">testFits(models1, degrees, xVals1, yVals1, <span class="string">'DataSet 1.txt'</span>)</span><br><span class="line"></span><br><span class="line">pylab.figure()</span><br><span class="line">xVals2, yVals2 = getData(<span class="string">'Dataset 2.txt'</span>)</span><br><span class="line">models2 = genFits(xVals2, yVals2, degrees)</span><br><span class="line">testFits(models2, degrees, xVals2, yVals2, <span class="string">'DataSet 2.txt'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://i.imgur.com/kw5Ju19.jpg" style="width:250px" /> <img src="https://i.imgur.com/ptvPwi5.jpg" style="width:250px" /></p>
</li>
<li>
<p>Hence Degree 16 Is Tightest Fit, But</p>
<ul>
<li>We want model to work well on other data generated by the same process</li>
<li>It needs to generalize</li>
</ul>
</li>
<li>
<p><strong>Cross Validate</strong></p>
<ul>
<li>Generate models using one dataset, and then test it on the other
<ul>
<li>Use models for Dataset 1 to predict points for Dataset 2</li>
<li>Use models for Dataset 2 to predict points for Dataset 1</li>
</ul>
</li>
<li>Expect testing error to be larger than training error</li>
<li>A better indication of generalizability than training error</li>
</ul>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pylab.figure()</span><br><span class="line">testFits(models1, degrees, xVals2, yVals2, <span class="string">'DataSet 2/Model 1'</span>)</span><br><span class="line">pylab.figure()</span><br><span class="line">testFits(models2, degrees, xVals1, yVals1, <span class="string">'DataSet 1/Model 2'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://i.imgur.com/2LmtbF0.jpg" style="width:250px" /> <img src="https://i.imgur.com/yh2kz3R.jpg" style="width:250px" /></p>
</li>
<li>
<p>Increasing the Complexity</p>
<ul>
<li>coefficient may be zero if Fitting a quadratic to a perfect line
<ul>
<li>for example one:
<ul>
<li>data: [{0, 0}, {1, 1}, {2, 2}, {3, 3}]</li>
<li>Fit <code>y=ax^2+bx+c</code>, will get <code>y=0x^2+1x+0</code>, which is <code>y=x</code></li>
</ul>
</li>
<li>for example two:
<ul>
<li>data: [{0, 0}, {1, 1}, {2, 2}, {3, 3.1}]</li>
<li>Fit <code>y=ax^2+bx+c</code>, will get <code>y=0.025x^2+0.955x+0.005</code>, which is <code>y=x</code></li>
</ul>
</li>
</ul>
</li>
<li>if data is noisy, can fit the noise rather than the underlying pattern in the data?
<ul>
<li>for example:
<ul>
<li>data: [{0, 0}, {1, 1}, {2, 2}, {3, 3}, {4, 20}]</li>
<li>Fit <code>y=ax^2+bx+c</code>, we got:
<ul>
<li>
<img src="https://i.imgur.com/QYRlNdg.jpg" style="width:250px" />
</li>
<li>R-squared = 0.7026</li>
</ul>
</li>
<li>suppose we had used a first-degree fit
<ul>
<li><code>model = pylab.polyfit(xVals, yVals, 1)</code></li>
<li>
<img src="https://i.imgur.com/WMhH0oF.jpg" style="width:250px" />
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Conclusion</p>
<ul>
<li>Choosing an overly-complex model can leads to <strong>overfitting</strong> to the training data</li>
<li>Increases the risk of a model that works poorly on data not included in the training set</li>
<li>On the other hand choosing an insufficiently complex model has other problems
<ul>
<li>when we fit a line to data that was basically parabolic</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="suppose-we-dont-have-a-solid-theory-to-validate-model"><a class="markdownIt-Anchor" href="#suppose-we-dont-have-a-solid-theory-to-validate-model"></a> Suppose We Don’t Have a Solid Theory to Validate Model</h3>
<ul>
<li>Use <strong>cross-validation</strong> results to guide the choice of model complexity</li>
<li>If dataset small, use leave-one-out cross validation</li>
<li>If dataset large enough, use k-fold cross validation or repeated-random-sampling validation</li>
</ul>
<h4 id="leave-one-out-cross-validation"><a class="markdownIt-Anchor" href="#leave-one-out-cross-validation"></a> Leave-one-out Cross Validation</h4>
<ul>
<li>
<p>Let <code>D</code> be the original data set</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">testResults &#x3D; []</span><br><span class="line"></span><br><span class="line">for i in range(len(D)): </span><br><span class="line">    training &#x3D; D[:].pop(i)</span><br><span class="line">    model &#x3D; buildModel(training)</span><br><span class="line">    testResults.append(test(model, D[i]))</span><br><span class="line">Average testResults</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="repeated-random-sampling"><a class="markdownIt-Anchor" href="#repeated-random-sampling"></a> Repeated Random Sampling</h4>
<ul>
<li>
<p>Let <code>D</code> be the original data set</p>
</li>
<li>
<p>Let <code>n</code> be the number of random samples</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">testResults &#x3D; []</span><br><span class="line">for i in range(n)</span><br><span class="line">    randomly partition D into two sets:</span><br><span class="line">    training and test</span><br><span class="line">    model &#x3D; buildModel(training)</span><br><span class="line">    testResults.append(test(model, test))</span><br><span class="line">Average testResults</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="an-example-temperature-by-year"><a class="markdownIt-Anchor" href="#an-example-temperature-by-year"></a> An Example, Temperature By Year</h4>
<ul>
<li>
<p>Task: Model how the mean daily high temperature in the U.S. varied from 1961 through 2015</p>
<ul>
<li>Get means for each year and plot them</li>
<li>Randomly divide data in half n times
<ul>
<li>For each dimensionality to be tried
<ul>
<li>Train on one half of data</li>
<li>Test on other half</li>
<li>Record r-squared on test data</li>
</ul>
</li>
</ul>
</li>
<li>Report mean r-squared for each dimensionality</li>
<li>Code: <a href="https://github.com/erictt/computer-science-learning/blob/master/computational-thinking/unit-4/lecture11-3.py" target="_blank" rel="noopener">lecture11-3.py</a></li>
</ul>
</li>
<li>
<p>The Whole Data Set:</p>
<ul>
<li>
<img src="https://i.imgur.com/oAmUXUr.jpg" style="width:250px" />
</li>
</ul>
</li>
<li>
<p>Results:</p>
<ul>
<li>Mean R-squares for test data
<ul>
<li>For dimensionality 1 mean = 0.7535 Std = 0.0656</li>
<li>For dimensionality 2 mean = 0.7291 Std = 0.0744</li>
<li>For dimensionality 3 mean = 0.7039 Std = 0.0684</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Line seems to be the winner</p>
<ul>
<li>Highest average r-squared</li>
<li>Simplest model</li>
</ul>
</li>
</ul>
<h3 id="wrapping-up-curve-fitting"><a class="markdownIt-Anchor" href="#wrapping-up-curve-fitting"></a> Wrapping Up Curve Fitting</h3>
<ul>
<li>We can use linear regression to fit a curve to data
<ul>
<li>Mapping from independent values to dependent values</li>
</ul>
</li>
<li>That curve is a model of the data that can be used to predict the value associated with independent values we haven’t seen (out of sample data)</li>
<li>R-squared used to evaluate model
<ul>
<li>Higher not always “better” because of risk of over fitting</li>
</ul>
</li>
<li>Choose complexity of model based on
<ul>
<li>Theory about structure of data</li>
<li>Cross validation</li>
<li>Simplicity</li>
</ul>
</li>
</ul>

</div>


  <div class="book-comments">
    




  </div>



<script src="/js/book-post.js"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>


        </div>
      </div>
      <div class="column col-2 hide-lg">
        <div class="book-post-info">
  
    <div class="book-post-meta">

  <div class="author">

    <!-- Author image -->
    <div class="author-img">
      
        <figure
          class="avatar avatar-lg"
          data-initial="E"
          style="background-color: #3b4351;">
        </figure>
      
    </div>

    <!-- Author title -->
    <div class="author-title">
      <div>Eric Yang</div>
      <div>2020-04-17</div>
    </div>
  </div>

  

  <div class="divider"></div>
</div>
  

  <div class="book-tocbot">
</div>
<div class="book-tocbot-menu">
  <a class="book-toc-expand" onclick="expand_toc()">Expand all</a>
  <a onclick="go_top()">Back to top</a>
  <a onclick="go_bottom()">Go to bottom</a>
</div>


<script src="/js/book-toc.js"></script>

</div>

      </div>
    </div>
  </div>
  
  <a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>

</body>
</html>


<script src="/js/book.js"></script>
