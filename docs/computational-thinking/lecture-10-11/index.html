<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">

    
      <link rel="icon" href="/favicon.png" />
    

    <title>
        
          Eric&#39;s CS Notes
        
    </title>

    <!-- Spectre.css framework -->
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-exp.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-icons.min.css">

    <!-- theme css & js -->
    
<link rel="stylesheet" href="/css/book.css">

    
<script src="/js/book.js"></script>


    <!-- tocbot -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.18.2/tocbot.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.18.2/tocbot.css">
    
    <!-- katex css -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css" integrity="sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/" crossorigin="anonymous">

    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/zooming/2.1.1/zooming.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    const zooming = new Zooming()
    zooming.listen('.book-content img')
})
</script>

<meta name="generator" content="Hexo 6.3.0"></head>


<body>

<div class="book-container">
  <div class="book-sidebar">
    <div class="book-brand">
  <a href="/">
    <img src="/favicon.png">
    <span>ERIC&#39;S CS NOTES</span>
  </a>
</div>

    <div class="book-menu">
  <!--
## Introduction to Probability

* [Unit 1: Probability models and axioms](/introduction-to-probability/unit-1/index.html)
* [Unit 2: Conditioning and independence](/introduction-to-probability/unit-2/index.html)
* [Unit 3: Counting](/introduction-to-probability/unit-3/index.html)
* [Unit 4: Discrete random variables](/introduction-to-probability/unit-4/index.html)
* [Unit 5: Continuous random variables](/introduction-to-probability/unit-5/index.html)
* [Unit 6: Further topics on random variables](/introduction-to-probability/unit-6/index.html)
* [Unit 7: Bayesian inference](/introduction-to-probability/unit-7/index.html)
* [Unit 8: Limit theorems and classical statistics](/introduction-to-probability/unit-8/index.html)
* [Unit 9: Bernoulli and Poisson processes](/introduction-to-probability/unit-9/index.html)
* [Unit 10: Markov chains](/introduction-to-probability/unit-10/index.html)
-->
<!--
## Multivariable Calculus

* [Unit 1: Thinking about multivariable functions](/multivariable-calculus/unit-1/index.html)
* [Unit 2: Derivatives of multivariable functions](/multivariable-calculus/unit-2/index.html)

* [Unit 3: Applications of multivariable derivatives](/multivariable-calculus/unit-3/index.html)
* [Unit 4: Integrating multivariable functions](/multivariable-calculus/unit-4/index.html)
* [Unit 5: Green's, Stokes', and the divergence theorems](/multivariable-calculus/unit-5/index.html)
-->
<h2 id="CS6210-Advanced-Operating-Systems">CS6210: Advanced Operating Systems</h2>
<ul>
<li><a href="/cs6210/lesson-02/index.html">Lesson 02: OS Structure</a></li>
<li><a href="/cs6210/lesson-03/index.html">Lesson 3: Virtualization</a></li>
<li><a href="/cs6210/lesson-04-1/index.html">Lesson 4: Parallel Systems - Part 1</a></li>
<li><a href="/cs6210/lesson-04-2/index.html">Lesson 4: Parallel Systems - Part 2</a></li>
<li><a href="/cs6210/lesson-05/index.html">Lesson 5: Distributed Systems</a></li>
<li><a href="/cs6210/lesson-06/index.html">Lesson 6: Distributed Objects and Middleware</a></li>
<li><a href="/cs6210/lesson-07a/index.html">Lesson 7a: Distributed Subsystems - GMS</a></li>
<li><a href="/cs6210/lesson-07b/index.html">Lesson 7b: Distributed Subsystems - DSM</a></li>
</ul>
<!--
* [Lesson 7c: Distributed Subsystems - DFS](/cs6210/lesson-07c/index.html)
* [Lesson 9: Internet Computing](/cs6210/lesson-09/index.html)
* [Lesson 10: RT and Multimedia](/cs6210/lesson-10/index.html)
* [Lesson 8: Failures and Recovery](/cs6210/lesson-08/index.html)
* [Lesson 11: Security](/cs6210/lesson-11/index.html)
-->
<h2 id="CS6250-Computer-Networks">CS6250 Computer Networks</h2>
<ul>
<li><a href="/cs6250/week-1-internet-architecture/index.html">Week 1 - Internet Architecture</a></li>
<li><a href="/cs6250/week-2-transport-and-application-layers/index.html">Week 2 - Transport and Application Layers</a></li>
<li><a href="/cs6250/week-3-intradomain-routing/index.html">Week 3 - Intradomain Routing</a></li>
<li><a href="/cs6250/week-4-as-relationships-and-interdomain-routing/index.html">Week 4 - AS Relationships and Interdomain Routing</a></li>
<li><a href="/cs6250/week-5-router-design-and-algorithems-part-1/index.html">Week 5 - Router Design and Algorithms (Part 1)</a></li>
<li><a href="/cs6250/week-6-router-design-and-algorithems-part-2/index.html">Week 6 - Router Design and Algorithms (Part 2)</a></li>
<li><a href="/cs6250/week-7-software-defined-networking-part-1/index.html">Week 7 - Software Defined Networking (Part 1)</a></li>
<li><a href="/cs6250/week-8-software-defined-networking-part-2/index.html">Week 8 - Software Defined Networking (Part 2)</a></li>
<li><a href="/cs6250/week-9-internet-security/index.html">Week 9 - Internet Security</a></li>
<li><a href="/cs6250/week-10-internet-surveillance-and-censorship/index.html">Week 10 - Internet Surveillance and Censorship</a></li>
<li><a href="/cs6250/week-11-applications-video/index.html">Week 11 - Applications Videos</a></li>
<li><a href="/cs6250/week-12-applications-cdns-and-overlay-networks/index.html">Week 12 - Applications CDNs and Overlay Networks</a></li>
</ul>
<h2 id="CS6200-Graduate-Introduction-to-Operating-Systems">CS6200 Graduate Introduction to Operating Systems</h2>
<ul>
<li><a href="/cs6200/p1-preparation/index.html">P0 - Preparation</a></li>
<li><a href="/cs6200/p1l2-introduction/index.html">P1L2 - Introduction</a></li>
<li><a href="/cs6200/p2l1-processes-and-process-management/index.html">P2L1 - Processes and Process Management</a></li>
<li><a href="/cs6200/p2l2-threads-and-concurrency/index.html">P2L2 - Threads and Concurrency</a></li>
<li><a href="/cs6200/p2l3-pthread/index.html">P2L3 - PThread</a></li>
<li><a href="/cs6200/p2l4-thread-design-consideration/index.html">P2L4 - Thread Design Considerations</a></li>
<li><a href="/cs6200/p2l5-thread-performance-consideration/index.html">P2L5 - Thread Performance Considerations</a></li>
<li><a href="/cs6200/p3l1-scheduling/index.html">P3L1 - Scheduling</a></li>
<li><a href="/cs6200/p3l2-memory-management/index.html">P3L2 - Memory Management</a></li>
<li><a href="/cs6200/p3l3-inter-process-communication/index.html">P3L3 - Inter-Process Communication</a></li>
<li><a href="/cs6200/p3l4-synchronization-constructs/index.html">P3L4 - Synchronization Constructs</a></li>
<li><a href="/cs6200/p3l5-io-management/index.html">P3L5 - I/O Management</a></li>
<li><a href="/cs6200/p3l6-virtualization/index.html">P3L6 - Virtualization</a></li>
</ul>
<!--
* [P4L1 - Remote Procedure Calls](/cs6200/p4l1-remote-procedure-calls/index.html) 
* [P4L2 - Distributed File Systems](/cs6200/p4l2-distributed-file-systems/index.html) 
* [P4L3 - Distributed Shared Memory](/cs6200/p4l3-distributed-shared-memory/index.html) 
* [P4L4 - Datacenter Technologies](/cs6200/p4l4-datacenter-technologies/index.html) 
-->
<h2 id="Algorithms-Part-II">Algorithms: Part II</h2>
<ul>
<li><a href="/algorithms-2/week-1/index.html">Week 1 - Undirected Graph &amp; Directed Graph</a></li>
<li><a href="/algorithms-2/week-2/index.html">Week 2 - Minimum Spanning Trees &amp; Shortest Path</a></li>
<li><a href="/algorithms-2/week-3/index.html">Week 3 - Maximum Flow and Minimum Cut &amp; String Sort</a></li>
<li><a href="/algorithms-2/week-4/index.html">Week 4 - Tries &amp; Substring Search</a></li>
<li><a href="/algorithms-2/week-5/index.html">Week 5 - Regular Expressions</a></li>
<li><a href="/algorithms-2/week-6/index.html">Week 6 - Reductions</a></li>
</ul>
<h2 id="Algorithms-Part-I">Algorithms: Part I</h2>
<ul>
<li><a href="/algorithms-1/week-1/index.html">Week 1 - Union-Find &amp; Analysis of Algorithms</a></li>
<li><a href="/algorithms-1/week-2/index.html">Week 2 - Stacks and Queues &amp; Elementary Sorts</a></li>
<li><a href="/algorithms-1/week-3/index.html">Week 3 - Mergesort &amp; Quicksort</a></li>
<li><a href="/algorithms-1/week-4/index.html">Week 4 - Priority Queues &amp; Elementary Symbols</a></li>
<li><a href="/algorithms-1/week-5/index.html">Week 5 - Balanced Search Trees</a></li>
<li><a href="/algorithms-1/week-6/index.html">Week 6 - Hash Tables</a></li>
</ul>
<h2 id="Introduction-to-Software-Design-and-Architecture">Introduction to Software Design and Architecture</h2>
<ul>
<li><a href="/introduction-to-software-design-and-architecture/design-pattern/index.html">Design Pattern</a></li>
</ul>
<h2 id="Calculus-Two-Sequences-and-Series">Calculus Two: Sequences and Series</h2>
<ul>
<li><a href="/calculus-two/week-1/index.html">Week 1 - Sequences</a></li>
<li><a href="/calculus-two/week-2/index.html">Week 2 - Series</a></li>
<li><a href="/calculus-two/week-3/index.html">Week 3 - Convergence Tests</a></li>
<li><a href="/calculus-two/week-4/index.html">Week 4 - Alternating Series</a></li>
<li><a href="/calculus-two/week-5/index.html">Week 5 - Power Series</a></li>
<li><a href="/calculus-two/week-6/index.html">Week 6 - Taylor Series</a></li>
</ul>
<h2 id="LAFF-Linear-Algebra">LAFF Linear Algebra</h2>
<ul>
<li><a href="/laff-linear-algebra/week-1/index.html">Week 1 - Vectors in Linear Algebra</a></li>
<li><a href="/laff-linear-algebra/week-2/index.html">Week 2 - Linear Transformations and Matrices</a></li>
<li><a href="/laff-linear-algebra/week-3/index.html">Week 3 - Matrix-Vector Operations</a></li>
<li><a href="/laff-linear-algebra/week-4/index.html">Week 4 - Matrix-Vector to Matrix-Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-5/index.html">Week 5 - Matrix- Matrix Multiplication</a></li>
<li><a href="/laff-linear-algebra/week-6/index.html">Week 6 - Gaussian Elimination</a></li>
<li><a href="/laff-linear-algebra/week-7/index.html">Week 7 - More Gaussian Elimination and Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-8/index.html">Week 8 - More on Matrix Inversion</a></li>
<li><a href="/laff-linear-algebra/week-9/index.html">Week 9 - Vector Spaces</a></li>
<li><a href="/laff-linear-algebra/week-10/index.html">Week 10 - Vector Spaces, Orthogonality, and Linear Least-Squares</a></li>
<li><a href="/laff-linear-algebra/week-11/index.html">Week 11 - Orthogonal Projection, Low Rank Approximation, and Orthogonal Bases</a></li>
<li><a href="/laff-linear-algebra/week-12/index.html">Week 12 - Eigenvalues and Eigenvectors</a></li>
</ul>
<h2 id="Stanford-Machine-Learning">Stanford Machine Learning</h2>
<ul>
<li><a href="/stanford-machine-learning/week-1/index.html">Week 1 - Introduction</a></li>
<li><a href="/stanford-machine-learning/week-2/index.html">Week 2 - Linear Regression with Multiple Variables</a></li>
<li><a href="/stanford-machine-learning/week-3/index.html">Week 3 - Logistic Regression &amp; Regularization</a></li>
<li><a href="/stanford-machine-learning/week-4/index.html">Week 4 - Neural Networks: Representation</a></li>
<li><a href="/stanford-machine-learning/week-5/index.html">Week 5 - Neural Networks: Learning</a></li>
<li><a href="/stanford-machine-learning/week-6a/index.html">Week 6a - Advice for Applying Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-6b/index.html">Week 6b - Machine Learning System Design</a></li>
<li><a href="/stanford-machine-learning/week-7/index.html">Week 7 - Support Vector Machines</a></li>
<li><a href="/stanford-machine-learning/week-8/index.html">Week 8 - Unsupervised Learning &amp; Dimensionality Reduction</a></li>
<li><a href="/stanford-machine-learning/week-9a/index.html">Week 9a - Anomaly Detection</a></li>
<li><a href="/stanford-machine-learning/week-9b/index.html">Week 9b - Recommender Systems</a></li>
<li><a href="/stanford-machine-learning/week-10/index.html">Week 10 - Large Scale Machine Learning</a></li>
<li><a href="/stanford-machine-learning/week-11/index.html">Week 11 - Application Example: Photo OCR</a></li>
</ul>
<h2 id="Calculus-One">Calculus One</h2>
<ul>
<li><a href="/calculus-one/week-2-3/index.html">Week 2-3 - Functions &amp; Limits</a></li>
<li><a href="/calculus-one/week-4/index.html">Week 4 - The Beginning of Derivatives</a></li>
<li><a href="/calculus-one/week-5/index.html">Week 5 - Techniques of Differentiation</a></li>
<li><a href="/calculus-one/week-6/index.html">Week 6 - Chain Rule</a></li>
<li><a href="/calculus-one/week-7/index.html">Week 7 - Derivatives of Trigonometric Functions</a></li>
<li><a href="/calculus-one/week-8/index.html">Week 8 - Derivatives in the Real World</a></li>
<li><a href="/calculus-one/week-9/index.html">Week 9 - Optimization</a></li>
<li><a href="/calculus-one/week-10/index.html">Week 10 - Linear Approximation</a></li>
<li><a href="/calculus-one/week-11-12/index.html">Week 11-12 - Antidifferentiation &amp; Integration</a></li>
<li><a href="/calculus-one/week-13/index.html">Week 13 - Fundamental Theorem of Calculus</a></li>
<li><a href="/calculus-one/week-14/index.html">Week 14 - Substitution Rule</a></li>
<li><a href="/calculus-one/week-15/index.html">Week 15 - Techniques of Integration</a></li>
<li><a href="/calculus-one/week-16/index.html">Week 16 - Applications of Integration</a></li>
</ul>
<h2 id="Computational-Thinking">Computational Thinking</h2>
<ul>
<li><a href="/computational-thinking/lecture-1/index.html">Lecture 1 - Optimization and Knapsack Problem</a></li>
<li><a href="/computational-thinking/lecture-2/index.html">Lecture 2 - Decision Trees and Dynamic Programming</a>
<ul>
<li><a href="/computational-thinking/lecture-2-powerset/index.html">Exercise: Power Set Function</a></li>
</ul>
</li>
<li><a href="/computational-thinking/lecture-3/index.html">Lecture 3 - Graphs</a></li>
<li><a href="/computational-thinking/lecture-4-5/index.html">Lecture 4-5 - Plotting</a></li>
<li><a href="/computational-thinking/lecture-6-7/index.html">Lecture 6-7 - Stochastic Programs &amp; Inferential Statistics</a></li>
<li><a href="/computational-thinking/lecture-8/index.html">Lecture 8 - Monte Carlo Simulation</a></li>
<li><a href="/computational-thinking/lecture-9/index.html">Lecture 9 - Sampling and Standard Error</a></li>
<li><a href="/computational-thinking/lecture-10-11/index.html">Lecture 10-11 - Experimental Data</a></li>
<li><a href="/computational-thinking/lecture-12/index.html">Lecture 12 - Machine Learning</a></li>
<li><a href="/computational-thinking/lecture-13/index.html">Lecture 13 - Statistical Abuses</a></li>
</ul>
<h2 id="Effective-Thinking-Through-Mathematics">Effective Thinking Through Mathematics</h2>
<ul>
<li><a href="/effective-thinking-through-mathematics/note/index.html">Note</a></li>
<li><a href="/effective-thinking-through-mathematics/week-4-telling-the-story-of-infinity/index.html">Week 4 (/Telling the Story of Infinity)</a></li>
<li><a href="/effective-thinking-through-mathematics/week-5-telling-the-story-of-the-euler-circuit-theorem/index.html">Week 5 (/Telling the Story of Euler Circuit Theorem)</a></li>
</ul>
<h2 id="CS50-Introduction-to-Computer-Science">CS50 Introduction to Computer Science</h2>
<ul>
<li><a href="/cs50/week-1/index.html">Week 1 - C</a></li>
<li><a href="/cs50/week-2/index.html">Week 2 - Arrays</a></li>
<li><a href="/cs50/week-3/index.html">Week 3 - Algorithms</a></li>
<li><a href="/cs50/week-4/index.html">Week 4 - Memory</a></li>
<li><a href="/cs50/week-5/index.html">Week 5 - Data Structures</a></li>
<li><a href="/cs50/week-6/index.html">Week 6 - HTTP</a></li>
<li><a href="/cs50/week-7-10/index.html">Week 7-10 - Machine Learning/Python/SQL/Javascript</a></li>
</ul>
<h2 id="Others">Others</h2>
<ul>
<li><a href="/symbols/index.html">Symbols of Mathematics</a></li>
<li><a href="/glossary/index.html">Glossary</a></li>
</ul>
<h2 id="About-Me"><a target="_blank" rel="noopener" href="https://ericyy.me/about/">About Me</a></h2>

</div>


<script src="/js/book-menu.js"></script>

  </div>

  <div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseover="add_inner()" onmouseleave="remove_inner()">
  <div class="sidebar-toggle-inner"></div>
</div>

<script>
function add_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.add('show')  
}

function remove_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.remove('show')
}

function sidebar_toggle() {
    let sidebar_toggle = document.querySelector('.sidebar-toggle')
    let sidebar = document.querySelector('.book-sidebar')
    let content = document.querySelector('.off-canvas-content')
    if (sidebar_toggle.classList.contains('extend')) { // show
        sidebar_toggle.classList.remove('extend')
        sidebar.classList.remove('hide')
        content.classList.remove('extend')
    }
    else { // hide
        sidebar_toggle.classList.add('extend')
        sidebar.classList.add('hide')
        content.classList.add('extend')
    }
}
</script>

  <div class="off-canvas-content">
    <div class="columns">
      <div class="column col-10 col-lg-12">
        <div class="book-navbar">
          <!-- For Responsive Layout -->

<header class="navbar">
  <section class="navbar-section">
    <a onclick="open_sidebar()">
      <i class="icon icon-menu"></i>
    </a>
  </section>
</header>

        </div>
        <div class="book-content">
          <div class="book-post">
  <h1>Lecture 10 - 11</h1>
<h2 id="Experimental-Data">Experimental Data</h2>
<ul>
<li>These lectures are talking about the interplay between statistics and  experimental science. And how to get a valid statistical conclusion.</li>
</ul>
<h3 id="The-Behavior-of-Springs">The Behavior of Springs</h3>
<ul>
<li>
<p><strong>Hooke’s law</strong> of elasticity: <code>F = -kx</code></p>
<ul>
<li>In other words, the force, <code>F</code>, stored in a spring is linearly related to the distance, <code>x</code>, the spring has been compressed(or stretched).</li>
<li><code>k</code> is called the spring constant.</li>
<li>All springs have an <strong>elastic limit</strong>, beyond which the law fails.</li>
</ul>
</li>
<li>
<p>For example, How much does a rider have to weigh to compress the spring  on motorbike 1cm? (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>≈</mo><mn>35</mn><mo separator="true">,</mo><mn>000</mn><mi>N</mi><mi mathvariant="normal">/</mi><mi>m</mi></mrow><annotation encoding="application/x-tex">k \approx 35,000 N/m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">35</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">000</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord">/</span><span class="mord mathnormal">m</span></span></span></span>)</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>=</mo><mn>0.01</mn><mi>m</mi><mo>∗</mo><mn>35</mn><mo separator="true">,</mo><mn>000</mn><mi>N</mi><mi mathvariant="normal">/</mi><mi>m</mi></mrow><annotation encoding="application/x-tex">F = 0.01m * 35,000 N/m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.01</span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">35</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">000</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord">/</span><span class="mord mathnormal">m</span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>=</mo><mn>350</mn><mi>N</mi></mrow><annotation encoding="application/x-tex">F = 350 N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">350</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>=</mo><mi>m</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo>∗</mo><mi>a</mi><mi>c</mi><mi>c</mi></mrow><annotation encoding="application/x-tex">F = mass * acc</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.4653em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">ss</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">a</span><span class="mord mathnormal">cc</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>=</mo><mi>m</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo>∗</mo><mn>9.81</mn><mi>m</mi><mi mathvariant="normal">/</mi><msup><mi>s</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">F = mass * 9.81m/s^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.4653em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">ss</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord">9.81</span><span class="mord mathnormal">m</span><span class="mord">/</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo>∗</mo><mn>9.81</mn><mi>m</mi><mi mathvariant="normal">/</mi><msup><mi>s</mi><mn>2</mn></msup><mo>=</mo><mn>350</mn><mi>N</mi></mrow><annotation encoding="application/x-tex">mass * 9.81m/s^2 = 350N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4653em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">ss</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord">9.81</span><span class="mord mathnormal">m</span><span class="mord">/</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">350</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo>≈</mo><mn>35.68</mn><mi>k</mi><mi>g</mi></mrow><annotation encoding="application/x-tex">mass \approx 35.68kg</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4831em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">ss</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord">35.68</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span></span></span></span></li>
</ul>
</li>
<li>
<p>Finding <code>k</code></p>
<ul>
<li>
<p>We can’t conduct the experiment perfectly, so, to hang a series of increasingly heavier weights on the spring, measure the stretch of the spring each time, and plot the results will be a better way.</p>
</li>
<li>
<p>First we should know:</p>
<ul>
<li><code>F=-kx</code></li>
<li><code>k=-F/x</code></li>
<li><code>k=9.81*m/x</code></li>
<li>
<img src="https://i.imgur.com/8cGM01y.jpg" style="width:200px" />
</li>
</ul>
</li>
<li>
<p>Then, we got some data</p>
   <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">0.0865 0.1</span><br><span class="line">0.1015 0.15</span><br><span class="line">...</span><br><span class="line">0.4263 0.65</span><br><span class="line">0.4562 0.7</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>Plot the data, we got:</p>
<ul>
<li>
<img src="https://i.imgur.com/ff83AbS.jpg" style="width:250px" />
</li>
<li>It has some measurement errors.</li>
</ul>
</li>
<li>
<p>Next step is to fit curves to data.</p>
<ul>
<li>
<p>When we fit a curve to a set of data, we are finding a fit that relates an independent variable (the mass) to an estimated value of a dependent variable (the distance)</p>
</li>
<li>
<p>To fit curves to data, we need to define an <strong>objective function</strong> that provides a quantitative assessment of how well the curve fits the data. In this case, a straight line, that is <strong>linear function</strong>.</p>
</li>
<li>
<p>Then, finding the best fit is an optimization problem. most commonly used <strong>objective function</strong> is called <strong>least squares</strong>:</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle scriptlevel="0" displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>l</mi><mi>e</mi><mi>n</mi><mo stretchy="false">(</mo><mi>o</mi><mi>b</mi><mi>s</mi><mi>e</mi><mi>r</mi><mi>v</mi><mi>e</mi><mi>d</mi><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn></mrow></munderover><mo stretchy="false">(</mo><mi>o</mi><mi>b</mi><mi>s</mi><mi>e</mi><mi>r</mi><mi>v</mi><mi>e</mi><mi>d</mi><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo>−</mo><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi><mi>e</mi><mi>d</mi><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mstyle></mrow><annotation encoding="application/x-tex">\displaystyle\sum_{i=0}^{len(observed)-1}(observed[i] - predicted[i])^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.2387em;vertical-align:-1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.961em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.386em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">b</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">ser</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">d</span><span class="mclose mtight">)</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">o</span><span class="mord mathnormal">b</span><span class="mord mathnormal" style="margin-right:0.02778em;">ser</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span><span class="mopen">[</span><span class="mord mathnormal">i</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1141em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">re</span><span class="mord mathnormal">d</span><span class="mord mathnormal">i</span><span class="mord mathnormal">c</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span><span class="mopen">[</span><span class="mord mathnormal">i</span><span class="mclose">]</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></li>
</ul>
</li>
<li>
<p>Next is to use a successive approximation algorithm to find the best least-squares fit. <strong>PyLab</strong> provides a built-in function, <strong>polyfit</strong>:</p>
   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pylab.polyfit(observedXVals, observedYVals, n)</span><br></pre></td></tr></table></figure>
<ul>
<li>this function finds the coefficients of a polynomial of degree <code>n</code> that provides a best least-squares fit for the set of points defined by the arrays observedXVals and observedYVals.</li>
<li>The algorithm used by polyfit is called <strong>linear regression</strong>.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Visualizing the Fit</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">getData</span>(<span class="params">fileName</span>):</span><br><span class="line">    dataFile = <span class="built_in">open</span>(fileName, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    distances = []</span><br><span class="line">    masses = []</span><br><span class="line">    dataFile.readline() <span class="comment">#discard header</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> dataFile:</span><br><span class="line">        d, m = line.split()</span><br><span class="line">        distances.append(<span class="built_in">float</span>(d))</span><br><span class="line">        masses.append(<span class="built_in">float</span>(m))</span><br><span class="line">    dataFile.close()</span><br><span class="line">    <span class="keyword">return</span> (masses, distances)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">labelPlot</span>():</span><br><span class="line">    pylab.title(<span class="string">&#x27;Measured Displacement of Spring&#x27;</span>)</span><br><span class="line">    pylab.xlabel(<span class="string">&#x27;|Force| (Newtons)&#x27;</span>)</span><br><span class="line">    pylab.ylabel(<span class="string">&#x27;Distance (meters)&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotData</span>(<span class="params">fileName</span>):</span><br><span class="line">    xVals, yVals = getData(fileName)</span><br><span class="line">    xVals = pylab.array(xVals)</span><br><span class="line">    yVals = pylab.array(yVals)</span><br><span class="line">    xVals = xVals*<span class="number">9.81</span>  <span class="comment">#acc. due to gravity</span></span><br><span class="line">    pylab.plot(xVals, yVals, <span class="string">&#x27;bo&#x27;</span>,</span><br><span class="line">               label = <span class="string">&#x27;Measured displacements&#x27;</span>)</span><br><span class="line">    labelPlot()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fitData</span>(<span class="params">fileName</span>):</span><br><span class="line">    xVals, yVals = getData(fileName)</span><br><span class="line">    xVals = pylab.array(xVals)</span><br><span class="line">    yVals = pylab.array(yVals)</span><br><span class="line">    xVals = xVals*<span class="number">9.81</span> <span class="comment">#get force</span></span><br><span class="line">    pylab.plot(xVals, yVals, <span class="string">&#x27;bo&#x27;</span>,</span><br><span class="line">               label = <span class="string">&#x27;Measured points&#x27;</span>)</span><br><span class="line">    labelPlot()                 </span><br><span class="line">    a,b = pylab.polyfit(xVals, yVals, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># estYVals = a*xVals + b</span></span><br><span class="line">    estYVals = pylab.polyval(model, xVals)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;a =&#x27;</span>, a, <span class="string">&#x27;b =&#x27;</span>, b)</span><br><span class="line">    pylab.plot(xVals, estYVals, <span class="string">&#x27;r&#x27;</span>,</span><br><span class="line">               label = <span class="string">&#x27;Linear fit, k = &#x27;</span></span><br><span class="line">               + <span class="built_in">str</span>(<span class="built_in">round</span>(<span class="number">1</span>/a, <span class="number">5</span>)))</span><br><span class="line">    pylab.legend(loc = <span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">fitData(<span class="string">&#x27;springData.txt&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>
<img src="https://i.imgur.com/XGEOx1N.jpg" style="width:250px" />
</li>
<li>
<p><code>k = 21.53686</code></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Another-Experiment">Another Experiment</h3>
<ul>
<li>
<p>to fit curves to these mystery data:</p>
<ul>
<li>
<img src="https://i.imgur.com/AdHxTCO.jpg" style="width:250px" />
</li>
<li>
<img src="https://i.imgur.com/q0aeuY3.jpg" style="width:250px" />
</li>
</ul>
</li>
<li>
<p>how good are these fits</p>
<ul>
<li>
<p>we can see that quadratic model is better than linear model, but how bad a fit is the line and how good is the quadratic fit?</p>
</li>
<li>
<p>comparing <strong>mean squared error</strong></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">aveMeanSquareError</span>(<span class="params">data, predicted</span>):</span><br><span class="line">    error = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">        error += (data[i] - predicted[i])**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> error/<span class="built_in">len</span>(data)</span><br></pre></td></tr></table></figure>
<ul>
<li>then we got:
<ul>
<li>Ave. mean square error for linear model = 9372.73078965</li>
<li>Ave. mean square error for quadratic model = 1524.02044718</li>
</ul>
</li>
<li>Seems like, quadratic model is better than linear model. But we still have to ask, is the quadratic fit good in an absolute sense?</li>
<li>The mean square error is useful for comparing two different models of the same data, but it’s not actually very useful for getting the absolute goodness of fit.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Coefficient-of-Determination-R-2">Coefficient of Determination ( R^2 )</h3>
<ul>
<li>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mrow><msub><mo>∑</mo><mi>i</mi></msub><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>p</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><mrow><msub><mo>∑</mo><mi>i</mi></msub><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">R^{2}=1-\frac{\sum_{i}(y_i-p_i)^2}{\sum_{i}(y_i-\mu_i)^2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.7289em;vertical-align:-0.57em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1589em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1496em;"><span style="top:-2.1786em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3214em;"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight"><span class="mclose mtight">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.535em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1496em;"><span style="top:-2.1786em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3214em;"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight"><span class="mclose mtight">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.57em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<ul>
<li>By comparing the estimation errors (the numerator) with the variability of the original values (the denominator), R^2 is intended to capture the proportion of variability in a data set that is accounted for by the statistical model provided by the fit</li>
<li>Always between 0 and 1 when fit generated by a linear regression and tested on training data</li>
<li>If R^2 = 1, the model explains all of the variability in the data. If R^2 = 0, there is no relationship between the values predicted by the model and the actual data.</li>
</ul>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rSquared</span>(<span class="params">observed, predicted</span>):</span><br><span class="line"> error = ((predicted - observed)**<span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line"> meanError = error/<span class="built_in">len</span>(observed)</span><br><span class="line"> <span class="keyword">return</span> <span class="number">1</span> - (meanError/numpy.var(observed))</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>Testing Goodness of Fits</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">genFits</span>(<span class="params">xVals, yVals, degrees</span>):</span><br><span class="line"> models = []</span><br><span class="line"> <span class="keyword">for</span> d <span class="keyword">in</span> degrees:</span><br><span class="line">     model = pylab.polyfit(xVals, yVals, d)</span><br><span class="line">     models.append(model)</span><br><span class="line"> <span class="keyword">return</span> models</span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">testFits</span>(<span class="params">models, degrees, xVals, yVals, title</span>):</span><br><span class="line"> pylab.plot(xVals, yVals, <span class="string">&#x27;o&#x27;</span>, label = <span class="string">&#x27;Data&#x27;</span>)</span><br><span class="line"> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(models)):</span><br><span class="line">     estYVals = pylab.polyval(models[i], xVals)</span><br><span class="line">     error = rSquared(yVals, estYVals)</span><br><span class="line">     pylab.plot(xVals, estYVals,</span><br><span class="line">                label = <span class="string">&#x27;Fit of degree &#x27;</span>\</span><br><span class="line">                + <span class="built_in">str</span>(degrees[i])\</span><br><span class="line">                + <span class="string">&#x27;, R2 = &#x27;</span> + <span class="built_in">str</span>(<span class="built_in">round</span>(error, <span class="number">5</span>)))</span><br><span class="line"> pylab.legend(loc = <span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"> pylab.title(title)</span><br><span class="line">  </span><br><span class="line">xVals, yVals = getData(<span class="string">&#x27;mysteryData.txt&#x27;</span>)</span><br><span class="line">degrees = (<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">models = genFits(xVals, yVals, degrees)</span><br><span class="line">testFits(models, degrees, xVals, yVals, <span class="string">&#x27;Mystery Data&#x27;</span>)</span><br></pre></td></tr></table></figure>
  <img src="https://i.imgur.com/4wim8KX.jpg" style="width:250px" />
<ul>
<li>Quadratic model get 84%, and Linear get almost 0%.</li>
<li>Since the degree of polynomial can affect the Goodness of Fits, what if we use some bigger ones? Can we get a tighter Fit?
<ul>
<li>
<img src="https://i.imgur.com/Jr5Pi74.jpg" style="width:250px" />
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="A-Tighter-Fit-May-Not-Be-We-Need-So-How-to-Choose">A Tighter Fit May Not Be We Need, So How to Choose?</h3>
<ul>
<li>
<p>Why We Build Models</p>
<ul>
<li>Help us understand the process that generated the data</li>
<li>Help us make predictions about out-of-sample data</li>
<li>A good model helps us do these things</li>
</ul>
</li>
<li>
<p>How Mystery Data Was Generated</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def genNoisyParabolicData(a, b, c, xVals, fName):</span><br><span class="line">    yVals = []</span><br><span class="line">    for x in xVals:</span><br><span class="line">        theoreticalVal = a*x**2 + b*x + c</span><br><span class="line">        yVals.append(theoreticalVal + random.gauss(0, 35))</span><br><span class="line">    f = open(fName,&#x27;w&#x27;)</span><br><span class="line">    f.write(&#x27;y        x\n&#x27;)</span><br><span class="line">    for i in range(len(yVals)):</span><br><span class="line">        f.write(str(yVals[i]) + &#x27; &#x27; + str(xVals[i]) + &#x27;\n&#x27;)</span><br><span class="line">    f.close()</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>Let’s Look at Two Data Sets</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#parameters for generating data</span></span><br><span class="line">xVals = <span class="built_in">range</span>(-<span class="number">10</span>, <span class="number">11</span>, <span class="number">1</span>)</span><br><span class="line">a, b, c = <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">degrees = (<span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#generate data</span></span><br><span class="line">random.seed(<span class="number">0</span>)</span><br><span class="line">genNoisyParabolicData(a, b, c, xVals, <span class="string">&#x27;Dataset 1.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">genNoisyParabolicData(a, b, c, xVals, <span class="string">&#x27;Dataset 2.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">xVals1, yVals1 = getData(<span class="string">&#x27;Dataset 1.txt&#x27;</span>)</span><br><span class="line">models1 = genFits(xVals1, yVals1, degrees)</span><br><span class="line">testFits(models1, degrees, xVals1, yVals1, <span class="string">&#x27;DataSet 1.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">pylab.figure()</span><br><span class="line">xVals2, yVals2 = getData(<span class="string">&#x27;Dataset 2.txt&#x27;</span>)</span><br><span class="line">models2 = genFits(xVals2, yVals2, degrees)</span><br><span class="line">testFits(models2, degrees, xVals2, yVals2, <span class="string">&#x27;DataSet 2.txt&#x27;</span>)</span><br></pre></td></tr></table></figure>
 <img src="https://i.imgur.com/kw5Ju19.jpg" style="width:250px" /> 
 <img src="https://i.imgur.com/ptvPwi5.jpg" style="width:250px" />
</li>
<li>
<p>Hence Degree 16 Is Tightest Fit, But</p>
<ul>
<li>We want model to work well on other data generated by the same process</li>
<li>It needs to generalize</li>
</ul>
</li>
<li>
<p><strong>Cross Validate</strong></p>
<ul>
<li>Generate models using one dataset, and then test it on the other
<ul>
<li>Use models for Dataset 1 to predict points for Dataset 2</li>
<li>Use models for Dataset 2 to predict points for Dataset 1</li>
</ul>
</li>
<li>Expect testing error to be larger than training error</li>
<li>A better indication of generalizability than training error</li>
</ul>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pylab.figure()</span><br><span class="line">testFits(models1, degrees, xVals2, yVals2, <span class="string">&#x27;DataSet 2/Model 1&#x27;</span>)</span><br><span class="line">pylab.figure()</span><br><span class="line">testFits(models2, degrees, xVals1, yVals1, <span class="string">&#x27;DataSet 1/Model 2&#x27;</span>)</span><br></pre></td></tr></table></figure>
 <img src="https://i.imgur.com/2LmtbF0.jpg" style="width:250px" /> 
 <img src="https://i.imgur.com/yh2kz3R.jpg" style="width:250px" />
</li>
<li>
<p>Increasing the Complexity</p>
<ul>
<li>coefficient may be zero if Fitting a quadratic to a perfect line
<ul>
<li>for example one:
<ul>
<li>data: [{0, 0}, {1, 1}, {2, 2}, {3, 3}]</li>
<li>Fit <code>y=ax^2+bx+c</code>, will get <code>y=0x^2+1x+0</code>, which is <code>y=x</code></li>
</ul>
</li>
<li>for example two:
<ul>
<li>data: [{0, 0}, {1, 1}, {2, 2}, {3, 3.1}]</li>
<li>Fit <code>y=ax^2+bx+c</code>, will get <code>y=0.025x^2+0.955x+0.005</code>, which is <code>y=x</code></li>
</ul>
</li>
</ul>
</li>
<li>if data is noisy, can fit the noise rather than the underlying pattern in the data?
<ul>
<li>for example:
<ul>
<li>data: [{0, 0}, {1, 1}, {2, 2}, {3, 3}, {4, 20}]</li>
<li>Fit <code>y=ax^2+bx+c</code>, we got:
<ul>
<li>
<img src="https://i.imgur.com/QYRlNdg.jpg" style="width:250px" />
</li>
<li>R-squared = 0.7026</li>
</ul>
</li>
<li>suppose we had used a first-degree fit
<ul>
<li><code>model = pylab.polyfit(xVals, yVals, 1)</code></li>
<li>
<img src="https://i.imgur.com/WMhH0oF.jpg" style="width:250px" />
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Conclusion</p>
<ul>
<li>Choosing an overly-complex model can leads to <strong>overfitting</strong> to the training data</li>
<li>Increases the risk of a model that works poorly on data not included in the training set</li>
<li>On the other hand choosing an insufficiently complex model has other problems
<ul>
<li>when we fit a line to data that was basically parabolic</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Suppose-We-Don’t-Have-a-Solid-Theory-to-Validate-Model">Suppose We Don’t Have a Solid Theory to Validate Model</h3>
<ul>
<li>Use <strong>cross-validation</strong> results to guide the choice of model complexity</li>
<li>If dataset small, use leave-one-out cross validation</li>
<li>If dataset large enough, use k-fold cross validation or repeated-random-sampling validation</li>
</ul>
<h4 id="Leave-one-out-Cross-Validation">Leave-one-out Cross Validation</h4>
<ul>
<li>
<p>Let <code>D</code> be the original data set</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">testResults = []</span><br><span class="line"></span><br><span class="line">for i in range(len(D)): </span><br><span class="line">    training = D[:].pop(i)</span><br><span class="line">    model = buildModel(training)</span><br><span class="line">    testResults.append(test(model, D[i]))</span><br><span class="line">Average testResults</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="Repeated-Random-Sampling">Repeated Random Sampling</h4>
<ul>
<li>
<p>Let <code>D</code> be the original data set</p>
</li>
<li>
<p>Let <code>n</code> be the number of random samples</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">testResults = []</span><br><span class="line">for i in range(n)</span><br><span class="line">    randomly partition D into two sets:</span><br><span class="line">    training and test</span><br><span class="line">    model = buildModel(training)</span><br><span class="line">    testResults.append(test(model, test))</span><br><span class="line">Average testResults</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="An-Example-Temperature-By-Year">An Example, Temperature By Year</h4>
<ul>
<li>
<p>Task: Model how the mean daily high temperature in the U.S. varied from 1961 through 2015</p>
<ul>
<li>Get means for each year and plot them</li>
<li>Randomly divide data in half n times
<ul>
<li>For each dimensionality to be tried
<ul>
<li>Train on one half of data</li>
<li>Test on other half</li>
<li>Record r-squared on test data</li>
</ul>
</li>
</ul>
</li>
<li>Report mean r-squared for each dimensionality</li>
<li>Code: <a target="_blank" rel="noopener" href="https://github.com/erictt/computer-science-learning/blob/master/computational-thinking/unit-4/lecture11-3.py">lecture11-3.py</a></li>
</ul>
</li>
<li>
<p>The Whole Data Set:</p>
<ul>
<li>
<img src="https://i.imgur.com/oAmUXUr.jpg" style="width:250px" />
</li>
</ul>
</li>
<li>
<p>Results:</p>
<ul>
<li>Mean R-squares for test data
<ul>
<li>For dimensionality 1 mean = 0.7535 Std = 0.0656</li>
<li>For dimensionality 2 mean = 0.7291 Std = 0.0744</li>
<li>For dimensionality 3 mean = 0.7039 Std = 0.0684</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Line seems to be the winner</p>
<ul>
<li>Highest average r-squared</li>
<li>Simplest model</li>
</ul>
</li>
</ul>
<h3 id="Wrapping-Up-Curve-Fitting">Wrapping Up Curve Fitting</h3>
<ul>
<li>We can use linear regression to fit a curve to data
<ul>
<li>Mapping from independent values to dependent values</li>
</ul>
</li>
<li>That curve is a model of the data that can be used to predict the value associated with independent values we haven’t seen (out of sample data)</li>
<li>R-squared used to evaluate model
<ul>
<li>Higher not always “better” because of risk of over fitting</li>
</ul>
</li>
<li>Choose complexity of model based on
<ul>
<li>Theory about structure of data</li>
<li>Cross validation</li>
<li>Simplicity</li>
</ul>
</li>
</ul>

</div>


  <div class="book-comments">
    




  </div>



<script src="/js/book-post.js"></script>


<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js" integrity="sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

        </div>
      </div>
      <div class="column col-2 hide-lg">
        <div class="book-post-info">
  
    <div class="book-post-meta">
  
  <div class="divider"></div>
</div>

  

  <div class="book-tocbot">
</div>
<div class="book-tocbot-menu">
  <a class="book-toc-expand" onclick="expand_toc()">Expand all</a>
  <a onclick="go_top()">Back to top</a>
  <a onclick="go_bottom()">Go to bottom</a>
</div>


<script src="/js/book-toc.js"></script>

</div>

      </div>
    </div>
  </div>
  
  <a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>

</body>
</html>


<script src="/js/book.js"></script>
